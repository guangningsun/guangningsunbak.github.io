<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Adolph孙广宁的博客]]></title>
  <link href="http://guangningsun.github.io/atom.xml" rel="self"/>
  <link href="http://guangningsun.github.io/"/>
  <updated>2016-03-09T18:38:10+08:00</updated>
  <id>http://guangningsun.github.io/</id>
  <author>
    <name><![CDATA[Guangning Sun]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[ceph多节点部署]]></title>
    <link href="http://guangningsun.github.io/blog/2016/03/09/cephduo-jie-dian-bu-shu/"/>
    <updated>2016-03-09T18:20:28+08:00</updated>
    <id>http://guangningsun.github.io/blog/2016/03/09/cephduo-jie-dian-bu-shu</id>
    <content type="html"><![CDATA[<h3>实验环境</h3>

<hr />

<ul>
<li>VMWare 虚拟4台 centos7.1 测试机</li>
<li>ceph osd不支持操作系统所在挂载盘，需要新挂载盘 并格式化xfs 并按照配置文件创建存储目录</li>
<li><p>查看 /var/lib/ceph/osd/ceph-*</p>

<p> 官方文档推荐用ceph-depoly 半自动化部署工具，详细部署过程后续补充</p>

<p> 但部署过程中出现错误较多，所以改为先手动配置</p></li>
<li><p>ceph-deploy install &ndash;rgw nodename 安装ceph gw并可更新ceph版本</p></li>
</ul>


<h3>打通各节点的ssh机器互信</h3>

<ul>
<li><p>ssh-keygen -t rsa -f .ssh/id_rsa -P &lsquo;&rsquo;</p>

<p>  此处一路回车并没有设置验证码，生产可能并不允许。</p></li>
</ul>


<h3>向被授权的主机上拷贝</h3>

<ul>
<li>ssh-copy-id -i .ssh/id_rsa.pub <a href="&#109;&#x61;&#x69;&#108;&#x74;&#111;&#58;&#114;&#x6f;&#x6f;&#x74;&#x40;&#x31;&#x39;&#50;&#46;&#x31;&#x36;&#56;&#46;&#x32;&#x38;&#x2e;&#x31;&#51;&#x30;">&#x72;&#111;&#x6f;&#116;&#x40;&#49;&#x39;&#x32;&#46;&#49;&#x36;&#56;&#x2e;&#50;&#56;&#46;&#x31;&#x33;&#48;</a></li>
</ul>


<h3>安装ntpdate</h3>

<ul>
<li>yum -y install ntpdate</li>
</ul>


<h3>开始同步时间</h3>

<ul>
<li>ntpdate time.windows.com</li>
</ul>


<h3>修改每台机器的主机名 (hostname)  ( mon,mds,osd,client 都必须设置 )</h3>

<hr />

<pre><code>    echo 192.168.28.128 ceph-mon &gt;&gt; /etc/hosts

    echo 192.168.28.129 ceph-osd0 &gt;&gt; /etc/hosts

    echo 192.168.28.130 ceph-osd1 &gt;&gt; /etc/hosts

    echo 192.168.28.131 ceph-osd2 &gt;&gt; /etc/hosts
</code></pre>

<h3>yum更新，安装相关依赖包(适用于mon,mds,osd)</h3>

<hr />

<pre><code>    rpm --import 'https://ceph.com/git/?p=ceph.git;a=blob_plain;f=keys/release.asc'
    rpm -Uvh http://mirrors.yun-idc.com/epel/7/x86_64/e/epel-release-7-5.noarch.rpm
    yum install snappy leveldb gdisk python-argparse gperftools-libs -y
    rpm -Uvh http://ceph.com/rpm-dumpling/el7/noarch/ceph-release-1-0.el7.centos.noarch.rpm
    yum install ceph-deploy -y
    yum install ceph -y

    yum install btrfs-progs           (适用于所有osd) 
</code></pre>

<h3>配置/etc/ceph/ceph.conf，  (适用于mon,mds,osd)</h3>

<hr />

<p>vi /etc/ceph/ceph.conf</p>

<hr />

<p>[global]</p>

<p>public_network = 192.168.28.1/24</p>

<p>pid_file = /var/run/ceph/$name.pid</p>

<p>auth_cluster_required = none</p>

<p>auth_service_required = none</p>

<p>auth_client_required = none</p>

<p>keyring = /etc/ceph/keyring.$name</p>

<p>osd_pool_default_size = 1</p>

<p>osd_pool_default_min_size = 1</p>

<p>osd_pool_default_crush_rule = 0</p>

<p>osd_crush_chooseleaf_type = 1</p>

<p>[mon]</p>

<p>mon_data = /var/lib/ceph/mon/$name</p>

<p>mon_clock_drift_allowed = .15</p>

<p>keyring = /etc/ceph/keyring.$name</p>

<p>[mon.0]</p>

<p>host = ceph-mon</p>

<p>mon_addr = 192.168.28.129:6789</p>

<p>[osd]</p>

<p>osd_recovery_max_active = 5</p>

<p>osd_mkfs_type = xfs</p>

<p>keyring = /etc/ceph/keyring.$name</p>

<p>[osd.0]</p>

<p>host = ceph-osd0</p>

<p>devs = /dev/sda3</p>

<p>[osd.1]</p>

<p>host = ceph-osd1</p>

<p>devs = /dev/sda3</p>

<p>[osd.2]</p>

<p>host = ceph-osd2</p>

<p>devs = /dev/sda3</p>

<p>[client.radosgw.gateway]</p>

<p>host = ceph-mon</p>

<p>log file = /var/log/radosgw/client.radosgw.ustack.log</p>

<h3>创建目录 ( osd )</h3>

<hr />

<pre><code>    mkdir -p /var/lib/ceph/osd/ceph-* 默认为该目录下存放数据，可以在[osd]中配置存放路径
</code></pre>

<h3>启动ceph(在mon上执行)</h3>

<hr />

<pre><code>    初始化：mkcephfs -a -c /etc/ceph/ceph.conf
    此时数据目录下应为空
    /etc/init.d/ceph -a start
</code></pre>

<h3>启动时遇到问题</h3>

<hr />

<pre><code>    遇到 Error ENOENT: osd.0 does not exist.  create it before updating the crush map
    执行如下代码: ceph osd create
    然后在执行   /etc/init.d/ceph -a start       既可完成（仍需清空数据目录）
</code></pre>

<h3>启动后健康检查</h3>

<hr />

<pre><code>ceph health      #也可以使用ceph -s命令查看状态  (如果返回的是HEALTH_OK，则代表成功！) 
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[ceph原理分析]]></title>
    <link href="http://guangningsun.github.io/blog/2016/03/01/ceph-analyze/"/>
    <updated>2016-03-01T15:05:00+08:00</updated>
    <id>http://guangningsun.github.io/blog/2016/03/01/ceph-analyze</id>
    <content type="html"><![CDATA[<h1>1.分析块存储、文件存储、对象存储</h1>

<ul>
<li><h3>1.1 什么是块存储</h3></li>
</ul>


<hr />

<ul>
<li><p>维基百科中定义为：块（block）数据存储，一种将电子数据存储为相同大小的方法</p></li>
<li><p>DSA和SAN是两种经典的块存储模型：</p>

<pre><code>  1） DAS（Direct Attach STorage）：是直接连接于主机服务器的一种储存方式。（主要代表设备就是个人PC电脑，硬盘直接挂载在服务器上）。
  - 每一台主机服务器有独立的储存设备，每台主机服务器的储存设备无法互通，需要跨主机存取资料时，必须经过相对复杂的设定，若主机服务器分属不同的操作系统，要存取彼此的资料，更是复杂，有些系统甚至不能存取。（windows数据优盘在mint系统就无法读取）
  - 通常用在单一网络环境下且数据交换量不大，性能要求不高的环境下，可以说是一种应用较为早的技术实现。

  2）SAN（Storage Area Network）：是一种用高速（光纤）网络联接专业主机服务器的一种储存方式，此系统会位于主机群的后端，它使用高速I/O 联结方式。
  - 例：（如 SCSI及 Fibre- Channels SAN应用在对网络速度要求高、对数据的可靠性和安全性要求高、对数据共享的性能要求高的应用环境中，特点是代价高，性能好。例如电信、银行的大数据量关键应用。）        
  * 它采用SCSI 块I/O的命令集，通过在磁盘或FC（Fiber Channel）级的数据访问提供高性能的随机I/O和数据吞吐率，它具有高带宽、低延迟的优势，在高性能计算中占有一席之地，但是由于SAN系统的价格较高，且可扩展性较差，已不能满足成千上万个CPU规模的系统。
  - 对于用户来说，SAN好比是一块大磁盘，用户可以根据需要随意将SAN格式化成想要的文件系统来使用。SAN在网络中通过iSCSI（IPSAN）协议连接，属block及存储，但可扩展性较差。
</code></pre></li>
</ul>


<p><img src="http://7xrcb8.com1.z0.glb.clouddn.com/%E5%9D%97%E5%AD%98%E5%82%A8.png" alt="storage" /></p>

<ul>
<li><h3>1.2  什么是文件存储</h3></li>
</ul>


<hr />

<ul>
<li><p>文件系统出现解决了块存储模式共享困难的弊端</p>

<pre><code>  通常，NAS产品都是文件级存储。  NAS（Network Attached Storage）：是一套网络储存设备，通常是直接连在网络上并提供资料存取服务，一套 NAS 储存设备就如同一个提供数据文件服务的系统，特点是性价比高。例如教育、政府、企业等数据存储应用。
  - 对于用户来说，NAS好比是一个共享文件夹，文件系统已经存在，用户可以直接将自己的数据存放在NAS上。NAS以文件为传输协议，开销很大，不利于在高性能集群中使用

  * 它采用NFS或CIFS命令集访问数据，以文件为传输协议（FTP  ’ （学校里有ftp服务器，下载过期的资料）FTP 是File Transfer Protocol（文件传输协议）的英文简称，而中文简称为“文传协议”。用于Internet上的控制文件的双向传输。同时，它也是一个应用程序（Application）。基于###不同的操作系统有不同的FTP应用程序####，而所有这些应用程序都遵守同一种协议以传输文件。在FTP的使用当中，用户经常遇到两个概念："下载"（Download）和"上传"（Upload）。"下载"文件就是从远程主机拷贝文件至自己的计算机上；"上传"文件就是将文件从自己的计算机中拷贝至远程主机上。用Internet语言来说，用户可通过客户机程序向（从）远程主机上传（下载）文件。‘通过TCP/IP实现网络化存储，可扩展性好、价格便宜、用户易管理，如目前在集群计算中应用较多的NFS文件系统。

  - window  和 linux 内核都有自己的ftp客户端
  - 基于不同的 操作系统可以共享文件
  - 但由于NAS的协议开销高、带宽低、延迟大，不利于在高性能集群中应用。
</code></pre></li>
<li><h3>1.3  什么是对象存储</h3></li>
</ul>


<hr />

<pre><code>    针对Linux集群对存储系统高性能和数据共享的需求，国际上希望能有效结合SAN和NAS系统的优点，支持直接访问磁盘以提高性能，通过共享的文件和元数据以简化管理，对象存储应用而生。

    - 维基百科：对象存储（也称为基于对象的存储[1]）是管理数据作为对象，而不是其他存储架构象它管理数据作为文件层次结构和块存储用于管理数据扇区内的块的文件系统的存储体系结构和跟踪。 
    - 如下图中的每个对象通常包括数据本身，元数据的可变数量，以及一个全局唯一标识符。对象存储可在多个级别，包括设备水平（对象的存储设备），系统级和接口电平来实现。在每一种情况下，对象存储旨在使未由其他存储架构寻址能力，这样可以由应用程序直接编程，可以跨越物理硬件的多个实例命名空间接口，和数据管理功能。
    - 例如数据复制和数据分配在对象级粒度。HDFS数据存储在块力度，编写程序时候要了解block数据长度，偏移量以及每个byte字节所代表含义等信息，再计算数据
</code></pre>

<p><img src="http://7xrcb8.com1.z0.glb.clouddn.com/osd%E5%AD%98%E5%82%A8.png" alt="" /></p>

<pre><code>    而所谓对象存储，就是每个数据对应着一个唯一的id，在面向对象存储中。
    - 不再有类似文件系统的目录层级结构，完全扁平化存储，即可以根据对象的id直接定位到数据的位置，这一点类似SAN。
    - 而每个数据对象即包含元数据又包括存储数据，含有文件的概念，这一点类似NAS。
    - 除此之外，用户不必关系数据对象的安全性，数据恢复，自动负载平衡等等问题，这些均由对象存储系统自身完成。
    - 而且，面向对象存储还解决了SAN面临的有限扩充和NAS传输性能开销大问题，能够实现海量数据存储。
</code></pre>

<p><img src="http://7xrcb8.com1.z0.glb.clouddn.com/%E6%99%AE%E9%80%9A%E5%AD%98%E5%82%A8%E4%BA%8E%E5%9D%97%E5%AD%98%E5%82%A8%E5%AF%B9%E6%AF%94.png" alt="osd &amp; common" /></p>

<p>总体上来讲，对象存储同兼具SAN高速直接访问磁盘特点及NAS的分布式共享特点。</p>

<ul>
<li><h3>1.4  各类存储总结</h3></li>
</ul>


<hr />

<ul>
<li><h4>1.4.1传统存储与OSD存储对比</h4>

<pre><code>  - LBA，全称为Logical Block Address，是PC数据存储装置上用来表示数据所在位置的通用机制，我们最常见到使用它的装置就是硬盘。
</code></pre>

<p><img src="http://7xrcb8.com1.z0.glb.clouddn.com/osd%E5%AD%98%E5%82%A8%E4%B8%8E%E5%9D%97%E5%AD%98%E5%82%A8.png" alt="storage end" /></p></li>
</ul>


<h1>2.经典分布式存储HDFS存储原理及特点</h1>

<h3>2.1、HDFS的主要设计理念</h3>

<hr />

<p>HDFS是Hadoop Distribute File System 的简称，也就是Hadoop的一个分布式文件系统。</p>

<pre><code>    1、存储超大文件 这里的“超大文件”是指几百MB、GB甚至TB级别的文件。
    2、最高效的访问模式是 一次写入、多次读取(流式数据访问)HDFS存储的数据集作为hadoop的分析对象。
    3、运行在普通廉价的服务器上HDFS设计理念之一就是让它能运行在普通的硬件之上，即便硬件出现故障，也可以通过容错策略来保证数据的高可用。
</code></pre>

<h3>2.2 HDFS基本概念</h3>

<hr />

<pre><code>    数据块（block）：大文件会被分割成多个block进行存储，block大小默认为64MB。每一个block会在多个datanode上存储多份副本，默认是3份。

    namenode：namenode负责管理文件目录、文件和block的对应关系以及block和datanode的对应关系。

    datanode：datanode就负责存储了，当然大部分容错机制都是在datanode上实现的。
</code></pre>

<h3>2.3 HDFS基本架构图</h3>

<hr />

<p><img src="http://7xrcb8.com1.z0.glb.clouddn.com/hdfs%E6%9E%B6%E6%9E%84%E5%9B%BE.png" alt="" /></p>

<h5>图中有几个概念需要介绍一下</h5>

<pre><code>    Rack 是指机柜的意思，一个block的三个副本通常会保存到两个或者两个以上的机柜中（当然是机柜中的服务器），这样做的目的是做防灾容错，因为发生一个机柜掉电或者一个机柜的交换机挂了的概率还是蛮高的。
</code></pre>

<h3>2.4 HDFS写文件流程</h3>

<hr />

<p><img src="http://7xrcb8.com1.z0.glb.clouddn.com/HDFS%E5%86%99%E6%96%87%E4%BB%B6.png" alt="" /></p>

<ul>
<li><p>思考：</p>

<pre><code>  1.在datanode执行create file后，namenode采用什么策略给client分配datanode？
  2.顺序写入三个datanode，写入过程中有一个datanode挂掉了，如何容错？
  3.client往datanode写入数据时挂掉了，怎么容错？
</code></pre></li>
</ul>


<h3>2.5 HDFS读文件流程</h3>

<hr />

<p><img src="http://7xrcb8.com1.z0.glb.clouddn.com/HDFS%E8%AF%BB%E6%96%87%E4%BB%B6.png" alt="" /></p>

<h3>2.6 HDFS NN 高可用容错机制</h3>

<hr />

<ul>
<li><h4>secondary NN 工作原理</h4>

<p><img src="http://7xrcb8.com1.z0.glb.clouddn.com/NN%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6.jpeg" alt="" /></p></li>
<li><h4>NN HA 高可用机制</h4>

<p><img src="http://tech.uc.cn/wp-content/uploads/2012/08/HA%E6%9E%B6%E6%9E%84%E5%9B%BE.png" alt="" /></p></li>
</ul>


<h3>2.6 HDFS的弊端</h3>

<hr />

<pre><code>    1、将HDFS用于对数据访问要求低延迟的场景,由于HDFS是为高数据吞吐量应用而设计的，必然以高延迟为代价。
    2、存储大量小文件,HDFS中元数据（文件的基本信息）存储在namenode的内存中，而namenode为单点，小文件数量大到一定程度，namenode内存就吃不消了。
    3、扔会丢失一部分数据
</code></pre>

<h1>3.CEPH的工作原理</h1>

<h2>3.1 CEPH生态环境</h2>

<hr />

<ul>
<li><p>Ceph是统一存储系统，支持三种接口。</p>

<pre><code>  Object：有原生的API，而且也兼容Swift和S3的API
  Block：支持精简配置、快照、克隆
  File：Posix接口，支持快照
</code></pre></li>
<li><p>Ceph也是分布式存储系统，它的特点是：</p>

<pre><code>  高扩展性：使用普通x86服务器，支持10~1000台服务器，支持TB到PB级的扩展。
  高可靠性：没有单点故障，多数据副本，自动管理，自动修复。
  高性能：数据分布均衡，并行化度高。对于objects storage和block storage,不需要元数据服务器。
  sage论文中说是可以无限扩展，但目前还没有证实
</code></pre>

<p><img src="http://7xrcb8.com1.z0.glb.clouddn.com/ceph%E6%9E%B6%E6%9E%84.png" alt="" /></p></li>
</ul>


<h6>一个Ceph集群由两种类型的后台进程（Daemon）组成：</h6>

<pre><code>    Ceph的底层是RADOS，它的意思是“A reliable, autonomous, distributed object storage”。 RADOS由两个组件组成：
    OSD Daemon： Object Storage Device，提供存储资源。
    Ceph Monitor：维护整个Ceph集群的全局状态。
</code></pre>

<h6>Ceph OSD Daemon</h6>

<pre><code>Object Storage Device（OSD）是Ceph集群中的重要组成部分。OSD可以存储文件或数据的内容，它使用文件系统来存储数据。OSD Daemon主要负责管理集群中的所有磁盘。OSD Daemon还负责在本地文件系统存储数据，并为不同的客户软件或存取媒介通过网络提供数据访问。而且，OSD Daemon还负责添加和删除磁盘，磁盘分区，管理OSD、低层空间管理，提供安全措施和磁盘数据的可复制性。
</code></pre>

<h6>Ceph Monitor</h6>

<pre><code>Ceph Monitor也是一种Ceph OSD Daemon，它主要负责管理全部集群。当你运行一个Ceph集群时，你就会需要Ceph Monitor每天帮你检查集群的健康情况和状态。管理一个集群需要每天做很多工作比如检测所有OSD的状态和文件系统或块数据的状态。你可以通过Ceph Monitor来管理负载均衡和数据响应的详细信息。为了更好的了解Ceph集群的工作原理，我们来看看它是如何处理三种类型数据存储的机制。
</code></pre>

<h6>Ceph Object storage</h6>

<pre><code>当向Ceph写入数据时，Ceph通过内部机制自动跨集群标记和复制数据。Ceph存储对象数据时，不仅可以通过调用Ceph内部的API来实现，还可以通过亚马逊的S3服务或AWS REST提供的API来实现。Ceph块存储机制提供了RADOS（Reliable Autonomic Distributed Object Store）服务。RADOS服务存储机制中不可或缺的；RADOS服务通过使用节点中安装的软件管理工具能够扩展千级的硬件设备（通常被应用为“Nodes“）。
</code></pre>

<h6>Ceph Block Storage</h6>

<pre><code>Ceph的块存储模式使用户可以像挂载一个小型块设备一样挂载Ceph。在块数据存储级别上，RADOS服务也保证块数据的可扩展性。Librados就是包含在这一级别上的一个python类库，你可以使用librados类库和存储服务器或节点进行通信。Librados是一个开源的应用，你可以调整和增强它。Librados通过“RADOS Block Device“即RBD与后台进行交互。RBD不仅继承了Librados的功能，还能够为集群建立快照和恢复数据。
</code></pre>

<h6>Ceph File Storage</h6>

<pre><code>CephFS 是一个为Ceph集群设计的，且遵循POSIX标准的分布式文件系统。CephFS提供把数据目录和文件映射到存储在RADOS中对象的存储的服务。通过这种方式，CephFS和RADOS可以相互协作。在这里，RADOS动态均等地把数据分布到不同的节点上。这种文件系统支持无限的数据存储和更强的数据安全性。在文件存储集群系统中，Ceph因提供容量大和高可扩展性而闻名。请注意你可以同时把Ceph与btrfs或EXT4一起使用，但Red Hat推荐使用最新Linux内核（3.14版本或者更新版本）。
</code></pre>

<h5>Object Storage、Block Storage、FileSystem。Ceph另外两个组件是：</h5>

<pre><code>MDS：用于保存CephFS的元数据。
RADOS Gateway：对外提供REST接口，兼容S3和Swift的API。
</code></pre>

<h6>结论</h6>

<pre><code>Red Hat下的Ceph文件系统拥有性价比高、操作简单、集群数据高可靠性的特点。RedHat也一直为Ceph投入了很多人力，这也确保了Bug可的跟进速度，以及新特性的引入。由于Ceph是开源的，所以你可以按照你的需求随意修改它。
</code></pre>

<p><img src="http://7xrcb8.com1.z0.glb.clouddn.com/1.png" alt="" /></p>

<h2>3.2 CEPH存储原理</h2>

<hr />

<h3>3.2.1 CEPH寻址流程</h3>

<hr />

<pre><code>    本节将对Ceph的工作原理和若干关键工作流程进行扼要介绍。如前所述，由于Ceph的功能实现本质上依托于RADOS，因而，此处的介绍事实上也是针对RADOS进行。对于上层的部分，特别是RADOS GW和RBD，由于现有的文档中（包括Sage的论文中）并未详细介绍。

    ‘不用查找算算就好’ 首先介绍RADOS中最为核心的、基于计算的对象寻址机制，然后说明对象存取的工作流程
</code></pre>

<h5>Ceph系统中的寻址流程如下图所示：</h5>

<p><img src="http://7xrcb8.com1.z0.glb.clouddn.com/ceph%E5%AF%BB%E5%9D%80%E6%B5%81%E7%A8%8B.png" alt="" /></p>

<pre><code>    1. File —— 此处的file就是用户需要存储或者访问的文件。对于一个基于Ceph开发的对象存储应用而言，这个file也就对应于应用中的“对象”，也就是用户直接操作的“对象”。

    2. Ojbect —— 此处的object是RADOS所看到的“对象”。Object与上面提到的file的区别是，object的最大size由RADOS限定（通常为2MB或4MB），以便实现底层存储的组织管理。因此，当上层应用向RADOS存入size很大的file时，需要将file切分成统一大小的一系列object（最后一个的大小可以不同）进行存储。为避免混淆，在本文中将尽量避免使用中文的“对象”这一名词，而直接使用file或object进行说明。

    3. PG（Placement Group）—— 顾名思义，PG的用途是对object的存储进行组织和位置映射。具体而言，一个PG负责组织若干个object（可以为数千个甚至更多），但一个object只能被映射到一个PG中，即，PG和object之间是“一对多”映射关系。同时，一个PG会被映射到n个OSD上，而每个OSD上都会承载大量的PG，即，PG和OSD之间是“多对多”映射关系。在实践当中，n至少为2，如果用于生产环境，则至少为3。一个OSD上的PG则可达到数百个。事实上，PG数量的设置牵扯到数据分布的均匀性问题。关于这一点，下文还将有所展开。

    4. OSD —— 即object storage device，前文已经详细介绍，此处不再展开。唯一需要说明的是，OSD的数量事实上也关系到系统的数据分布均匀性，因此其数量不应太少。在实践当中，至少也应该是数十上百个的量级才有助于Ceph系统的设计发挥其应有的优势。

    5. Failure domain —— 这个概念在论文中并没有进行定义，好在对分布式存储系统有一定概念的读者应该能够了解其大意。
</code></pre>

<h5>基于上述定义，解释寻址流程。具体而言Ceph中的寻址至少要经历以下三次映射：</h5>

<pre><code>    1. File -&gt; object映射

    这次映射的目的是，将用户要操作的file，映射为RADOS能够处理的object。其映射十分简单，本质上就是按照object的最大size对file进行切分，相当于RAID中的条带化过程。这种切分的好处有二：一是让大小不限的file变成最大size一致、可以被RADOS高效管理的object；二是让对单一file实施的串行处理变为对多个object实施的并行化处理。

    每一个切分后产生的object将获得唯一的oid，即object id。其产生方式也是线性映射，极其简单。图中，ino是待操作file的元数据，可以简单理解为该file的唯一id。ono则是由该file切分产生的某个object的序号。而oid就是将这个序号简单连缀在该file id之后得到的。举例而言，如果一个id为filename的file被切分成了三个object，则其object序号依次为0、1和2，而最终得到的oid就依次为filename0、filename1和filename2。

    这里隐含的问题是，ino的唯一性必须得到保证，否则后续映射无法正确进行。

    2. Object -&gt; PG映射

    在file被映射为一个或多个object之后，就需要将每个object独立地映射到一个PG中去。这个映射过程也很简单，如图中所示，其计算公式是：

    hash(oid) &amp; mask -&gt; pgid

    由此可见，其计算由两步组成。首先是使用Ceph系统指定的一个静态哈希函数计算oid的哈希值，将oid映射成为一个近似均匀分布的伪随机值。然后，将这个伪随机值和mask按位相与，得到最终的PG序号（pgid）。根据RADOS的设计，给定PG的总数为m（m应该为2的整数幂），则mask的值为m-1。因此，哈希值计算和按位与操作的整体结果事实上是从所有m个PG中近似均匀地随机选择一个。基于这一机制，当有大量object和大量PG时，RADOS能够保证object和PG之间的近似均匀映射。又因为object是由file切分而来，大部分object的size相同，因而，这一映射最终保证了，各个PG中存储的object的总数据量近似均匀。

    从介绍不难看出，这里反复强调了“大量”。只有当object和PG的数量较多时，这种伪随机关系的近似均匀性才能成立，Ceph的数据存储均匀性才有保证。为保证“大量”的成立，一方面，object的最大size应该被合理配置，以使得同样数量的file能够被切分成更多的object；另一方面，Ceph也推荐PG总数应该为OSD总数的数百倍，以保证有足够数量的PG可供映射。

    3. PG -&gt; OSD映射

    第三次映射就是将作为object的逻辑组织单元的PG映射到数据的实际存储单元OSD。如图所示，RADOS采用一个名为CRUSH的算法，将pgid代入其中，然后得到一组共n个OSD。这n个OSD即共同负责存储和维护一个PG中的所有object。前已述及，n的数值可以根据实际应用中对于可靠性的需求而配置，在生产环境下通常为3。具体到每个OSD，则由其上运行的OSD deamon负责执行映射到本地的object在本地文件系统中的存储、访问、元数据维护等操作。

    和“object -&gt; PG”映射中采用的哈希算法不同，这个CRUSH算法的结果不是绝对不变的，而是受到其他因素的影响。其影响因素主要有二：

    一是当前系统状态，也就是上文逻辑结构中曾经提及的cluster map。当系统中的OSD状态、数量发生变化时，cluster map可能发生变化，而这种变化将会影响到PG与OSD之间的映射。

    二是存储策略配置。这里的策略主要与安全相关。利用策略配置，系统管理员可以指定承载同一个PG的3个OSD分别位于数据中心的不同服务器乃至机架上，从而进一步改善存储的可靠性。

    因此，只有在系统状态（cluster map）和存储策略都不发生变化的时候，PG和OSD之间的映射关系才是固定不变的。在实际使用当中，策略一经配置通常不会改变。而系统状态的改变或者是由于设备损坏，或者是因为存储集群规模扩大。好在Ceph本身提供了对于这种变化的自动化支持，因而，即便PG与OSD之间的映射关系发生了变化，也并不会对应用造成困扰。事实上，Ceph正是需要有目的的利用这种动态映射关系。正是利用了CRUSH的动态特性，Ceph可以将一个PG根据需要动态迁移到不同的OSD组合上，从而自动化地实现高可靠性、数据分布re-blancing等特性。

    之所以在此次映射中使用CRUSH算法，而不是其他哈希算法，原因之一正是CRUSH具有上述可配置特性，可以根据管理员的配置参数决定OSD的物理位置映射策略；另一方面是因为CRUSH具有特殊的“稳定性”，也即，当系统中加入新的OSD，导致系统规模增大时，大部分PG与OSD之间的映射关系不会发生改变，只有少部分PG的映射关系会发生变化并引发数据迁移。这种可配置性和稳定性都不是普通哈希算法所能提供的。因此，CRUSH算法的设计也是Ceph的核心内容之一，具体介绍可以参考。
</code></pre>

<h3>3.2.2 CEPH寻址总结及问题解析</h3>

<hr />

<pre><code>    至此为止，Ceph通过三次映射，完成了从file到object、PG和OSD整个映射过程。通观整个过程，可以看到，这里没有任何的全局性查表操作需求。至于唯一的全局性数据结构cluster map，在后文中将加以介绍。可以在这里指明的是，cluster map的维护和操作都是轻量级的，不会对系统的可扩展性、性能等因素造成不良影响。
</code></pre>

<h6>一个可能出现的困惑是：为什么需要同时设计第二次和第三次映射？难道不重复么？分析如下：</h6>

<pre><code>    我们可以反过来想像一下，如果没有PG这一层映射，又会怎么样呢？在这种情况下，一定需要采用某种算法，将object直接映射到一组OSD上。如果这种算法是某种固定映射的哈希算法，则意味着一个object将被固定映射在一组OSD上，当其中一个或多个OSD损坏时，object无法被自动迁移至其他OSD上（因为映射函数不允许），当系统为了扩容新增了OSD时，object也无法被re-balance到新的OSD上（同样因为映射函数不允许）。这些限制都违背了Ceph系统高可靠性、高自动化的设计初衷。

    如果采用一个动态算法（例如仍然采用CRUSH算法）来完成这一映射，似乎是可以避免静态映射导致的问题。但是，其结果将是各个OSD所处理的本地元数据量爆增，由此带来的计算复杂度和维护工作量也是难以承受的。

    例如，在Ceph的现有机制中，一个OSD平时需要和与其共同承载同一个PG的其他OSD交换信息，以确定各自是否工作正常，是否需要进行维护操作。由于一个OSD上大约承载数百个PG，每个PG内通常有3个OSD，因此，一段时间内，一个OSD大约需要进行数百至数千次OSD信息交换。然而，如果没有PG的存在，则一个OSD需要和与其共同承载同一个object的其他OSD交换信息。由于每个OSD上承载的object很可能高达数百万个，因此，同样长度的一段时间内，一个OSD大约需要进行的OSD间信息交换将暴涨至数百万乃至数千万次。而这种状态维护成本显然过高。

    综上引入PG的好处至少有二：一方面实现了object和OSD之间的动态映射，从而为Ceph的可靠性、自动化等特性的实现留下了空间；另一方面也有效简化了数据的存储组织，大大降低了系统的维护管理开销。理解这一点，对于彻底理解Ceph的对象寻址机制，是十分重要的。
</code></pre>

<h3>3.3.3 数据操作流程</h3>

<hr />

<pre><code>此处将首先以file写入过程为例，对数据操作流程进行说明。为简化说明，便于理解，此处进行若干假定。首先，假定待写入的file较小，无需切分，仅被映射为一个object。其次，假定系统中一个PG被映射到3个OSD上。
</code></pre>

<h6>file写入流程如下图表示：</h6>

<p><img src="http://7xrcb8.com1.z0.glb.clouddn.com/Ceph%E5%86%99%E5%85%A5%E6%B5%81%E7%A8%8B.png" alt="" /></p>

<pre><code>如图所示，当某个client需要向Ceph集群写入一个file时，首先需要在本地完成5.1节中所叙述的寻址流程，将file变为一个object，然后找出存储该object的一组三个OSD。这三个OSD具有各自不同的序号，序号最靠前的那个OSD就是这一组中的Primary OSD，而后两个则依次是Secondary OSD和Tertiary OSD。

找出三个OSD后，client将直接和Primary OSD通信，发起写入操作（步骤1）。Primary OSD收到请求后，分别向Secondary OSD和Tertiary OSD发起写入操作（步骤2、3）。当Secondary OSD和Tertiary OSD各自完成写入操作后，将分别向Primary OSD发送确认信息（步骤4、5）。当Primary OSD确信其他两个OSD的写入完成后，则自己也完成数据写入，并向client确认object写入操作完成（步骤6）。

之所以采用这样的写入流程，本质上是为了保证写入过程中的可靠性，尽可能避免造成数据丢失。同时，由于client只需要向Primary OSD发送数据，因此，在Internet使用场景下的外网带宽和整体访问延迟又得到了一定程度的优化。

当然，这种可靠性机制必然导致较长的延迟，特别是，如果等到所有的OSD都将数据写入磁盘后再向client发送确认信号，则整体延迟可能难以忍受。因此，Ceph可以分两次向client进行确认。当各个OSD都将数据写入内存缓冲区后，就先向client发送一次确认，此时client即可以向下执行。待各个OSD都将数据写入磁盘后，会向client发送一个最终确认信号，此时client可以根据需要删除本地数据。

分析上述流程可以看出，在正常情况下，client可以独立完成OSD寻址操作，而不必依赖于其他系统模块。因此，大量的client可以同时和大量的OSD进行并行操作。同时，如果一个file被切分成多个object，这多个object也可被并行发送至多个OSD。

从OSD的角度来看，由于同一个OSD在不同的PG中的角色不同，因此，其工作压力也可以被尽可能均匀地分担，从而避免单个OSD变成性能瓶颈。

如果需要读取数据，client只需完成同样的寻址过程，并直接和Primary OSD联系。目前的Ceph设计中，被读取的数据仅由Primary OSD提供。但目前也有分散读取压力以提高性能的讨论。
</code></pre>

<h2>3.3 普通Hash算法及一致性Hash算法</h2>

<hr />

<h3>3.3.1 普通Hash算法</h3>

<hr />

<pre><code>    哈希表就是一种以 键-值(key-indexed) 存储数据的结构，我们只要输入待查找的值即key，即可查找到其对应的值。

    哈希的思路很简单，如果所有的键都是整数，那么就可以使用一个简单的无序数组来实现：将键作为索引，值即为其对应的值，这样就可以快速访问任意键的值。这是对于简单的键的情况，我们将其扩展到可以处理更加复杂的类型的键。
</code></pre>

<ul>
<li><p>使用哈希查找有两个步骤:</p>

<pre><code>  使用哈希函数将被查找的键转换为数组的索引。在理想的情况下，不同的键会被转换为不同的索引值，但是在有些情况下我们需要处理多个键被哈希到同一个索引值的情况。所以哈希查找的第二个步骤就是处理冲突
</code></pre></li>
<li><p>处理哈希碰撞冲突。有很多处理哈希碰撞冲突的方法，本文后面会介绍拉链法和线性探测法。</p>

<pre><code>  哈希表是一个在时间和空间上做出权衡的经典例子。如果没有内存限制，那么可以直接将键作为数组的索引。那么所有的查找时间复杂度为O(1)；如果没有时间限制，那么我们可以使用无序数组并进行顺序查找，这样只需要很少的内存。哈希表使用了适度的时间和空间来在这两个极端之间找到了平衡。只需要调整哈希函数算法即可在时间和空间上做出取舍。
</code></pre></li>
<li><p>数据分布是分布式存储系统的一个重要部分，数据分布算法至少要考虑以下三个因素：</p>

<pre><code>  1) 故障域隔离。同份数据的不同副本分布在不同的故障域，降低数据损坏的风险；

  2) 负载均衡。数据能够均匀地分布在磁盘容量不等的存储节点，避免部分节点空闲部分节点超载，从而影响系统性能；

  3) 控制节点加入离开时引起的数据迁移量。当节点离开时，最优的数据迁移是只有离线节点上的数据被迁移到其它节点，而正常工作的节点的数据不会发生迁移。

  对象存储中一致性Hash和Ceph的CRUSH算法是使用地比较多的数据分布算法。在Aamzon的Dyanmo键值存储系统中采用一致性Hash算法，并且对它做了很多优化。OpenStack的Swift对象存储系统也使用了一致性Hash算法。
</code></pre></li>
</ul>


<h3>3.3.2 一致性Hash算法</h3>

<hr />

<pre><code>    假设数据为 x ，存储节点数目为 N 。将数据分布到存储节点的最直接做法是，计算数据 x 的Hash值，并将结果同节点数目 N 取余数，余数就是数据x的目的存储节点。即目的存储节点为 Hash(x) % N 。对数据计算Hash值的目的为了可以让数据均匀分布在N个节点中。这种做法的一个严重问题是，当加入新节点或则节点离开时，几乎所有数据都会受到影响，需要重新分布。因此，数据迁移量非常大。
</code></pre>

<p><img src="http://7xrcb8.com1.z0.glb.clouddn.com/%E6%99%AE%E9%80%9AHash.png" alt="" /></p>

<pre><code>    一致性Hash算法将数据和存储节点映射到同个Hash空间，如上图所示。Hash环中的3存储节点把Hash空间划分成3个分区，每个存储节点负责一个分区上的数据。例如，落在分区[N2,N0]上的数据存储在节点N0。

    一致性Hash算法能够很好地控制节点加入离开导致的迁移数据的数量。如图(b)所示，当节点N0离开时，原来由它负责的[N2, N0]分区将同[N0, N1]分区合并成[N2, N1]分区，并且都由节点N1负责。也就是说，本来存储在节点N0上的数据都迁移到节点N1，而原来存储在N1和N2节点的数据不受影响。图(c)给出了当节点N3加入时，原来[N2, N0]分区分裂成[N3, N0]和[N2, N3]两个分区，其中[N3, N0]分区上是数据迁移到新加入的N3节点。
</code></pre>

<h4>虚拟节点</h4>

<pre><code>一致性Hash的一个问题是，存储节点不能将Hash空间划分地足够均匀。如上图(a)所示，分区[N2, N0]的大小几乎是其它两个分区大小之和。这容易让负责该分区的节点N0负载过重。假设3个节点的磁盘容量相等，那么当节点N0的磁盘已经写满数据时其它两个节点上的磁盘还有很大的空闲空间，但此时系统已经无法继续向分区[N2, N0]写入数据，从而造成资源浪费。
</code></pre>

<p><img src="http://7xrcb8.com1.z0.glb.clouddn.com/%E8%99%9A%E6%8B%9F%E8%8A%82%E7%82%B9HASH.png" alt="" /></p>

<pre><code>虚拟节点是相对于物理存储节点而言的，虚拟节点负责的分区上的数据最终存储到其对应的物理节点。在一致性Hash中引入虚拟节点可以把Hash空间划分成更多的分区，从而让数据在存储节点上的分布更加均匀。如上图(b)所示，黄颜色的节点代表虚拟节点，Ni_0代表该虚拟节点对应于物理节点i的第0个虚拟节点。增加虚拟节点后，物理节点N0负责[N1_0, N0]和[N0, N0_0]两个分区，物理节点N1负责[N0_0, N1]和[N2_0, N1_0]两个分区，物理节点N2负责[N2, N1]和[N2_0, N2]两个分区，三个物理节点负责的总的数据量趋于平衡。

实际应用中，可以根据物理节点的磁盘容量的大小来确定其对应的虚拟节点数目。虚拟节点数目越多，节点负责的数据区间也越大。
</code></pre>

<h4>分区与分区位置</h4>

<pre><code>前文提到，当节点加入或者离开时，分区会相应地进行分裂或合并。这不对新写入的数据构成影响，但对已经写入到磁盘的数据需要重新计算Hash值以确定它是否需要迁移到其它节点。因为需要遍历磁盘中的所有数据，这个计算过程非常耗时。如下图(a)所示，分区是由落在Hash环上的虚拟节点 Ti 来划分的，并且分区位置(存储分区数据的节点)也同虚拟节点相关，即存储到其顺时针方向的第1个虚拟节点。
</code></pre>

<p><img src="http://7xrcb8.com1.z0.glb.clouddn.com/%E5%88%86%E5%8C%BAHASH.png" alt="" /></p>

<pre><code>在Dynamo的论文中提出了分离分区和分区位置的方法来解决这个问题。该方法将Hash空间划分成固定的若干个分区，虚拟节点不再用于划分分区而用来确定分区的存储位置。如上图(b)所示，将Hash空间划分成[A,B]，[B,C], [C,D]和[D,A]四个固定的分区。虚拟节点用于确定分区位置，例如T1负责分区[B,C]，T2负责分区[C,D]，T0负责[D,A]和[A,B]两个分区。由于分区固定，因此迁移数据时可以很容易知道哪些数据需要迁移哪些数据不需要迁移。

上图(b)中虚拟节点T0负责了[D,A]和[A,B]两个分区的数据，这是由分区数目和虚拟节点数目不相同导致的。为让分区分布地更加均匀，Dyanmo提出了维持分区数目和虚拟节点数目相等的方法。这样每个虚拟节点负责一个分区，在物理节点的磁盘容量都相同并且虚拟节点数目都相同的情况下，每个物理节点负责的分区大小是完全相同的，从而可以达到最佳的数据分布。
</code></pre>

<h2>3.4 CEPH核心算法CRUSH原理</h2>

<pre><code>Ceph分布数据的过程：首先计算数据 x 的Hash值并将结果和PG数目取余，以得到数据 x 对应的 PG 编号。然后，通过CRUSH算法将PG映射到一组OSD中。最后把数据 x 存放到PG对应的OSD中。这个过程中包含了两次映射，第一次是数据 x 到PG的映射。如果把PG当作存储节点，那么这和文章开头提到的普通Hash算法一样。不同的是，PG是抽象的存储节点，它不会随着物理节点的加入或则离开而增加或减少，因此数据到PG的映射是稳定的。
</code></pre>

<p><img src="http://7xrcb8.com1.z0.glb.clouddn.com/CRUSH%E7%AE%97%E6%B3%95.png" alt="" /></p>

<pre><code>在这个过程中，PG起到了两个作用：第一个作用是划分数据分区。每个PG管理的数据区间相同，因而数据能够均匀地分布到PG上；第二个作用是充当Dyanmo中Token的角色，即决定分区位置。实际上，这和Dynamo中固定分区数目，以及维持分区数目和虚拟节点数目相等的原则是同一回事。

在没有多副本的情况下，Dynamo中分区的数据直接存储到Token，而每个Token对应唯一的一个物理存储节点。在多副本(假设副本数目为 N )的情况下，分区的数据会存储到连续的 N 个Token中。但这会引入一个新问题：因为副本必须保持在不同的物理节点，但是如果这组Token中存在两个或多个Token对应到同个物理存储节点，那么就必须要跳过这样的节点。Dynamo采用Preference列表来记录每个分区对应的物理节点。然而，Dynmao论文中没有详述分区的Preference列表如何选取物理节点，以及选取物理节点时该如何隔离故障域等问题。

(osd0, osd1, osd2 … osdn) = CRUSH(x)

Ceph的PG担当起Dynamo中Token、固定分区以及Preference列表的角色，解决的是同样的问题。PG的Acting集合对应于Dynamo的Preference列表。CRUSH算法解决了Dynamo论文中未提及的问题。
</code></pre>

<h3>OSD层级结构和权重大小</h3>

<ul>
<li><p>CRUSH算法的目的是，为给定的PG(即分区)分配一组存储数据的OSD节点。选择OSD节点的过程，要考虑以下几个因素：</p>

<pre><code>  1) PG在OSD间均匀分布。假设每个OSD的磁盘容量都相同，那么我们希望PG在每个OSD节点上是均匀分布的，也就是说每个OSD节点包含相同数目的PG。假如节点的磁盘容量不等，那么容量大的磁盘的节点能够处理更多数量的PG。 
  2) PG的OSD分布在不同的故障域。因为PG的OSD列表用于保存数据的不同副本，副本分布在不同的OSD中可以降低数据损坏的风险。
</code></pre></li>
</ul>


<p><img src="http://7xrcb8.com1.z0.glb.clouddn.com/osd%E6%9D%83%E9%87%8D.png" alt="" /></p>

<pre><code>    Ceph使用树型层级结构描述OSD的空间位置以及权重(同磁盘容量相关)大小。如上图所示，层级结构描述了OSD所在主机、主机所在机架以及机架所在机房等空间位置。这些空间位置隐含了故障区域，例如使用不同电源的不同的机架属于不同的故障域。CRUSH能够依据一定的规则将副本放置在不同的故障域。

    OSD节点在层级结构中也被称为Device，它位于层级结构的叶子节点，所有非叶子节点称为Bucket。Bucket拥有不同的类型，如上图所示，所有机架的类型为Rack，所有主机的类型为Host。使用者还可以自己定义Bucket的类型。Device节点的权重代表存储节点的性能，磁盘容量是影响权重大小的重要参数。Bucket节点的权重是其子节点的权重之和。

    CRUSH通过重复执行Take(bucketID)和Select(n, bucketType)两个操作选取副本位置。      
    Take(bucketID)指定从给定的bucketID中选取副本位置，例如可以指定从某台机架上选取副本位置，以实现将不同的副本隔离在不同的故障域; Select(n, bucketType)则在给定的Bucket下选取 n 个类型为bucketType的Bucket，它选取Bucket主要考虑层级结构中节点的容量，以及当节点离线或者加入时的数据迁移量。
</code></pre>

<h1>4.浅析CEPH所能提供的服务</h1>

<h1>5.CEPH API Documents</h1>

<h6>LIBRADOS (PYTHON)</h6>

<p>The rados module is a thin Python wrapper for librados.</p>

<h6>INSTALLATION</h6>

<p>To install Python libraries for Ceph, see Getting librados for Python.</p>

<h6>GETTING STARTED</h6>

<p>You can create your own Ceph client using Python. The following tutorial will show you how to import the Ceph Python module, connect to a Ceph cluster, and perform object operations as a client.admin user.</p>

<p>Note To use the Ceph Python bindings, you must have access to a running Ceph cluster. To set one up quickly, see Getting Started.
First, create a Python source file for your Ceph client. ::
linenos:    sudo vim client.py
IMPORT THE MODULE</p>

<p>To use the rados module, import it into your source file.</p>

<pre><code> import rados
</code></pre>

<h6>CONFIGURE A CLUSTER HANDLE</h6>

<p>Before connecting to the Ceph Storage Cluster, create a cluster handle. By default, the cluster handle assumes a cluster named ceph (i.e., the default for deployment tools, and our Getting Started guides too), and a client.admin user name. You may change these defaults to suit your needs.</p>

<p>To connect to the Ceph Storage Cluster, your application needs to know where to find the Ceph Monitor. Provide this information to your application by specifying the path to your Ceph configuration file, which contains the location of the initial Ceph monitors.</p>

<pre><code> import rados, sys

 #Create Handle Examples.
 cluster = rados.Rados(conffile='ceph.conf')
 cluster = rados.Rados(conffile=sys.argv[1])
 cluster = rados.Rados(conffile = 'ceph.conf', conf = dict (keyring = '/path/to/keyring'))
</code></pre>

<p>Ensure that the conffile argument provides the path and file name of your Ceph configuration file. You may use the sys module to avoid hard-coding the Ceph configuration path and file name.</p>

<p>Your Python client also requires a client keyring. For this example, we use the client.admin key by default. If you would like to specify the keyring when creating the cluster handle, you may use the conf argument. Alternatively, you may specify the keyring path in your Ceph configuration file. For example, you may add something like the following line to you Ceph configuration file:</p>

<p>keyring = /path/to/ceph.client.admin.keyring
For additional details on modifying your configuration via Python, see Configuration.</p>

<h6>CONNECT TO THE CLUSTER</h6>

<p>Once you have a cluster handle configured, you may connect to the cluster. With a connection to the cluster, you may execute methods that return information about the cluster.</p>

<pre><code> import rados, sys

 cluster = rados.Rados(conffile='ceph.conf')
 print "\nlibrados version: " + str(cluster.version())
 print "Will attempt to connect to: " + str(cluster.conf_get('mon initial members'))

 cluster.connect()
 print "\nCluster ID: " + cluster.get_fsid()

 print "\n\nCluster Statistics"
 print "=================="
 cluster_stats = cluster.get_cluster_stats()

 for key, value in cluster_stats.iteritems():
         print key, value
</code></pre>

<p>By default, Ceph authentication is on. Your application will need to know the location of the keyring. The python-ceph module doesn’t have the default location, so you need to specify the keyring path. The easiest way to specify the keyring is to add it to the Ceph configuration file. The following Ceph configuration file example uses the client.admin keyring you generated with ceph-deploy.</p>

<pre><code> [global]
 ...
 keyring=/path/to/keyring/ceph.client.admin.keyring
</code></pre>

<h6>MANAGE POOLS</h6>

<p>When connected to the cluster, the Rados API allows you to manage pools. You can list pools, check for the existence of a pool, create a pool and delete a pool.</p>

<pre><code> print "\n\nPool Operations"
 print "==============="

 print "\nAvailable Pools"
 print "----------------"
 pools = cluster.list_pools()

 for pool in pools:
         print pool

 print "\nCreate 'test' Pool"
 print "------------------"
 cluster.create_pool('test')

 print "\nPool named 'test' exists: " + str(cluster.pool_exists('test'))
 print "\nVerify 'test' Pool Exists"
 print "-------------------------"
 pools = cluster.list_pools()

 for pool in pools:
         print pool

 print "\nDelete 'test' Pool"
 print "------------------"
 cluster.delete_pool('test')
 print "\nPool named 'test' exists: " + str(cluster.pool_exists('test'))
</code></pre>

<h6>INPUT/OUTPUT CONTEXT</h6>

<p>Reading from and writing to the Ceph Storage Cluster requires an input/output context (ioctx). You can create an ioctx with the open_ioctx() method of the Rados class. The ioctx_name parameter is the name of the pool you wish to use.</p>

<pre><code> ioctx = cluster.open_ioctx('data')
</code></pre>

<p>Once you have an I/O context, you can read/write objects, extended attributes, and perform a number of other operations. After you complete operations, ensure that you close the connection. For example:</p>

<pre><code> print "\nClosing the connection."
 ioctx.close()
</code></pre>

<h6>WRITING, READING AND REMOVING OBJECTS</h6>

<p>Once you create an I/O context, you can write objects to the cluster. If you write to an object that doesn’t exist, Ceph creates it. If you write to an object that exists, Ceph overwrites it (except when you specify a range, and then it only overwrites the range). You may read objects (and object ranges) from the cluster. You may also remove objects from the cluster. For example:</p>

<pre><code>print "\nWriting object 'hw' with contents 'Hello World!' to pool 'data'."

ioctx.write_full("hw", "Hello World!")

print "\n\nContents of object 'hw'\n------------------------\n"
print ioctx.read("hw")

print "\nRemoving object 'hw'"
ioctx.remove_object("hw")
</code></pre>

<h6>WRITING AND READING XATTRS</h6>

<p>Once you create an object, you can write extended attributes (XATTRs) to the object and read XATTRs from the object. For example:</p>

<pre><code>print "\n\nWriting XATTR 'lang' with value 'en_US' to object 'hw'"
ioctx.set_xattr("hw", "lang", "en_US")

print "\n\nGetting XATTR 'lang' from object 'hw'\n"
print ioctx.get_xattr("hw", "lang")
</code></pre>

<h6>LISTING OBJECTS</h6>

<p>If you want to examine the list of objects in a pool, you may retrieve the list of objects and iterate over them with the object iterator. For example:</p>

<pre><code>object_iterator = ioctx.list_objects()

while True :

    try :
            rados_object = object_iterator.next()
            print "Object contents = " + rados_object.read()

    except StopIteration :
            break
</code></pre>

<p>The Object class provides a file-like interface to an object, allowing you to read and write content and extended attributes. Object operations using the I/O context provide additional functionality and asynchronous capabilities.</p>

<h1>6.基于CEPH的网盘设计与实现</h1>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[abstract_ceph]]></title>
    <link href="http://guangningsun.github.io/blog/2016/01/22/abstract-ceph/"/>
    <updated>2016-01-22T16:22:21+08:00</updated>
    <id>http://guangningsun.github.io/blog/2016/01/22/abstract-ceph</id>
    <content type="html"><![CDATA[<h1>Ceph简介及研究计划书</h1>

<h2>简介</h2>

<h3>系统架构</h3>

<p>Ceph 生态系统架构可以划分为四部分：</p>

<ul>
<li>client：客户端（数据用户）</li>
<li>mds：Metadata server cluster，元数据服务器（缓存和同步分布式元数据）</li>
<li>osd：Object storage cluster，对象存储集群（将数据和元数据作为对象存储，执行其他关键职能）</li>
<li>mon：Cluster monitors，集群监视器（执行监视功能）</li>
</ul>


<p><img src="http://b.hiphotos.baidu.com/baike/c0%3Dbaike80%2C5%2C5%2C80%2C26/sign=cfb8f11a0a7b020818c437b303b099b6/91ef76c6a7efce1b3ca81175af51f3deb58f658a.jpg" alt="Alt text" /></p>

<pre><code>                  图1 ceph生态架构图
</code></pre>

<h1>各组件浅析</h1>

<ul>
<li>各组件作用可参考
<a href="http://blog.csdn.net/jackjones_008/article/details/43303667">http://blog.csdn.net/jackjones_008/article/details/43303667</a></li>
<li>系列讲解和官方文档详解
<a href="http://docs.ceph.com/docs/master/start/intro/">http://docs.ceph.com/docs/master/start/intro/</a></li>
</ul>


<h3>Ceph客户端 client</h3>

<pre><code>   因为 Linux 显示文件系统的一个公共界面（通过虚拟文件系统交换机 [VFS]），Ceph 的用户透视图就是透明的。管理员的透视图肯定是不同的，考虑到很多服务器会包含存储系统这一潜在因素。从用户的角度看，他们访问大容量的存储系统，却不知道下面聚合成一个大容量的存储池的元数据服务器，监视器，还有独立的对象存储设备。用户只是简单地看到一个安装点，在这点上可以执行标准文件 I/O。Ceph 文件系统 — 或者至少是客户端接口 — 在 Linux 内核中实现。值得注意的是，在大多数文件系统中，所有的控制和智能在内核的文件系统源本身中执行。但是，在 Ceph 中，文件系统的智能分布在节点上，这简化了客户端接口，并为 Ceph 提供了大规模（甚至动态）扩展能力。Ceph 使用一个有趣的备选，而不是依赖分配列表（将磁盘上的块映射到指定文件的元数据）。Linux 透视图中的一个文件会分配到一个来自元数据服务器的 inode number（INO），对于文件这是一个唯一的标识符。然后文件被推入一些对象中（根据文件的大小）。使用 INO 和 object number（ONO），每个对象都分配到一个对象 ID（OID）。在 OID 上使用一个简单的哈希，每个对象都被分配到一个放置组。放置组（标识为 PGID）是一个对象的概念容器。最后，放置组到对象存储设备的映射是一个伪随机映射，使用一个叫做 Controlled Replication Under Scalable Hashing（CRUSH）的算法。这样一来，放置组（以及副本）到存储设备的映射就不用依赖任何元数据，而是依赖一个伪随机的映射函数。这种操作是理想的，因为它把存储的开销最小化，简化了分配和数据查询。分配的最后组件是集群映射。集群映射 是设备的有效表示，显示了存储集群。有了 PGID 和集群映射，您就可以定位任何对象。
</code></pre>

<h3>Ceph元数据服务器 mds</h3>

<pre><code>    元数据服务器（cmds）的工作就是管理文件系统的名称空间。虽然元数据和数据两者都存储在对象存储集群，但两者分别管理，支持可扩展性。事实上，元数据在一个元数据服务器集群上被进一步拆分，元数据服务器能够自适应地复制和分配名称空间，避免出现热点。如图 4 所示，元数据服务器管理名称空间部分，可以（为冗余和性能）进行重叠。元数据服务器到名称空间的映射在 Ceph 中使用动态子树逻辑分区执行，它允许 Ceph 对变化的工作负载进行调整（在元数据服务器之间迁移名称空间）同时保留性能的位置。


图 4. 元数据服务器的 Ceph 名称空间的分区
首页：元数据服务器的 Ceph 名称空间的分区
但是因为每个元数据服务器只是简单地管理客户端人口的名称空间，它的主要应用就是一个智能元数据缓存（因为实际的元数据最终存储在对象存储集群中）。进行写操作的元数据被缓存在一个短期的日志中，它最终还是被推入物理存储器中。这个动作允许元数据服务器将最近的元数据回馈给客户（这在元数据操作中很常见）。这个日志对故障恢复也很有用：如果元数据服务器发生故障，它的日志就会被重放，保证元数据安全存储在磁盘上。元数据服务器管理 inode 空间，将文件名转变为元数据。元数据服务器将文件名转变为索引节点，文件大小，和 Ceph 客户端用于文件 I/O 的分段数据（布局）。
</code></pre>

<h3>Ceph 监视器 mon</h3>

<pre><code>   Ceph 包含实施集群映射管理的监视器，但是故障管理的一些要素是在对象存储本身中执行的。当对象存储设备发生故障或者新设备添加时，监视器就检测和维护一个有效的集群映射。这个功能按一种分布的方式执行，这种方式中映射升级可以和当前的流量通信。Ceph 使用 Paxos，它是一系列分布式共识算法。
</code></pre>

<h3>Ceph对象存储 osd</h3>

<pre><code>    和传统的对象存储类似，Ceph 存储节点不仅包括存储，还包括智能。传统的驱动是只响应来自启动者的命令的简单目标。但是对象存储设备是智能设备，它能作为目标和启动者，支持与其他对象存储设备的通信和合作。从存储角度来看，Ceph 对象存储设备执行从对象到块的映射（在客户端的文件系统层中常常执行的任务）。这个动作允许本地实体以最佳方式决定怎样存储一个对象。Ceph 的早期版本在一个名为 EBOFS 的本地存储器上实现一个自定义低级文件系统。这个系统实现一个到底层存储的非标准接口，这个底层存储已针对对象语义和其他特性（例如对磁盘提交的异步通知）调优。今天，B-tree 文件系统（BTRFS）可以被用于存储节点，它已经实现了部分必要功能（例如嵌入式完整性）。因为 Ceph 客户实现 CRUSH，而且对磁盘上的文件映射块一无所知，下面的存储设备就能安全地管理对象到块的映射。这允许存储节点复制数据（当发现一个设备出现故障时）。分配故障恢复也允许存储系统扩展，因为故障检测和恢复跨生态系统分配。Ceph 称其为 RADOS
</code></pre>

<hr />

<h3>ceph工作原理浅析</h3>

<p>有待后续补充</p>

<hr />

<h3>ceph与HDFS对比浅析</h3>

<pre><code>   Ceph对比HDFS优势在于易扩展，无单点。HDFS是专门为Hadoop这样的云计算而生，在离线批量处理大数据上有先天的优势，而Ceph是一个通用的实时存储系统。虽然Hadoop可以利用Ceph作为存储后端，但整合过程不及HDFS轻便，且执行计算任务上性能还是略逊于HDFS
</code></pre>

<p><code>有待后续补充</code></p>

<h3>ceph安装部署</h3>

<pre><code>    部署可分为单节点部署和多节点部署，软件版本可从测试环境和生产环境来区分，具体部署过程可参考官方文档和一些博客，部署中遇到问题再详细分析。
</code></pre>

<ul>
<li>官方安装文档 <a href="http://docs.ceph.com/docs/master/start/">http://docs.ceph.com/docs/master/start/</a></li>
<li>手动安装文档  <a href="http://docs.ceph.com/docs/master/install/">http://docs.ceph.com/docs/master/install/</a></li>
<li>可参考博客<br/>
<a href="http://blog.csdn.net/i_chips/article/details/19985795">http://blog.csdn.net/i_chips/article/details/19985795</a>
<a href="http://blog.csdn.net/pc620/article/details/9002045">http://blog.csdn.net/pc620/article/details/9002045</a></li>
</ul>


<p><code>详细部署步骤可参考《ceph部署实验过程》文档</code></p>

<h3>ceph整体架构分析工作原理分析</h3>

<p><code>有待后续补充</code></p>

<h3>ceph API接口分析</h3>

<p><code>有待后续补充</code></p>

<h3>ceph 开发解析</h3>

<p><code>有待后续补充</code></p>

<h3>硬件配置推荐</h3>

<ul>
<li><p>小型测试集群最低配置推荐</p>

<p> Ceph可以运行在廉价的普通硬件上，小型生产集群和开发集群可以在一般的硬件上。</p></li>
<li><table>
<thead>
<tr>
<th>组件 </th>
<th> 标准 </th>
<th> 最低配置推荐</th>
</tr>
</thead>
<tbody>
<tr>
<td>ceph-osd     </td>
<td> 处理器          </td>
<td> 1X 64位AMD-64/i386的双核</td>
</tr>
<tr>
<td></td>
<td> RAM           </td>
<td> 每个守护进程需要500 MB</td>
</tr>
<tr>
<td></td>
<td>磁盘空间</td>
<td>1x storage drive per daemon</td>
</tr>
<tr>
<td></td>
<td>网络</td>
<td>2个1GB以太网网卡</td>
</tr>
<tr>
<td>ceph-mds         </td>
<td> 处理器          </td>
<td> 1X 64位AMD-64/i386的双核</td>
</tr>
<tr>
<td></td>
<td> RAM           </td>
<td> 每个守护进程最低1GB</td>
</tr>
<tr>
<td></td>
<td>磁盘空间</td>
<td>每个守护进程1MB</td>
</tr>
<tr>
<td></td>
<td>网络</td>
<td>2个1GB以太网网卡</td>
</tr>
<tr>
<td>ceph-mon     </td>
<td> 处理器          </td>
<td> 1X 64位AMD-64/i386的双核</td>
</tr>
<tr>
<td></td>
<td> RAM           </td>
<td> 每个守护进程最低1GB</td>
</tr>
<tr>
<td></td>
<td>磁盘空间</td>
<td>每个守护进程10GB</td>
</tr>
<tr>
<td></td>
<td>网络</td>
<td>2个1GB以太网网卡</td>
</tr>
</tbody>
</table>
</li>
<li><p>生产集群最低标准配置推荐</p>

<p>PB级生产集群也可以使用普通硬件，但应该配备更多内存、CPU和数据存储空间来解决流量压力。一个2012年的Ceph集群项目使用了两个相当强悍的OSD硬件配置，和稍逊的监视器配置。</p></li>
<li><table>
<thead>
<tr>
<th>组件 </th>
<th> 标准 </th>
<th> 最低配置推荐</th>
</tr>
</thead>
<tbody>
<tr>
<td>戴尔PE R510   </td>
<td>处理器      </td>
<td>2个64位四核Xeon处理器</td>
</tr>
<tr>
<td></td>
<td>RAM</td>
<td>   16 GB</td>
</tr>
<tr>
<td></td>
<td>磁盘空间   </td>
<td>8个2TB驱动器。1个操作系统，7个存储</td>
</tr>
<tr>
<td></td>
<td>客户端网络</td>
<td>       2个1GB以太网网卡</td>
</tr>
<tr>
<td></td>
<td>OSD网络</td>
<td> 2个1GB以太网网卡</td>
</tr>
<tr>
<td></td>
<td>管理网络   </td>
<td>   2个1GB以太网网卡</td>
</tr>
<tr>
<td>戴尔PE R515</td>
<td>      处理器   </td>
<td>   1X 16核 Opteron CPU</td>
</tr>
<tr>
<td></td>
<td>RAM    </td>
<td>16GB</td>
</tr>
<tr>
<td></td>
<td>os存储   </td>
<td>1X 500GB硬盘的操作系统。</td>
</tr>
<tr>
<td></td>
<td>磁盘空间   </td>
<td>12X 3TB硬盘的存储</td>
</tr>
<tr>
<td></td>
<td>客户端网络</td>
<td>       2个1GB以太网网卡</td>
</tr>
<tr>
<td></td>
<td>OSD网络  </td>
<td>2个1GB以太网网卡</td>
</tr>
<tr>
<td></td>
<td>管理网络   </td>
<td>   2个1GB以太网网卡</td>
</tr>
</tbody>
</table>
</li>
</ul>


<h3>操作系统推荐</h3>

<p>Linux内核</p>

<ul>
<li>Ceph的核心客户端目前建议：

<ol>
<li>v4.1.4.或更高版本</li>
<li>v3.16.3或更高版本</li>
</ol>
</li>
<li>btrfs文件系统： B-tree File System (Btrfs)如果想在btrfs上运行Ceph，我们推荐使用一个最新的Linux内核（V3.14或更高版本）</li>
</ul>


<p>系统平台
       下面的表格展示了Ceph需求和各种Linux发行版的对应关系。一般来说，Ceph对内核和系统初始化阶段的依赖很少（如sysvinit，upstart, systemd）。</p>

<ul>
<li><table>
<thead>
<tr>
<th>Distro    </th>
<th>Release    </th>
<th>Code Name  </th>
<th>Kernel </th>
<th>Notes  </th>
<th>Testing</th>
</tr>
</thead>
<tbody>
<tr>
<td>CentOS      </td>
<td>7  </td>
<td>N/A</td>
<td>   linux-3.10.0    </td>
<td>   </td>
<td>B, I, C</td>
</tr>
<tr>
<td>Debian  </td>
<td>8.0    </td>
<td>Jessie</td>
<td>    linux-3.16.0</td>
<td>   </td>
<td>   B, I</td>
</tr>
<tr>
<td>Fedora</td>
<td> 22  </td>
<td>N/A    </td>
<td>linux-3.14.0   </td>
<td></td>
<td>  B, I</td>
</tr>
<tr>
<td>RHEL    </td>
<td>7  </td>
<td>Maipo  </td>
<td>linux-3.10.0   </td>
<td></td>
<td>  B, I</td>
</tr>
<tr>
<td>Ubuntu  </td>
<td>14.04  </td>
<td>Trusty Tahr    </td>
<td>linux-3.13.0</td>
<td></td>
<td> B, I, C</td>
</tr>
</tbody>
</table>
</li>
<li><p>详细操作系统推荐可查询</p></li>
</ul>


<p> <a href="http://docs.ceph.com/docs/master/start/os-recommendations/">http://docs.ceph.com/docs/master/start/os-recommendations/</a></p>

<h2>- 附注</h2>

<ol>
<li>默认内核btrfs版本较老，不推荐用于Ceph-osd存储节点；要升级到推荐的内核，或者改用xfs、ext4。</li>
<li>默认内核带的Ceph客户端较老，不推荐做内核空间客户端（内核RBD或Ceph文件系统），请升级到推荐内核。</li>
<li>已安装的glibc版本不支持syncfs（2）系统调用，同一台机器上使用xfs或ext4的Ceph-osd守护进程性能一般，它可以更好。</li>
</ol>


<h2>- 测试</h2>

<ol>
<li>B：我们持续地在这个平台上编译所有分支、做基本单元测试；也为这个平台构建可发布软件包。</li>
<li>I： 我们在这个平台上做基本的安装和功能测试。</li>
<li>C：我们在这个平台上持续地做全面的功能、退化、压力测试，包括开发分支、预发布版本、正式发布版本。</li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[ning]]></title>
    <link href="http://guangningsun.github.io/blog/2016/01/22/ning/"/>
    <updated>2016-01-22T10:39:50+08:00</updated>
    <id>http://guangningsun.github.io/blog/2016/01/22/ning</id>
    <content type="html"><![CDATA[<p><meta http-equiv="Content-Type" content="text/html; charset=utf-8"></p>

<h2>单点安装部署ceph实验过程</h2>

<p><strong>1.官方推荐Ubuntu12.0.4服务器版本操作系统。</strong></p>

<blockquote><p>下载地址
<a href="http://releases.ubuntu.com/12.04/ubuntu-12.04.5-alternate-amd64.iso.torrent">http://releases.ubuntu.com/12.04/ubuntu-12.04.5-alternate-amd64.iso.torrent</a></p></blockquote>

<p><strong>2.更新系统source</strong></p>

<blockquote><ul>
<li>命令：sudo apt-get update 若网速不好 ，最好更换镜像 方法1.直接下载镜像的sources.list
方法2.手动修改sources.list<br/>
针对方法一 以163镜像为例，Ubuntu12.04 cd /etc/apt/
备份原有list</li>
<li>命令：mv sources.list sources.list.bak
curl -0 <a href="http://mirrors.163.com/.help/sources.list.precise">http://mirrors.163.com/.help/sources.list.precise</a>
下载新的source.list</li>
<li>命令：wget <a href="http://mirrors.163.com/.help/sources.list.precise">http://mirrors.163.com/.help/sources.list.precise</a>
应用source.list.precise</li>
<li>命令：mv source.list.precise source/list</li>
</ul>
</blockquote>

<p><strong>3.查看防火墙状态</strong></p>

<blockquote><ul>
<li>命令 ：sudo ufw status 若是开启状态，需关闭防火墙</li>
<li>命令：sudo ufw disable</li>
</ul>
</blockquote>

<p><strong>4.查看系统时间是否正确</strong></p>

<blockquote><ul>
<li>命令：sudo date -s “2016-01-20 11:38:50”</li>
<li>命令：sudo hwclock -w 写入硬件时间</li>
</ul>
</blockquote>

<p><strong>5.安装ceph</strong></p>

<blockquote><p>ubuntu12.0.4默认ceph为0.4版本，但当并没有发现安装痕迹</p>

<ul>
<li>命令：sudo apt-get install ceph ceph-common ceph-mds</li>
<li>命令：ceph  -v 查看ceph版本信息</li>
</ul>
</blockquote>

<p><strong>6.创建 ceph配置文件</strong></p>

<blockquote><ul>
<li>命令：vi /etc/ceph/ceph.conf</li>
</ul>
</blockquote>

<p>配置文件内容</p>

<blockquote><p>[global]
    max open files = 131072
    auth cluster required = none
    auth client required = none</p>

<p> [osd] <br/>
      osd journal size = 1000  <br/>
       filestore xattruse omap = true<br/>
       osd mkfs options xfs = -f      osd mount
options xfs = rw,noatime</p>

<p>[mon.a]   <br/>
host = ceph1
mon addr = 192.168.73.129:6789</p>

<p>[osd.0]   <br/>
     host = ceph1   <br/>
     devs = /dev/sdb1</p>

<p>[osd.1]
    host= ceph1
    devs= /dev/sdb2</p>

<p>  [mds.a]
    host= ceph1</p></blockquote>

<p>注意:对于较低的Ceph版本（例如0.42），需要在[mon]项下添加一行内容:mondata=/data/name,以及在[osd]项下添加一行内容：osd data = /data/$name，需要新建/data目录以作为后续的数据目录；相应的，后续针对数据目录的步骤也需要调整。</p>

<p><strong>7.创建数据目录</strong></p>

<blockquote><p>安装分区工具 命令: apt-get install xfsprogs 创建数据目录命令：
mkdir -p /var/lib/ceph/osd/ceph-0
mkdir -p /var/lib/ceph/osd/ceph-1
mkdir -p /var/lib/ceph/mon/ceph-a
mkdir -p /var/lib/ceph/mds/ceph-a</p></blockquote>

<p><strong>8.为osd创建分区和进行挂载</strong></p>

<blockquote><p>对新分区进行xfs或者btrfs的格式化名称根据实际情况来命名，实验机为sda3和sda4
mkfs.xfs -f /dev/sdb1
mkfs.xfs -f /dev/sdb2<br/>
自动挂载分区到指定路径 第一次必须先挂载分区写入初始化数据：
mount /dev/sdb1 /var/lib/ceph/osd/ceph-0
mount /dev/sdb2 /var/lib/ceph/osd/ceph-1</p></blockquote>

<p><strong>9.执行初始化</strong></p>

<blockquote><p>执行初始化前，都先停止Ceph服务，并清空原有数据目录：</p>

<ul>
<li>/etc/init.d/ceph stop</li>
<li>rm -rf /var/lib/ceph/<em>/ceph-</em>/*<br/>
然后，就在mon所在的节点上执行初始化了：</li>
<li><p> sudo mkcephfs -a -c /etc/ceph/ceph.conf -k /etc/ceph/ceph1.keyring</p></li>
<li><p> 注意，一旦配置文件ceph.conf发生改变，初始化最好重新执行一遍。</p></li>
</ul>
</blockquote>

<p><strong>10.启动ceph</strong></p>

<blockquote><p>在mon所在的节点上执行：</p>

<ul>
<li>sudo service ceph -a start<br/>
注意，执行上面这步时，可能会遇到如下提示：
=== osd.0 ===  Mounting xfs onceph4:/var/lib/ceph/osd/ceph-0  Error ENOENT: osd.0 does not exist.  create it before updating the crush map
执行如下命令后，再重复执行上面那条启动服务的命令，就可以解决：</li>
<li>ceph osd create</li>
</ul>
</blockquote>

<p><strong>11.执行健康检查</strong></p>

<blockquote><ul>
<li>命令：sudo ceph health
也可以使用ceph -s命令查看状态 <br/>
如果返回的是HEALTH_OK，则代表成功！ <br/>
如果遇到如下提示： <br/>
HEALTH_WARN 384 pgs degraded; 384 pgs stuck unclean;
recovery 21/42degraded (50.000%) <br/>
则说明osd数量不够，Ceph默认至少需要提供两个osd。</li>
</ul>
</blockquote>

<hr />
]]></content>
  </entry>
  
</feed>
