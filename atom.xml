<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Adolph孙广宁的博客]]></title>
  <link href="http://guangningsun.github.io/atom.xml" rel="self"/>
  <link href="http://guangningsun.github.io/"/>
  <updated>2016-04-29T14:43:37+08:00</updated>
  <id>http://guangningsun.github.io/</id>
  <author>
    <name><![CDATA[Guangning Sun]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[puppet cookbook]]></title>
    <link href="http://guangningsun.github.io/blog/2016/04/29/puppet-cookbook/"/>
    <updated>2016-04-29T13:15:24+08:00</updated>
    <id>http://guangningsun.github.io/blog/2016/04/29/puppet-cookbook</id>
    <content type="html"><![CDATA[<ul>
<li>puppet 代表一种代码语言</li>
<li>puppet 代表一种对集群基础设施进行管理的平台</li>
</ul>


<h3>第一章 puppet安装总结</h3>

<hr />

<h4>第一节 工作原理</h4>

<p><img src="http://guangningsun.github.io/images/puppet-arc.png" alt="" /></p>

<pre><code>    Puppet是开源的基于Ruby的系统配置管理工具，依赖于C/S的部署架构。puppet使用跨平台语言规
范，管理配置文件、用户、软件包、系统服务等内容，在puppet里这些内容都被看做是“资源”，每种
资源都有对应的属性，如软件包有安装不安装的属性，文件有权限属性等。Puppet的代码主要由这些
资源和其属性组成。其代码化的好处：分享，保存，快速的恢复和部署。

    Puppet客户端默认每半小时连接一次服务器端（此时puppet客户端进程工作在后台模式，也可根据需
要手动执行），从服务器端下载最新的配置文件,并且严格按照配置文件来配置服务器. 配置完成以
后,puppet客户端可以反馈给服务器端一个消息.         
</code></pre>

<p><img src="http://guangningsun.github.io/images/puppet-flow.jpg" alt="" /></p>

<pre><code>1. 客户端puppetd向master发起认证请求。或使用带签名的证书。

2. master告诉client你是合法的。

3. 客户端puppetd调用facter，facter探测出主机的一些变量，例如主机名，内存大小
</code></pre>

<p>，IP地址等。pupppetd 把这些信息通过ssl连接发送到服务器端；</p>

<pre><code>4. 服务器端的puppetmaster 检测客户端的主机名，然后找到manifest里面对应的node
</code></pre>

<p>配置，并对该部分内容进行解析，facter送过来的信息可以作为变量处理，node牵涉到的代</p>

<p>码才解析，其他没牵涉的代码不解析。解析分为几个阶段，语法检查，如果语法错误就报错</p>

<p>。如果语法没错，就继续解析，解析的结果生成一个中间的“伪代码”(catelog)，然后把伪</p>

<p>代码发给客户端.</p>

<pre><code>5. 客户端接收到“伪代码”，并且执行。

6. 客户端在执行时判断有没有file文件，如果有向fileserver 发起请求。

7. 客户端判断有没有配置report。如果配置把执行结果发送给服务器。

8. 服务器端把客户端的执行结果写入日志。并可以发送给报告系统(DashBoard) 
</code></pre>

<hr />

<h4>第二节 准备工作</h4>

<ul>
<li>关闭防火墙</li>
<li>NTP同步</li>
<li>更改hosts</li>
<li>安装ruby环境</li>
<li>下载EPEL对应的版本和安装</li>
<li>配置yum源，或者将puppet server和puppet client的rpm包下载</li>
</ul>


<p>以下将讲述详细操作步骤</p>

<h6>2.1 关闭selinux</h6>

<pre><code>setenforce 0
sed -i 's/SELINUX=enforcing/SELINUX=disabled/' /etc/selinux/config  
</code></pre>

<h6>2.2 NTP同步</h6>

<pre><code>ntpdate 0.rhel.pool.ntp.org
crontab -e
0 0 * * * /usr/sbin/ntpdate 0.rhel.pool.ntp.org;/usr/sbin/hwclock -w
</code></pre>

<h6>2.3 添加hosts</h6>

<p>vi /etc/hosts</p>

<pre><code>    192.168.28.148 cookbook3.localdomain.com cookbook3

    192.168.28.149 cookbook4.localdomain.com cookbook4
</code></pre>

<h6>2.4 puppet</h6>

<p><a href="http://downloads.puppetlabs.com/puppet/">http://downloads.puppetlabs.com/puppet/</a></p>

<h6>2.5 facter</h6>

<p><a href="http://downloads.puppetlabs.com/facter/">http://downloads.puppetlabs.com/facter/</a></p>

<h6>2.6 ruby</h6>

<p><a href="https://www.ruby-lang.org/zh_cn/downloads/">https://www.ruby-lang.org/zh_cn/downloads/</a></p>

<h6>2.7 配置CentOS YUM源</h6>

<p>cd /etc/yum.repos.d/</p>

<p>cp Centos-Base.repo Centos-Base.repo.bak</p>

<p>vi Centos-Base.repo</p>

<pre><code>    [base]
    name=CentOS-$releasever-Base
    baseurl=http://centos.ustc.edu.cn/centos/6/os/x86_64/
    gpgcheck=1
    gpgkey=http://mirrors.163.com/centos/RPM-GPG-KEY-CentOS-6

    [updates]
    name=CentOS-$releasever-Updates
    baseurl=http://centos.ustc.edu.cn/centos/6/os/x86_64/
    gpgcheck=1
    gpgkey=http://mirrors.163.com/centos/RPM-GPG-KEY-CentOS-6

    [extras]
    name=CentOS-$releasever-Extras
    baseurl=http://centos.ustc.edu.cn/centos/6/os/x86_64/
    gpgcheck=1
    gpgkey=http://mirrors.163.com/centos/RPM-GPG-KEY-CentOS-6

    [centosplus]
    name=CentOS-$releasever-Plus
    baseurl=http://centos.ustc.edu.cn/centos/6/os/x86_64/
    gpgcheck=1
</code></pre>

<p>yum clean all</p>

<p>yum makecache</p>

<p>yum repolist</p>

<hr />

<h4>第三节 Puppet Server安装</h4>

<ul>
<li>现已经将puppet server 所需的rpm包全部下载</li>
<li>如需安装只需在 rpm安装包目录下执行 rpm -Uvh * 命令即可</li>
<li>安装版本为puppet 3.8</li>
</ul>


<h6>在网络畅通的情况下也可以按照以下步骤执行</h6>

<h6>3.1 Enable the Puppet Labs Package Repository</h6>

<p>rpm -ivh <a href="https://yum.puppetlabs.com/puppetlabs-release-el-6.noarch.rpm">https://yum.puppetlabs.com/puppetlabs-release-el-6.noarch.rpm</a></p>

<h6>3.2 安装yum-plugin-downloadonly插件</h6>

<p>yum install -y yum-plugin-downloadonly</p>

<h6>3.3 安装puppet server</h6>

<p>yum install puppetserver</p>

<p>yum install &ndash;downloadonly &ndash;downloaddir=/tmp/puppetserver puppetserver</p>

<pre><code>    Total download size: 73 M
    Is this ok [y/N]: y
    Downloading Packages:
    (1/22): augeas-libs-1.0.0-10.el6.x86_64.rpm          
    (2/22): compat-readline5-5.2-17.1.el6.x86_64.rpm     
    (3/22): facter-2.4.6-1.el6.x86_64.rpm                
    (4/22): hiera-1.3.4-1.el6.noarch.rpm                 
    (5/22): java-1.7.0-openjdk-1.7.0.79-2.5.5.4.el6.x86_6
    (6/22): libjpeg-turbo-1.2.1-3.el6_5.x86_64.rpm       
    (7/22): libselinux-2.0.94-5.8.el6.i686.rpm           
    (8/22): libselinux-2.0.94-5.8.el6.x86_64.rpm         
    (9/22): libselinux-devel-2.0.94-5.8.el6.x86_64.rpm   
    (10/22): libselinux-python-2.0.94-5.8.el6.x86_64.rpm 
    (11/22): libselinux-ruby-2.0.94-5.8.el6.x86_64.rpm   
    (12/22): libselinux-utils-2.0.94-5.8.el6.x86_64.rpm  
    (13/22): puppet-3.8.6-1.el6.noarch.rpm               
    (14/22): puppetserver-1.1.3-1.el6.noarch.rpm         
    (15/22): ruby-1.8.7.374-4.el6_6.x86_64.rpm           
    (16/22): ruby-augeas-0.4.1-3.el6.x86_64.rpm          
    (17/22): ruby-irb-1.8.7.374-4.el6_6.x86_64.rpm       
    (18/22): ruby-libs-1.8.7.374-4.el6_6.x86_64.rpm      
    (19/22): ruby-rdoc-1.8.7.374-4.el6_6.x86_64.rpm      
    (20/22): ruby-shadow-2.2.0-2.el6.x86_64.rpm          
    (21/22): rubygem-json-1.5.5-3.el6.x86_64.rpm         
    (22/22): rubygems-1.3.7-5.el6.noarch.rpm             

    cookbook3@/tmp/puppetserver#rpm -Uvh *
    warning: augeas-libs-1.0.0-10.el6.x86_64.rpm: Header V3 RSA/SHA1 Signature, key ID c105b9de: NOKEY
    warning: facter-2.4.6-1.el6.x86_64.rpm: Header V4 RSA/SHA1 Signature, key ID 4bd6ec30: NOKEY

Preparing...                 ########################################### [100%]
    1:libselinux             ###########################################
    2:augeas-libs            ########################################### [  9%]
    3:libselinux-ruby        ########################################### [ 14%]
    4:libselinux-utils       ########################################### [ 18%]
    5:libjpeg-turbo          ########################################### [ 23%]
    6:java-1.7.0-openjdk     ########################################### [ 27%]
    7:compat-readline5       ########################################### [ 32%]
    8:ruby-libs              ########################################### [ 36%]
    9:ruby                   ########################################### [ 41%]
   10:facter                 ########################################### [ 45%]
   11:ruby-irb               ########################################### [ 50%]
   12:ruby-rdoc              ########################################### [ 55%]
   13:rubygems               ########################################### [ 59%]
   14:rubygem-json           ########################################### [ 64%]
   15:hiera                  ########################################### [ 68%]
   16:ruby-shadow            ########################################### [ 73%]
   17:ruby-augeas            ########################################### [ 77%]
   18:puppet                 ########################################### [ 82%]
   19:puppetserver           ########################################### [ 86%]
   20:libselinux-devel       ########################################### [ 91%]
   21:libselinux-python      ########################################### [ 95%]
   22:libselinux             ########################################### [100%]
</code></pre>

<h6>3.4 验证puppetserver</h6>

<p>cookbook3@/tmp/puppetserver#yum install puppetserver</p>

<pre><code>Loaded plugins: downloadonly, product-id, refresh-packagekit, security, subscription-manager
Updating certificate-based repositories.
Unable to read consumer identity
Setting up Install Process
Package puppetserver-1.1.3-1.el6.noarch already installed and latest version
Nothing to do
</code></pre>

<h6>3.5 增加puppet master配置信息</h6>

<p>vi /etc/puppet/puppet.conf</p>

<pre><code> [main]
     # master的主机名
     server = cookbook3.localdomain.com
     # master的主机名
     certname = cookbook3.localdomain.com
     #禁用插件同步
     pluginsync = false
     # The Puppet log directory.
     # The default value is '$vardir/log'.
     logdir = /var/log/puppet

     # Where Puppet PID files are kept.
     # The default value is '$vardir/run'.
     rundir = /var/run/puppet

     # Where SSL certificates are kept.
     # The default value is '$confdir/ssl'.
     ssldir = $vardir/ssl

 [agent]
     # The file in which puppetd stores a list of the classes
     # associated with the retrieved configuratiion.  Can be loaded in
     # the separate ``puppet`` executable using the ``--loadclasses``
     # option.
     # The default value is '$confdir/classes.txt'.
     classfile = $vardir/classes.txt

     # Where puppetd caches the local configuration.  An
     # extension indicating the cache format is added automatically.
     # The default value is '$confdir/localconfig'.
     localconfig = $vardir/localconfig
</code></pre>

<h6>3.6 启动puppet</h6>

<p>puppet master</p>

<pre><code>    ps -ef |grep master
    puppet    3077     1  0 10:09 ?        00:00:00 /usr/bin/ruby /usr/bin/puppet master
    root      3081  3015  0 10:09 pts/0    00:00:00 grep master
</code></pre>

<hr />

<h4>第四节 puppet client安装</h4>

<ul>
<li>现已经将puppet server 所需的rpm包全部下载</li>
<li>如需安装只需在 rpm安装包目录下执行 rpm -Uvh * 命令即可</li>
<li>安装版本为puppet 3.8</li>
</ul>


<h6>4.1 Enable the Puppet Labs Package Repository</h6>

<pre><code>rpm -ivh https://yum.puppetlabs.com/puppetlabs-release-el-6.noarch.rpm
</code></pre>

<h6>4.2 安装yum-plugin-downloadonly插件</h6>

<pre><code>yum install -y yum-plugin-downloadonly
</code></pre>

<h6>4.3 yum install puppet</h6>

<pre><code>yum install --downloadonly --downloaddir=/tmp/puppet puppet

Downloading Packages:
(1/19): augeas-libs-1.0.0-10.el6.x86_64.rpm          
(2/19): compat-readline5-5.2-17.1.el6.x86_64.rpm     
(3/19): facter-2.4.6-1.el6.x86_64.rpm                
(4/19): hiera-1.3.4-1.el6.noarch.rpm                 
(5/19): libselinux-2.0.94-5.8.el6.i686.rpm           
(6/19): libselinux-2.0.94-5.8.el6.x86_64.rpm         
(7/19): libselinux-devel-2.0.94-5.8.el6.x86_64.rpm   
(8/19): libselinux-python-2.0.94-5.8.el6.x86_64.rpm  
(9/19): libselinux-ruby-2.0.94-5.8.el6.x86_64.rpm    
(10/19): libselinux-utils-2.0.94-5.8.el6.x86_64.rpm  
(11/19): puppet-3.8.6-1.el6.noarch.rpm               
(12/19): ruby-1.8.7.374-4.el6_6.x86_64.rpm           
(13/19): ruby-augeas-0.4.1-3.el6.x86_64.rpm          
(14/19): ruby-irb-1.8.7.374-4.el6_6.x86_64.rpm       
(15/19): ruby-libs-1.8.7.374-4.el6_6.x86_64.rpm      
(16/19): ruby-rdoc-1.8.7.374-4.el6_6.x86_64.rpm      
(17/19): ruby-shadow-2.2.0-2.el6.x86_64.rpm          
(18/19): rubygem-json-1.5.5-3.el6.x86_64.rpm         
(19/19): rubygems-1.3.7-5.el6.noarch.rpm             
</code></pre>

<h6>4.4 离线安装</h6>

<p>cookbook4@/tmp/puppet#rpm -Uvh *</p>

<pre><code>warning: facter-2.4.6-1.el6.x86_64.rpm: Header V4 RSA/SHA1 Signature, key ID 4bd6ec30: NOKEY
Preparing...                ########################################### [100%]
   1:libselinux             ########################################### [  5%]
   2:augeas-libs            ########################################### [ 11%]
   3:libselinux-ruby        ########################################### [ 16%]
   4:libselinux-utils       ########################################### [ 21%]
   5:compat-readline5       ########################################### [ 26%]
   6:ruby-libs              ########################################### [ 32%]
   7:ruby                   ########################################### [ 37%]
   8:facter                 ########################################### [ 42%]
   9:ruby-irb               ########################################### [ 47%]
  10:ruby-rdoc              ########################################### [ 53%]
  11:rubygems               ########################################### [ 58%]
  12:rubygem-json           ########################################### [ 63%]
  13:hiera                  ########################################### [ 68%]
  14:ruby-shadow            ########################################### [ 74%]
  15:ruby-augeas            ########################################### [ 79%]
  16:puppet                 ########################################### [ 84%]
  17:libselinux-devel       ########################################### [ 89%]
  18:libselinux-python      ########################################### [ 95%]
  19:libselinux             ########################################### [100%]
</code></pre>

<h6>4.5 验证puppet</h6>

<pre><code>cookbook4@/tmp/puppet#yum install puppet

Loaded plugins: fastestmirror, product-id, refresh-packagekit, security,    subscription-manager
Updating certificate-based repositories.
Unable to read consumer identity
Setting up Install Process
Loading mirror speeds from cached hostfile
Package puppet-3.8.6-1.el6.noarch already installed and latest version
Nothing to do
</code></pre>

<h6>4.6 增加puppet client配置信息</h6>

<p>vi /etc/puppet/puppet.conf</p>

<pre><code>[main]
    # master的主机名
    server = cookbook3.localdomain.com
    # 禁用插件同步
    pluginsync = false
    # The Puppet log directory.
    # The default value is '$vardir/log'.
    logdir = /var/log/puppet

    # Where Puppet PID files are kept.
    # The default value is '$vardir/run'.
    rundir = /var/run/puppet

    # Where SSL certificates are kept.
    # The default value is '$confdir/ssl'.
    ssldir = $vardir/ssl

[agent]
    # The file in which puppetd stores a list of the classes
    # associated with the retrieved configuratiion.  Can be loaded in
    # the separate ``puppet`` executable using the ``--loadclasses``
    # option.
    # The default value is '$confdir/classes.txt'.
    classfile = $vardir/classes.txt

    # Where puppetd caches the local configuration.  An
    # extension indicating the cache format is added automatically.
    # The default value is '$confdir/localconfig'.
    localconfig = $vardir/localconfig
</code></pre>

<hr />

<h4>第五节 puppet client证书认证</h4>

<h6>5.1 client agent连接server</h6>

<pre><code>puppet agent --server=cookbook3.localdomain.com 或者

puppet agent -t --server=cookbook3.localdomain.com
</code></pre>

<h6>5.2 在master上查看申请证书请求</h6>

<pre><code>puppet cert --list

"cookbook4.localdomain.com" (SHA256) BC:1B:42:88:B0:A4:F0:F2:81:56:5D:0E:7A:49:90:83:79:F2:41:A5:E3:12:FC:E2:F2:DB:DE:30:8E:DB:0D:D0
</code></pre>

<h6>5.3 在master上签发证书</h6>

<pre><code>puppet cert sign --all

puppet cert --sign cookbook4.localdomain.com

Notice: Signed certificate request for client
Notice: Removing file Puppet::SSL::CertificateRequest client at '/var/lib/puppet/ssl/ca/requests/client.pem'
</code></pre>

<h6>5.4 查看证书，"+&ldquo;表示已经签名成功</h6>

<pre><code>puppet cert -all

+ "cookbook4.localdomain.com" (SHA256) BC:1B:42:88:B0:A4:F0:F2:81:56:5D:0E:7A:49:90:83:79:F2:41:A5:E3:12:FC:E2:F2:DB:DE:30:8E:DB:0D:D0
+ "cookbook3.localdomain.com" (SHA256) 04:45:21:0B:B5:5E:41:AE:9C:F4:B4:6B:EC:2F:AA:D2:BE:46:33:3E:B1:0E:85:2E:3C:B7:6B:98:95:A8:CE:4D
</code></pre>

<hr />

<h4>第六节 puppet同步测试</h4>

<h6>6.1 在master上创建一个site.pp文件</h6>

<pre><code>vi /etc/puppet/manifests/site.pp
</code></pre>

<ul>
<li>以下内容格式采用puppet 编码格式，在下一章节可以了解到puppet语言的风格及写法</li>
</ul>


<p>node default { file { &ldquo;/tmp/test.txt&rdquo;: content => &ldquo;Hello, First puppet test!&rdquo;} }</p>

<h6>6.2 在client机进行验证，如果/tmp/test.txt文件生成并有内容，则说明功能正常。</h6>

<p>puppet agent &ndash;test</p>

<pre><code>Info: Caching catalog for client
Info: Applying configuration version '1458888162'
Notice: /Stage[main]/Main/Node[default]/File[/tmp/test.txt]/ensure: defined content as '{md5}390b4c389233b9ae38a84ff8c731a8a1'
Info: Creating state file /var/lib/puppet/state/state.yaml
Notice: Finished catalog run in 0.03 seconds
</code></pre>

<hr />

<h3>第二章 puppet 语言和风格</h3>

<hr />

<pre><code>    配置清单：puppet用于配置服务器的主程序代码被称为配置清单
</code></pre>

<h4>第一节 社区推荐的puppet风格</h4>

<hr />

<h6>1.1 缩进</h6>

<pre><code>    配置清单中使用两个空格进行缩进（不适用Tab）参考一下代码
    node  ‘monitoring’  inherits  ‘server’  {
      include  icinga::server
      include  repo::apt
    }
</code></pre>

<h6>1.2 引号</h6>

<pre><code>    --资源的名字始终使用引号注明，如下：
    file  {  '/tmp/test.txt':
    而不是
    file  {  /tmp/test.txt:
    所有字符串使用单引号，除非
        * 字符串中有变量（如${name}）。
        * 字符串中包含了转义字符（如 \n）。
    在上述情况下应该使用双引号，否则puppet不会处理。
    --在puppet中始终要讲非保留字的参数值用单引号引起来，如
    name  =&gt;  'ning',
    mode  =&gt;  '0700',
    owner  =&gt;  'root',
    但是，如下保留字的参数值则不需要引号
    ensure  =&gt;  installed,
    enable  =&gt;  ture,
    ensure  =&gt;  running,
</code></pre>

<h6>1.3 变量</h6>

<pre><code>    --当字符串引入变量时，要确保他们被大括{}号括起来，如
    source  =&gt;  "puppet:///modules/weget/${brand}.conf",
    否则puppet语法解析器无法解析变量，

    --///表示该路径在根目录下 且已经忽略了file文件夹 
</code></pre>

<h6>1.4 参数</h6>

<pre><code>    --在参数行末尾要加逗号，最后一行也要添加,方便以后用户添加和调整参数
    service  {'memcached':
      ensure  =&gt;  running,
      enable  =&gt;  true,
    }
    --当声明仅仅带有一个参数的资源时，要把全部声明写在一行，并且不要在末尾添加逗号
    package  {  'puppet':ensure  =&gt;  installed}
    --当声明多个参数时，需要让每一个参数各占据一行。
    package  {  ‘rake’:
      ensure    =&gt;  installed,
      provider  =&gt;  gem,
      require   =&gt;  Package['rubygem'],   
    }
    --所有参数的箭头都要与最长参数名所在行箭头对齐，如上图所示，方便剪切。
</code></pre>

<h6>1.5符号链接</h6>

<pre><code>    当声明的file资源是符号链接这种类型时，要设置ensure =&gt; link 及 target 属性
    file  {  '/etc/php5/cli/php.ini':
      ensure  =&gt;  link,
      target  =&gt;  '/etc/php.ini',       
    }
</code></pre>

<hr />

<h4>第二节 使用puppet-lint检查配置清单（代码）</h4>

<p>代码主要风格如下</p>

<ul>
<li>必须使用两个空格，不能使用软跳格TAB</li>
<li>不得使用文字制表符</li>
<li>不得包含尾随空格</li>
<li>每行代码最好不得超过80个字符（检查会出错但是不影响使用）</li>
<li><p>代码块中的参数箭头=>需要对齐(主要是风格统一，实际没有影响)</p>

<p>  使用puppet-link来检查代码风格正确性
  1、安装 gem install puppet-link
  2、直接 使用命令 puppet-link site.pp检查site.pp文件的风格正确性</p></li>
</ul>


<p>正确时不会有输出</p>

<p>错误时会有提醒</p>

<p>如果不希望检查一行代码是否超出了80字符</p>

<p>可以使用命令
puppet-lint  &ndash;no-80chars-check</p>

<p>更多风格检查可以查看
<a href="http://puppet-link.com">http://puppet-link.com</a></p>

<hr />

<h4>第三节 使用模块</h4>

<blockquote><p>能够使puppet配置清单清晰并且易于维护的最重要方式就是将他们组织成模块。
下面创建一个模块来管理memcached</p></blockquote>

<ol>
<li><p>在puppet代码仓库中创建以下目录</p>

<pre><code> mkdir modules/memcached
 mkdir modules/memcached/manifests
 mkdir modueles/memcached/files
</code></pre></li>
<li><p>根据如下内容创建modules/memcached/manifests/init.pp 文件：</p>

<pre><code> class memcached  {
   package  {  'memecached':
     ensure  =&gt;  installed,
   }

   file  {  '/etc/memcached.conf':
     source   =&gt;  'puppet:///modules/memcached/memcached.conf',
     owner    =&gt;  'root',
     group    =&gt;  'root',
     mode     =&gt;  '0644'
     require  =&gt;  Package['memcached'],
   }

   service  {  'memcached'
     ensure   =&gt;  running,
     enable   =&gt;  true,
     require  =&gt;  [Package['memcached']],
                     File['/etc/memcached.conf'],
   }
 }
</code></pre></li>
<li><p>参考如下内容创建 modules/memcached/files/memcached.conf文件：</p>

<pre><code> -m 64
 -p 11211
 -u nobody
 -l 127.0.0.1
</code></pre></li>
<li><p>把下面内容加入nodes.pp</p>

<pre><code> node 'cookbook3'  {
   include memcached
 }
</code></pre></li>
<li><p>运行puppet来检查配置是否生效</p>

<pre><code> papply
</code></pre></li>
<li><p>检查memcached是否已经运行。</p>

<pre><code> service memcached status
</code></pre></li>
</ol>


<blockquote><p>模块具有特定的目录结构，但是并非所有这些目录都必须存在。</p></blockquote>

<hr />

<h4>第六节 使用数组语句</h4>

<pre><code>1、puppet 数组定义

$package  =  ['ruby1.8-dev',
              'ruby1.8',
  .....
  .....
]
package  {$package:ensure  =&gt;  installed}

2、执行puppet 命令，可见需要安装的软件包已经装好。
3、使用散列Hash

散列就像是一个数组，但每个元素都可以通过名称（键值）存储和搜索，代码如下

$interface = {  'name'    =&gt; 'eth0'
                'address'  =&gt; '192.168.28.148'
}

notify {  ""Interface ${interface['name']} has address ${interface['address']}":}

4、使用split函数创建数组

用户声明数组如下，
$lunch = ['egg','beans','chips']

define lunchprint(){
  notify  { "Lunch included ${name}":  }
}

lunchprint  {  $lunch: }
结果打印
Lunch included egg
Lunch included beans
Lunch included chips

Puppet可以使用split 函数将字符串拆分为数组，
$menu = 'egg beans chips'
$items = split($menu ,' ')
lunchprint  {  $items:  }

结果打印
Lunch included egg
Lunch included beans
Lunch included chips

split 有需要两个参数，第一个是需要拆分的字符，第二个是分隔字符（可以使字符也可以是字符串）。

分割字符也可以是正则表达式，例如可以使用(管道符|)分割一组字符串

$lunch = 'egg:beans,chips'
$items = split($lunch,':|,') 
</code></pre>

<hr />

<h4>第七节 使用条件语句</h4>

<blockquote><p>可以在清单中设置变量来调整类的行为，比如在服务器A和服务器B可能需要不同的操作系统，
这是我们可以用if语句</p></blockquote>

<pre><code>    if  $::operatingsystem  ==  'ubuntu'  {
      notify  {  'running on ubuntu ':  }
    }  else  {
      notify  {  'no ubuntu':  }
    }
</code></pre>

<blockquote><p>if 关键字后为一个表达式 如果值为true则执行大括号内的内容,也可使用elseif</p></blockquote>

<pre><code>    if  $::operatingsystem  ==  'ubuntu'  {
      notify  {  'running on ubuntu ':  }
    }  elseif  $::operatingsystem == 'debian'  {
      notify  {  'close enough...':  }
    }  else  {
      notify  {  'no ubuntu':  }
    }
</code></pre>

<blockquote><p>比较</p></blockquote>

<ul>
<li>可以使用==语法来检查两个值是否相等，如上述代码所示</li>
<li>使用 != 来检查是否不相等</li>
<li><p>使用 and  or 或 not 组合来实现复杂的逻辑表达式</p>

<pre><code>  if  ($::uptime_days  &gt;  356)  and  ($::operatingsystem  ==  'ubuntu')  {

  }
</code></pre></li>
<li><p>也可以使用正则表达式进行判断</p></li>
</ul>


<h4>第九节 使用选择器和case语句</h4>

<blockquote><p>  虽然可以使用 if 来编写任何条件语句，但Puppet提供了一些其他的形式来帮助用户
  更容易地编写表达条件语句：选择器（selector）和case语句。
  查看一下代码：</p></blockquote>

<pre><code>    $systemtype = $::operatingsystem?(
      'Ubuntu'  =&gt;  'debianlike',
      'Debian'  =&gt;  'debianlike',
      'RedHat'  =&gt;  'redhatlike',
      'Centos'  =&gt;  'redhatlike',
      default   =&gt;  'unknown',
    )
    notify  {  "you have a ${systemtype} system"}
</code></pre>

<blockquote><p>选择器的操作符为 ？根据 $::operatingsystem的值为$systemtype选择不同的值，
类似C语言的三元运算符，但是不仅仅是在两个值之间进行选择，而是可以根据需要有尽
可能多的可选值。</p></blockquote>

<pre><code>    查看一下代码

    class debianlike{
      notify  {  'special manifest for debian-like systems':}
    }
    class readhatlike{
      notify  {  'special manifest for readhat-like systems':}
    }
    case $::operatingsystem  {
      'ubuntu',
      'debian':{
        include debianlike
      }
      'redhat',
      'fedora',
      'centos':{
        include redhatlike
      }
      default:{
        notify  {  "i don\t know what kind of system you have":}
      }
    }
</code></pre>

<blockquote><p>case 语句
与选择器不同，case不返回一个值，case语句非常适合根据某个表达式的值选择
执行不同的代码。上述代码当使用case语句时，根据$operatingsystem的值
引入了 debianlike类或者redhatlike类。</p>

<p>puppet将$::operatingsystem与列表的值进行比较，列表的值可以使正则
表达式，也可以字符串，当找到匹配项时就执行 大括号里边相关代码。
默认语句 没有匹配项就执行default 语句。</p></blockquote>

<h4>第十节 使用in运算符</h4>

<pre><code>in 运算符可以用来测试一个字符串中是否包含另一个字符串，代码如下
if  'spring' in 'springfield'
如果包含则上述表达式的值为 true
1、清单代码可以如下编写测试
if $::operatingsystem in ['Ubuntu','Debian']{
  notify  {  'Debian-type operating system detected':  }
}elseif  $::operatingsystem in  ['RedHat',  'Fedora',  'SuSE',  'CentOS']{ 
  notify  {  'RedHat-type operating system detected':  }
}else  {'some other operationg system detected'}
2、运行 puppet papply
</code></pre>

<hr />

<h4>第十一节 使用正则表达式进行替换</h4>

<pre><code>puppet通过regsubst函数提供了一种简单的方式来处理文本，在字符串中进行搜索和替换，或者从字符串中提取所需格式的子串，可以用来处理fact和从外部程序传进来的数据。（并未进行试验）

1、配置清单中代码如下
$class_c  =  regsubst($::ipaddress,'(.*)\..','\1.0')
notify  {  " the network part of ${::ipaddress} is ${class_c}:"
}
2、运行 papply（我们自己编辑的命令）

打印 the network part of 192.168.28.148 is 192.168.28.0:

regsubst 至少需要三个参数 source（输入源）、pattern（匹配模式）、replacement（替换内容）
此例子中 
source为 $::ipaddress 值为192.168.28.148
pattern为 (.*)\..
replacement为 \1.0

pattern将匹配整个IP地址，补货圆括号内的IP地址前三位，捕获的文本可以通过\1的形式replacement字符串中使用。
所有配的字符都将被replacement替换
也可以使用其它方法进行替换
</code></pre>

<hr />

<h4>第五节 使用内联模板</h4>

<h3>第三章</h3>

<hr />

<p>创建第一个puppet 配置</p>

<h3>第四章 创建去中心化的puppet架构</h3>

<hr />

<blockquote><p>使用puppet最常见的方法就是运行一台Puppet Master 服务器，Puppet client连接到Puppet Master
并接受各自的配置清单，但是，puppet Master并不是必须的，可以直接在配置清单文件上运行
puppet apply 命令来应用变更</p></blockquote>

<pre><code>    [root@cookbook5 puppet]# puppet apply /etc/puppet/modules/test/manifests/init.pp    
Notice: Compiled catalog for cookbook5 in environment production in 0.01 seconds
Notice: Finished catalog run in 0.01 seconds
</code></pre>

<blockquote><p>也就是说，如果能安排适当的清单文件分发到客户端上，就可以使用puppet直接执行，而不需要puppet
Master来控制管理，</p></blockquote>

<ul>
<li>这消除了由于仅有一台主服务器而造成的性能瓶颈</li>
<li>也消除了单点故障</li>
<li>同时也避免了新加入客户机要办法SSL签名证书的问题</li>
</ul>


<h4>第一节 整合gitbub</h4>

<ol>
<li>在新机器中检出github仓库</li>
<li><p>编写测试site.pp
node  &lsquo;cookbook&rsquo;,  &lsquo;cookbook2&rsquo;  {
file  {  &ldquo;/tmp/hello&rdquo;:
 content  =>  &ldquo;hello world\n&rdquo;,
}
}</p></li>
<li><p>运行命令：</p>

<pre><code>     [root@cookbook5 tmp]# puppet apply test.pp 
     Notice: Compiled catalog for cookbook5 in environment production in 0.12 seconds
     Notice: /Stage[main]/Main/Node[cookbook5]/File[/tmp/hello]/ensure: defined content as '{md5}81d3135bd5022a4999c182b0440b86b2'
     Notice: Finished catalog run in 0.05 seconds
</code></pre></li>
</ol>


<h5>工作原理</h5>

<ol>
<li>新机器上创建一个puppet仓库副本。使用git clone</li>
<li>在运行puppet前需要配置清单中关于新节点的声明 cookbook2 也就是新节点的hostname</li>
<li>在新节点上执行相同命令，配置清单即生效</li>
</ol>


<h5>此处结论</h5>

<ol>
<li>现在已经可以很方便的将puppet基础设施扩展到另外一台节点，也可以扩展的任意多台。</li>
<li>只需检出git仓库，并执行puppet apply 命令。</li>
<li>这样做不需要多余的机器来作为puppet master</li>
<li>我们需要节点能够自动从gitbub拉取清单，并自动执行这些配置清单。</li>
</ol>


<h4>第二节 编写脚本完成节点自动化</h4>

<ol>
<li>在puppet仓库中创建仓库中puppet模块module必须的目录（参看*标识）</li>
</ol>


<p>[root@cookbook5 tmp]# tree /opt/ning/gitpuppet/puppetest/puppet/</p>

<p>├── manifests</p>

<p>│   ├── nodes.pp</p>

<p>│   └── site.pp</p>

<p>├── modules *</p>

<p>│   ├── puppet *</p>

<p>│   │   ├── files *</p>

<p>│   │   │   ├── papply.sh</p>

<p>│   │   │   ├── pull-update.sh</p>

<p>│   │   │   └── README</p>

<p>│   │   └── manifests *</p>

<p>│   │       └── init.pp</p>

<ol>
<li><p>根据上述目录结构创建 papply.sh文件
[root@cookbook5 tmp]# cat  /puppet/modules/puppet/files/papply.sh</p>

<pre><code>  #!/bin/sh
  sudo puppet apply /opt/ning/gitpuppet/puppetest/puppet/manifests/site.pp  
  --modulepath=/opt/ning/gitpuppet/puppetest/puppet/modules $*
</code></pre></li>
<li><p>创建 puppet的init.pp文件</p>

<pre><code> [root@cookbook5 tmp]# vi init.pp 
 class  puppet  {
   file  {"/usr/local/bin/papply":
     source  =&gt;  "puppet:///modules/puppet/papply.sh",
     mode    =&gt;  "0755",
 }

 }
</code></pre></li>
<li><p>更改nodes.pp文件</p>

<pre><code> [root@cookbook5 tmp]# cat nodes.pp 
     node  'cookbook5'  {
          include puppet
     }
</code></pre></li>
<li>应用这些配置清单的更改</li>
</ol>


<p>puppet apply manifest/site.pp</p>

<ol>
<li>测试脚本是否工作正常。
papply</li>
</ol>


<p>现在，当运行puppet时，只需要运行papply命令即可。</p>

<h5>工作原理</h5>

<ol>
<li>在一台机器上运行puppet需要给puppet指定 要运行的清单文件位置
puppet apply manifest/site.pp</li>
<li><p>使用模块时，还需要给puppet指定模块的位置，使用modulepath参数</p>

<pre><code>sudo puppet apply /opt/ning/gitpuppet/puppetest/puppet/manifests/site.pp  
--modulepath=/opt/ning/gitpuppet/puppetest/puppet/modules $*
</code></pre></li>
<li>为了使用root 权限可能还需要添加sudo</li>
<li>最后通过添加$* 参数，将所有module都作为参数传递给puppet</li>
<li>由于输入的东西太多，所以可以将其脚本化，</li>
<li>同时添加一个file资源，用来将papply部署到/usr/local/bin 目录并使其执行。</li>
</ol>


<h5>此处结论</h5>

<p>1.现在已经可以在各个节点简单执行papply命令即可使配置清单生效</p>

<h4>第三节 使用cron运行puppet</h4>

<p> 通过当前的配置，已经可以管理团队中的puppet清单，通过github同步变更，使用papply脚本在客户机上手动配置变更。
 但是到目前为止，还需要手动登录到各个客户机更新git仓库，并重新运行puppet，接下来我们编写脚本，使每台机器可以自动更新并应用变更。
思路就是使用cron作业，定期从仓库中拉去配置文件在有变更时运行puppet</p>

<h5>准备工作</h5>

<p>1.不要将gitbub公钥添加到puppet仓库，需要其它方式进行分发（可使用ansible）
2.创建*标识的init.pp文件</p>

<p>[root@cookbook5 tmp]# tree /opt/ning/gitpuppet/puppetest/puppet/</p>

<p>/opt/ning/gitpuppet/puppetest/puppet/</p>

<p>├── manifests</p>

<p>│   ├── nodes.pp</p>

<p>│   └── site.pp</p>

<p>├── modules</p>

<p>│   ├── puppet</p>

<p>│   │   ├── files</p>

<p>│   │   │   ├── papply.sh</p>

<p>│   │   │   ├── pull-update.sh</p>

<p>│   │   │   └── README</p>

<p>│   │   └── manifests</p>

<p>│   │       └── init.pp*</p>

<blockquote><p>内容如下</p></blockquote>

<pre><code>    [root@cookbook5 tmp]# vi  init.pp 
    class  puppet  {
      file  {"/usr/local/bin/papply":
        source  =&gt;  "puppet:///modules/puppet/papply.sh",
        mode    =&gt;  "0755",
    }

      file  {"/usr/local/bin/pull-update":
        source  =&gt;  "puppet:///modules/puppet/pull-update.sh",
        mode    =&gt;  "0755",
      }
      cron  {"run-puppet":
        ensure   =&gt;  present,
        user     =&gt;  "root",
        command  =&gt;  "/usr/local/bin/pull-update",
        minute   =&gt;  "*/2",
        hour     =&gt;  "*",
      }
    }
</code></pre>

<ol>
<li><p>创建pull-update.sh 文件</p>

<pre><code> #!/bin/sh
 cd /opt/ning/gitpuppet/puppetest
 git pull &amp;&amp; papply
</code></pre></li>
<li><p>运行 puppet</p>

<pre><code>     papply
</code></pre></li>
<li><p>测试pull-update 是否正常工作</p>

<pre><code>     pull-update

 [root@cookbook5 tmp]# pull-update 
 Already up-to-date.
 Notice: Compiled catalog for cookbook5 in environment production in 0.17 seconds
 Notice: /Stage[main]/Puppet/File[/usr/local/bin/pull-update]/content: content changed '{md5}d46f4e058912bdef5d41571fd6f27bc2' to '{md5}40711b84a1ca5aa19d0cd4e3b19702f7'
 Notice: Finished catalog run in 0.08 seconds
</code></pre></li>
</ol>


<h4>工作原理</h4>

<ol>
<li>将拉去git pull 和papply动作全部写入pull-update脚本</li>
<li>在所有主机上都分发该脚本</li>
<li>通过cron创建定期运行的作业，定期执行pull-update脚本</li>
</ol>


<p>从而实现了自动化执行配置清单功能</p>

<h6>#第四节 使用Rake引导puppet运行</h6>

<pre><code>可免去ssh登录到客户机执行pull-update的操作
</code></pre>

<h6>#第五节  使用git hook检查puppet语法检查</h6>

<pre><code>可避免提交puppet清单时出现错误代码，此处可以使用puppet parser validate 检查或者是puppet-lint
</code></pre>

<h1>Foreman V1.11</h1>

<hr />

<p>foreman 使用 ruby语言开发 ruby on rail</p>

<h3>Foreman 架构</h3>

<hr />

<pre><code>    Foreman 安装后，包括一个可依赖的中心Foreman实例，用于提供Web GUI，节点配置，节点
    初始化配置文件，等等。Foreman支持无人值守的完全自动化的安装过程。其中smart proxy 
    和Foreman一起管理TFTP，DHCP，DNS，Puppet，Puppet CA，Salt和Chef
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[puppet学习]]></title>
    <link href="http://guangningsun.github.io/blog/2016/04/20/puppetxue-xi/"/>
    <updated>2016-04-20T11:07:25+08:00</updated>
    <id>http://guangningsun.github.io/blog/2016/04/20/puppetxue-xi</id>
    <content type="html"><![CDATA[# Puppet
-----

* puppet 代表一种代码语言
* puppet 代表一种对集群基础设施进行管理的平台
* 


### 第一章 puppet安装总结
-----



#### 第一节 工作原理


----

#### 第二节 准备工作

		* 关闭防火墙
		* NTP同步
		* 更改hosts
		* 安装ruby环境
		* 下载EPEL对应的版本和安装
		* 配置yum源，或者将puppet server和puppet client的rpm包下载
		以下将讲述详细操作步骤


######2.1 关闭selinux

	setenforce 0
	sed -i 's/SELINUX=enforcing/SELINUX=disabled/' /etc/selinux/config  

######2.2 NTP同步
	ntpdate 0.rhel.pool.ntp.org
	crontab -e
	0 0 * * * /usr/sbin/ntpdate 0.rhel.pool.ntp.org;/usr/sbin/hwclock -w

######2.3 添加hosts

vi /etc/hosts

		192.168.28.148 cookbook3.localdomain.com cookbook3

		192.168.28.149 cookbook4.localdomain.com cookbook4

######2.4 puppet
http://downloads.puppetlabs.com/puppet/
######2.5 facter
http://downloads.puppetlabs.com/facter/
######2.6 ruby
https://www.ruby-lang.org/zh_cn/downloads/

######2.7 配置CentOS YUM源

cd /etc/yum.repos.d/

cp Centos-Base.repo Centos-Base.repo.bak

vi Centos-Base.repo

		[base]
		name=CentOS-$releasever-Base
		baseurl=http://centos.ustc.edu.cn/centos/6/os/x86_64/
		gpgcheck=1
		gpgkey=http://mirrors.163.com/centos/RPM-GPG-KEY-CentOS-6

		[updates]
		name=CentOS-$releasever-Updates
		baseurl=http://centos.ustc.edu.cn/centos/6/os/x86_64/
		gpgcheck=1
		gpgkey=http://mirrors.163.com/centos/RPM-GPG-KEY-CentOS-6
		
		[extras]
		name=CentOS-$releasever-Extras
		baseurl=http://centos.ustc.edu.cn/centos/6/os/x86_64/
		gpgcheck=1
		gpgkey=http://mirrors.163.com/centos/RPM-GPG-KEY-CentOS-6
		
		[centosplus]
		name=CentOS-$releasever-Plus
		baseurl=http://centos.ustc.edu.cn/centos/6/os/x86_64/
		gpgcheck=1

yum clean all

yum makecache

yum repolist

-----
#### 第三节 Puppet Server安装

* 现已经将puppet server 所需的rpm包全部下载
* 如需安装只需在 rpm安装包目录下执行 rpm -Uvh * 命令即可
* 安装版本为puppet 3.8

######在网络畅通的情况下也可以按照以下步骤执行

######3.1 Enable the Puppet Labs Package Repository

rpm -ivh https://yum.puppetlabs.com/puppetlabs-release-el-6.noarch.rpm

######3.2 安装yum-plugin-downloadonly插件

yum install -y yum-plugin-downloadonly

######3.3 安装puppet server 

yum install puppetserver

yum install --downloadonly --downloaddir=/tmp/puppetserver puppetserver

		Total download size: 73 M
		Is this ok [y/N]: y
		Downloading Packages:
		(1/22): augeas-libs-1.0.0-10.el6.x86_64.rpm          
		(2/22): compat-readline5-5.2-17.1.el6.x86_64.rpm     
		(3/22): facter-2.4.6-1.el6.x86_64.rpm                
		(4/22): hiera-1.3.4-1.el6.noarch.rpm                 
		(5/22): java-1.7.0-openjdk-1.7.0.79-2.5.5.4.el6.x86_6
		(6/22): libjpeg-turbo-1.2.1-3.el6_5.x86_64.rpm       
		(7/22): libselinux-2.0.94-5.8.el6.i686.rpm           
		(8/22): libselinux-2.0.94-5.8.el6.x86_64.rpm         
		(9/22): libselinux-devel-2.0.94-5.8.el6.x86_64.rpm   
		(10/22): libselinux-python-2.0.94-5.8.el6.x86_64.rpm 
		(11/22): libselinux-ruby-2.0.94-5.8.el6.x86_64.rpm   
		(12/22): libselinux-utils-2.0.94-5.8.el6.x86_64.rpm  
		(13/22): puppet-3.8.6-1.el6.noarch.rpm               
		(14/22): puppetserver-1.1.3-1.el6.noarch.rpm         
		(15/22): ruby-1.8.7.374-4.el6_6.x86_64.rpm           
		(16/22): ruby-augeas-0.4.1-3.el6.x86_64.rpm          
		(17/22): ruby-irb-1.8.7.374-4.el6_6.x86_64.rpm       
		(18/22): ruby-libs-1.8.7.374-4.el6_6.x86_64.rpm      
		(19/22): ruby-rdoc-1.8.7.374-4.el6_6.x86_64.rpm      
		(20/22): ruby-shadow-2.2.0-2.el6.x86_64.rpm          
		(21/22): rubygem-json-1.5.5-3.el6.x86_64.rpm         
		(22/22): rubygems-1.3.7-5.el6.noarch.rpm             

		cookbook3@/tmp/puppetserver#rpm -Uvh *
		warning: augeas-libs-1.0.0-10.el6.x86_64.rpm: Header V3 RSA/SHA1 Signature, key ID c105b9de: NOKEY
		warning: facter-2.4.6-1.el6.x86_64.rpm: Header V4 RSA/SHA1 Signature, key ID 4bd6ec30: NOKEY

	Preparing...                 ########################################### [100%]
		1:libselinux             ###########################################
		2:augeas-libs            ########################################### [  9%]
		3:libselinux-ruby        ########################################### [ 14%]
   		4:libselinux-utils       ########################################### [ 18%]
   		5:libjpeg-turbo          ########################################### [ 23%]
   		6:java-1.7.0-openjdk     ########################################### [ 27%]
   		7:compat-readline5       ########################################### [ 32%]
   		8:ruby-libs              ########################################### [ 36%]
   		9:ruby                   ########################################### [ 41%]
  	   10:facter                 ########################################### [ 45%]
       11:ruby-irb               ########################################### [ 50%]
       12:ruby-rdoc              ########################################### [ 55%]
       13:rubygems               ########################################### [ 59%]
       14:rubygem-json           ########################################### [ 64%]
       15:hiera                  ########################################### [ 68%]
       16:ruby-shadow            ########################################### [ 73%]
       17:ruby-augeas            ########################################### [ 77%]
       18:puppet                 ########################################### [ 82%]
       19:puppetserver           ########################################### [ 86%]
       20:libselinux-devel       ########################################### [ 91%]
       21:libselinux-python      ########################################### [ 95%]
       22:libselinux             ########################################### [100%]


######3.4 验证puppetserver

cookbook3@/tmp/puppetserver#yum install puppetserver

	Loaded plugins: downloadonly, product-id, refresh-packagekit, security, subscription-manager
	Updating certificate-based repositories.
	Unable to read consumer identity
	Setting up Install Process
	Package puppetserver-1.1.3-1.el6.noarch already installed and latest version
	Nothing to do

######3.5 增加puppet master配置信息

vi /etc/puppet/puppet.conf

     [main]
         # master的主机名
         server = cookbook3.localdomain.com
         # master的主机名
         certname = cookbook3.localdomain.com
         #禁用插件同步
         pluginsync = false
         # The Puppet log directory.
         # The default value is '$vardir/log'.
         logdir = /var/log/puppet

         # Where Puppet PID files are kept.
         # The default value is '$vardir/run'.
         rundir = /var/run/puppet
     
         # Where SSL certificates are kept.
         # The default value is '$confdir/ssl'.
         ssldir = $vardir/ssl

     [agent]
         # The file in which puppetd stores a list of the classes
         # associated with the retrieved configuratiion.  Can be loaded in
         # the separate ``puppet`` executable using the ``--loadclasses``
         # option.
         # The default value is '$confdir/classes.txt'.
         classfile = $vardir/classes.txt

         # Where puppetd caches the local configuration.  An
         # extension indicating the cache format is added automatically.
         # The default value is '$confdir/localconfig'.
         localconfig = $vardir/localconfig

######3.6 启动puppet

puppet master

		ps -ef |grep master
		puppet    3077     1  0 10:09 ?        00:00:00 /usr/bin/ruby /usr/bin/puppet master
		root      3081  3015  0 10:09 pts/0    00:00:00 grep master

-----
#### 第四节 puppet client安装

* 现已经将puppet server 所需的rpm包全部下载
* 如需安装只需在 rpm安装包目录下执行 rpm -Uvh * 命令即可
* 安装版本为puppet 3.8

######4.1 Enable the Puppet Labs Package Repository

	rpm -ivh https://yum.puppetlabs.com/puppetlabs-release-el-6.noarch.rpm

######4.2 安装yum-plugin-downloadonly插件

	yum install -y yum-plugin-downloadonly

######4.3 yum install puppet

	yum install --downloadonly --downloaddir=/tmp/puppet puppet

	Downloading Packages:
	(1/19): augeas-libs-1.0.0-10.el6.x86_64.rpm          
	(2/19): compat-readline5-5.2-17.1.el6.x86_64.rpm     
	(3/19): facter-2.4.6-1.el6.x86_64.rpm                
	(4/19): hiera-1.3.4-1.el6.noarch.rpm                 
	(5/19): libselinux-2.0.94-5.8.el6.i686.rpm           
	(6/19): libselinux-2.0.94-5.8.el6.x86_64.rpm         
	(7/19): libselinux-devel-2.0.94-5.8.el6.x86_64.rpm   
	(8/19): libselinux-python-2.0.94-5.8.el6.x86_64.rpm  
	(9/19): libselinux-ruby-2.0.94-5.8.el6.x86_64.rpm    
	(10/19): libselinux-utils-2.0.94-5.8.el6.x86_64.rpm  
	(11/19): puppet-3.8.6-1.el6.noarch.rpm               
	(12/19): ruby-1.8.7.374-4.el6_6.x86_64.rpm           
	(13/19): ruby-augeas-0.4.1-3.el6.x86_64.rpm          
	(14/19): ruby-irb-1.8.7.374-4.el6_6.x86_64.rpm       
	(15/19): ruby-libs-1.8.7.374-4.el6_6.x86_64.rpm      
	(16/19): ruby-rdoc-1.8.7.374-4.el6_6.x86_64.rpm      
	(17/19): ruby-shadow-2.2.0-2.el6.x86_64.rpm          
	(18/19): rubygem-json-1.5.5-3.el6.x86_64.rpm         
	(19/19): rubygems-1.3.7-5.el6.noarch.rpm             

######4.4 离线安装

cookbook4@/tmp/puppet#rpm -Uvh *

	warning: facter-2.4.6-1.el6.x86_64.rpm: Header V4 RSA/SHA1 Signature, key ID 4bd6ec30: NOKEY
	Preparing...                ########################################### [100%]
	   1:libselinux             ########################################### [  5%]
   	   2:augeas-libs            ########################################### [ 11%]
   	   3:libselinux-ruby        ########################################### [ 16%]
   	   4:libselinux-utils       ########################################### [ 21%]
   	   5:compat-readline5       ########################################### [ 26%]
   	   6:ruby-libs              ########################################### [ 32%]
   	   7:ruby                   ########################################### [ 37%]
   	   8:facter                 ########################################### [ 42%]
   	   9:ruby-irb               ########################################### [ 47%]
  	  10:ruby-rdoc              ########################################### [ 53%]
  	  11:rubygems               ########################################### [ 58%]
  	  12:rubygem-json           ########################################### [ 63%]
  	  13:hiera                  ########################################### [ 68%]
  	  14:ruby-shadow            ########################################### [ 74%]
  	  15:ruby-augeas            ########################################### [ 79%]
  	  16:puppet                 ########################################### [ 84%]
  	  17:libselinux-devel       ########################################### [ 89%]
  	  18:libselinux-python      ########################################### [ 95%]
  	  19:libselinux             ########################################### [100%]

######4.5 验证puppet
	cookbook4@/tmp/puppet#yum install puppet
	
	Loaded plugins: fastestmirror, product-id, refresh-packagekit, security, 	subscription-manager
	Updating certificate-based repositories.
	Unable to read consumer identity
	Setting up Install Process
	Loading mirror speeds from cached hostfile
	Package puppet-3.8.6-1.el6.noarch already installed and latest version
	Nothing to do

######4.6 增加puppet client配置信息
vi /etc/puppet/puppet.conf

	[main]
   	  	# master的主机名
      	server = cookbook3.localdomain.com
      	# 禁用插件同步
      	pluginsync = false
      	# The Puppet log directory.
      	# The default value is '$vardir/log'.
      	logdir = /var/log/puppet

      	# Where Puppet PID files are kept.
      	# The default value is '$vardir/run'.
      	rundir = /var/run/puppet

      	# Where SSL certificates are kept.
      	# The default value is '$confdir/ssl'.
      	ssldir = $vardir/ssl

  	[agent]
      	# The file in which puppetd stores a list of the classes
      	# associated with the retrieved configuratiion.  Can be loaded in
      	# the separate ``puppet`` executable using the ``--loadclasses``
      	# option.
      	# The default value is '$confdir/classes.txt'.
      	classfile = $vardir/classes.txt

      	# Where puppetd caches the local configuration.  An
      	# extension indicating the cache format is added automatically.
      	# The default value is '$confdir/localconfig'.
      	localconfig = $vardir/localconfig
   
-----

#### 第五节 puppet client证书认证

######5.1 client agent连接server

	puppet agent --server=cookbook3.localdomain.com 或者

	puppet agent -t --server=cookbook3.localdomain.com

######5.2 在master上查看申请证书请求
	puppet cert --list

	"cookbook4.localdomain.com" (SHA256) BC:1B:42:88:B0:A4:F0:F2:81:56:5D:0E:7A:49:90:83:79:F2:41:A5:E3:12:FC:E2:F2:DB:DE:30:8E:DB:0D:D0

######5.3 在master上签发证书

	puppet cert sign --all

	puppet cert --sign cookbook4.localdomain.com

	Notice: Signed certificate request for client
	Notice: Removing file Puppet::SSL::CertificateRequest client at '/var/lib/puppet/ssl/ca/requests/client.pem'

######5.4 查看证书，"+"表示已经签名成功

	puppet cert -all

	+ "cookbook4.localdomain.com" (SHA256) BC:1B:42:88:B0:A4:F0:F2:81:56:5D:0E:7A:49:90:83:79:F2:41:A5:E3:12:FC:E2:F2:DB:DE:30:8E:DB:0D:D0
	+ "cookbook3.localdomain.com" (SHA256) 04:45:21:0B:B5:5E:41:AE:9C:F4:B4:6B:EC:2F:AA:D2:BE:46:33:3E:B1:0E:85:2E:3C:B7:6B:98:95:A8:CE:4D

-----
#### 第六节 puppet同步测试

######6.1 在master上创建一个site.pp文件
	vi /etc/puppet/manifests/site.pp

* 以下内容格式采用puppet 编码格式，在下一章节可以了解到puppet语言的风格及写法

node default { file { "/tmp/test.txt": content => "Hello, First puppet test!"} }

######6.2 在client机进行验证，如果/tmp/test.txt文件生成并有内容，则说明功能正常。

puppet agent --test

	Info: Caching catalog for client
	Info: Applying configuration version '1458888162'
	Notice: /Stage[main]/Main/Node[default]/File[/tmp/test.txt]/ensure: defined content as '{md5}390b4c389233b9ae38a84ff8c731a8a1'
	Info: Creating state file /var/lib/puppet/state/state.yaml
	Notice: Finished catalog run in 0.03 seconds


----
### 第二章 puppet 语言和风格
---
		配置清单：puppet用于配置服务器的主程序代码被称为配置清单

#### 第一节 社区推荐的puppet风格
----
###### 1.1 缩进
		配置清单中使用两个空格进行缩进（不适用Tab）参考一下代码
		node  ‘monitoring’  inherits  ‘server’  {
		  include  icinga::server
		  include  repo::apt
		}
###### 1.2 引号
		--资源的名字始终使用引号注明，如下：
		file  {  '/tmp/test.txt':
		而不是
		file  {  /tmp/test.txt:
		所有字符串使用单引号，除非
			* 字符串中有变量（如${name}）。
			* 字符串中包含了转义字符（如 \n）。
		在上述情况下应该使用双引号，否则puppet不会处理。
		--在puppet中始终要讲非保留字的参数值用单引号引起来，如
		name  =>  'ning',
		mode  =>  '0700',
		owner  =>  'root',
		但是，如下保留字的参数值则不需要引号
		ensure  =>  installed,
		enable  =>  ture,
		ensure  =>  running,
###### 1.3 变量
		--当字符串引入变量时，要确保他们被大括{}号括起来，如
		source  =>  "puppet:///modules/weget/${brand}.conf",
		否则puppet语法解析器无法解析变量，
		
		--///表示该路径在根目录下 且已经忽略了file文件夹 
###### 1.4 参数
		--在参数行末尾要加逗号，最后一行也要添加,方便以后用户添加和调整参数
		service  {'memcached':
		  ensure  =>  running,
		  enable  =>  true,
		}
		--当声明仅仅带有一个参数的资源时，要把全部声明写在一行，并且不要在末尾添加逗号
		package  {  'puppet':ensure  =>  installed}
		--当声明多个参数时，需要让每一个参数各占据一行。
		package  {  ‘rake’:
		  ensure    =>  installed,
		  provider  =>  gem,
		  require   =>  Package['rubygem'],   
		}
		--所有参数的箭头都要与最长参数名所在行箭头对齐，如上图所示，方便剪切。
		
###### 1.5符号链接 
		
		当声明的file资源是符号链接这种类型时，要设置ensure => link 及 target 属性
		file  {  '/etc/php5/cli/php.ini':
		  ensure  =>  link,
		  target  =>  '/etc/php.ini',		
		}
		
-----
		
#### 第二节 使用puppet-lint检查配置清单（代码）

代码主要风格如下

* 必须使用两个空格，不能使用软跳格TAB
* 不得使用文字制表符
* 不得包含尾随空格
* 每行代码最好不得超过80个字符（检查会出错但是不影响使用）
* 代码块中的参数箭头=>需要对齐(主要是风格统一，实际没有影响)

	使用puppet-link来检查代码风格正确性 
	1、安装 gem install puppet-link
	2、直接 使用命令 puppet-link site.pp检查site.pp文件的风格正确性

正确时不会有输出

错误时会有提醒

如果不希望检查一行代码是否超出了80字符

可以使用命令
puppet-lint  --no-80chars-check

更多风格检查可以查看 
http://puppet-link.com


-----

#### 第三节 使用模块



-----

#### 第四节 使用标准的命名约定

-----

#### 第五节 使用内联模板

-----

#### 第六节 使用数组语句
	
	1、puppet 数组定义
	
	$package  =  ['ruby1.8-dev',
				  'ruby1.8',
	  .....
	  .....
	]
	package  {$package:ensure  =>  installed}
	
	2、执行puppet 命令，可见需要安装的软件包已经装好。
	3、使用散列Hash
	
	散列就像是一个数组，但每个元素都可以通过名称（键值）存储和搜索，代码如下
	
	$interface = {  'name'    => 'eth0'
					'address'  => '192.168.28.148'
	}
	
	notify {  ""Interface ${interface['name']} has address ${interface['address']}":}
	
	4、使用split函数创建数组
	
	用户声明数组如下，
	$lunch = ['egg','beans','chips']

	define lunchprint(){
	  notify  { "Lunch included ${name}":  }
	}
	
	lunchprint  {  $lunch: }
	结果打印
	Lunch included egg
	Lunch included beans
	Lunch included chips
	
	Puppet可以使用split 函数将字符串拆分为数组，
	$menu = 'egg beans chips'
	$items = split($menu ,' ')
	lunchprint  {  $items:  }
	
	结果打印
	Lunch included egg
	Lunch included beans
	Lunch included chips
	
	split 有需要两个参数，第一个是需要拆分的字符，第二个是分隔字符（可以使字符也可以是字符串）。
	
	分割字符也可以是正则表达式，例如可以使用(管道符|)分割一组字符串
	
	$lunch = 'egg:beans,chips'
	$items = split($lunch,':|,') 
	
----
#### 第七节 使用条件语句

> 可以在清单中设置变量来调整类的行为，比如在服务器A和服务器B可能需要不同的操作系统，
> 这是我们可以用if语句 

		if  $::operatingsystem  ==  'ubuntu'  {
		  notify  {  'running on ubuntu ':  }
		}  else  {
		  notify  {  'no ubuntu':  }
		}
		
> if 关键字后为一个表达式 如果值为true则执行大括号内的内容,也可使用elseif

		if  $::operatingsystem  ==  'ubuntu'  {
		  notify  {  'running on ubuntu ':  }
		}  elseif  $::operatingsystem == 'debian'  {
		  notify  {  'close enough...':  }
		}  else  {
		  notify  {  'no ubuntu':  }
		}
>比较 

* 可以使用==语法来检查两个值是否相等，如上述代码所示
* 使用 != 来检查是否不相等
* 使用 and  or 或 not 组合来实现复杂的逻辑表达式

		if  ($::uptime_days  >  356)  and  ($::operatingsystem  ==  'ubuntu')  {
		
		}
		
* 也可以使用正则表达式进行判断


     

#### 第九节 使用选择器和case语句

>   虽然可以使用 if 来编写任何条件语句，但Puppet提供了一些其他的形式来帮助用户
>   更容易地编写表达条件语句：选择器（selector）和case语句。
>   查看一下代码：


		$systemtype = $::operatingsystem?(
		  'Ubuntu'  =>  'debianlike',
		  'Debian'  =>  'debianlike',
		  'RedHat'  =>  'redhatlike',
		  'Centos'  =>  'redhatlike',
		  default   =>  'unknown',
		)
		notify  {  "you have a ${systemtype} system"}

> 选择器的操作符为 ？根据 $::operatingsystem的值为$systemtype选择不同的值，
> 类似C语言的三元运算符，但是不仅仅是在两个值之间进行选择，而是可以根据需要有尽
> 可能多的可选值。

		查看一下代码
		
		class debianlike{
		  notify  {  'special manifest for debian-like systems':}
		}
		class readhatlike{
		  notify  {  'special manifest for readhat-like systems':}
		}
		case $::operatingsystem  {
		  'ubuntu',
		  'debian':{
		    include debianlike
		  }
		  'redhat',
		  'fedora',
		  'centos':{
		    include redhatlike
		  }
		  default:{
		    notify  {  "i don\t know what kind of system you have":}
		  }
		}
> case 语句
> 与选择器不同，case不返回一个值，case语句非常适合根据某个表达式的值选择
> 执行不同的代码。上述代码当使用case语句时，根据$operatingsystem的值
> 引入了 debianlike类或者redhatlike类。

> puppet将$::operatingsystem与列表的值进行比较，列表的值可以使正则
> 表达式，也可以字符串，当找到匹配项时就执行 大括号里边相关代码。
> 默认语句 没有匹配项就执行default 语句。 



#### 第十节 使用in运算符
	
	in 运算符可以用来测试一个字符串中是否包含另一个字符串，代码如下
	if  'spring' in 'springfield'
	如果包含则上述表达式的值为 true
	1、清单代码可以如下编写测试
	if $::operatingsystem in ['Ubuntu','Debian']{
	  notify  {  'Debian-type operating system detected':  }
	}elseif  $::operatingsystem in  ['RedHat',  'Fedora',  'SuSE',  'CentOS']{ 
	  notify  {  'RedHat-type operating system detected':  }
	}else  {'some other operationg system detected'}
	2、运行 puppet papply
	

----
#### 第十一节 使用正则表达式进行替换

	puppet通过regsubst函数提供了一种简单的方式来处理文本，在字符串中进行搜索和替换，或者从字符串中提取所需格式的子串，可以用来处理fact和从外部程序传进来的数据。（并未进行试验）
	
	1、配置清单中代码如下
	$class_c  =  regsubst($::ipaddress,'(.*)\..','\1.0')
	notify  {  " the network part of ${::ipaddress} is ${class_c}:"
	}
	2、运行 papply（我们自己编辑的命令）
	
	打印 the network part of 192.168.28.148 is 192.168.28.0:
	
	regsubst 至少需要三个参数 source（输入源）、pattern（匹配模式）、replacement（替换内容）
	此例子中 
	source为 $::ipaddress 值为192.168.28.148
	pattern为 (.*)\..
	replacement为 \1.0
	
	pattern将匹配整个IP地址，补货圆括号内的IP地址前三位，捕获的文本可以通过\1的形式replacement字符串中使用。
	所有配的字符都将被replacement替换
	也可以使用其它方法进行替换
	
-----
	
### 第三章 puppet 使用

---



### 第四章 创建去中心化的puppet架构

-----

如果能安排适当的清单文件发送到客户端上puppet-client，就可以使用Puppet直接执行，而不需要通过Puppet Master控制。这将消除由于仅有一台主服务器导致的性能瓶颈，




#Foreman V1.11
-----

foreman 使用 ruby语言开发 ruby on rail

### Foreman 架构
---
		Foreman 安装后，包括一个可依赖的中心Foreman实例，用于提供Web GUI，节点配置，节点
		初始化配置文件，等等。Foreman支持无人值守的完全自动化的安装过程。其中smart proxy 
		和Foreman一起管理TFTP，DHCP，DNS，Puppet，Puppet CA，Salt和Chef


### Smart-Proxy		
---
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[ceph s3接口使用及结果分析]]></title>
    <link href="http://guangningsun.github.io/blog/2016/03/29/ceph-s3jie-kou-shi-yong-ji-jie-guo-fen-xi/"/>
    <updated>2016-03-29T14:44:10+08:00</updated>
    <id>http://guangningsun.github.io/blog/2016/03/29/ceph-s3jie-kou-shi-yong-ji-jie-guo-fen-xi</id>
    <content type="html"><![CDATA[<h1>ceph s3接口情况测试脚本</h1>

<p>用户access_key 和secret_key需要用命令生成，方法可参考如何搭建rgw相关内容</p>

<pre><code>测试逻辑
1、同一用户创建同bucket，测试原bucket里的内容是否会被覆盖掉（bucket维度）
2、同一用户创建不同bucket，但是创建相同object分别存于不同bucket，测试是否允许,在同一个bucket中创建相同名称object则数据被覆盖（object维度）
3、不同用户通过s3接口创建相同bucket，测试是否被允许（不被允许 bucket维度）
4、不同用户创建相同名称的object存于不同bucket和相同bucket，测试是否被允许（object维度）




# coding=utf-8
import boto
import boto.s3.connection

# author :ning
# date :2016-03-29
# desc: test the s3 API and the meanning of bucket


'''
param : access_key &amp; secret_key for the first user
    access_key2 &amp; secret_key2 for the second user
    con for the first user
    con2 for the second user
descreption:
    use rgw command to genterate the keys for user to access

'''

access_key = 'OXIHVCEYXVHDDC4G6C9Y'
secret_key = 'mZOVC6dW9dER1XTKP8tB33Vl3ai16W0DKeffSaxG'

access_key2 = 'QVALWEMRKW2UUWJFMPYE'
secret_key2 = 'yDUDDl4F1bAv2rxKwqWoPJCxAHmrFO9aIAJ2eAoH'

conn2 = boto.connect_s3(
    aws_access_key_id = access_key2,
    aws_secret_access_key = secret_key2,
    host = '192.168.28.138', port = 7480,
    is_secure=False, calling_format = boto.s3.connection.OrdinaryCallingFormat(),
    )

conn = boto.connect_s3(
    aws_access_key_id = access_key,
    aws_secret_access_key = secret_key,
    host = '192.168.28.138', port = 7480,
    is_secure=False, calling_format = boto.s3.connection.OrdinaryCallingFormat(),
    )


'''
测试逻辑
1、同一用户创建同bucket，测试原bucket里的内容是否会被覆盖掉（bucket维度）
2、同一用户创建不同bucket，但是创建相同object分别存于不同bucket，测试是否允许,在同一个bucket中创建相同名称object则数据被覆盖（object维度）
3、不同用户通过s3接口创建相同bucket，测试是否被允许（不被允许 bucket维度）
4、不同用户创建相同名称的object存于不同bucket和相同bucket，测试是否被允许（object维度）

'''
#测试逻辑1

bucket = conn.create_bucket('first_user_bucket')
for bucket in conn.get_all_buckets():
    print "{name}\t{created}".format(
                name = bucket.name,
                created = bucket.creation_date,
    )


key = bucket.new_key('hello_oragin.txt')
key.set_contents_from_string('this is the first Hello World !')

hello_key =bucket.get_key('hello_oragin.txt')
hello_key.set_canned_acl('public-read')

hello_url = hello_key.generate_url(0,query_auth=False,force_http=True)

print "首次创建bucket 首次创建 object"
print hello_url
print "我们可以确认从上述url中可以get下文件"

#key.get_contents_to_filename('/opt/ning/hello_oragin.txt')

#first用户创建相同名称的bucket，但并不创建object，看是否能够得到原有的object

bucket1 = conn.create_bucket('first_user_bucket')
for bucket in conn.get_all_buckets():
    print "{name}\t{created}".format(
                name = bucket.name,
                created = bucket.creation_date,

    )

print "重新生成url"
hello_url = hello_key.generate_url(0,query_auth=False,force_http=True)
print "查看url下是否有文件和内容this is the first Hello World !"
print " 结论为  仍然可以下载到文件，覆盖bucket并不影响object内容"
print "------------------------------------------------------"
print ""

#测试逻辑2
bucket2 = conn.create_bucket('first_user_bucket2')
for bucket in conn.get_all_buckets():
    print "{name}\t{created}".format(
                name = bucket.name,
                created = bucket.creation_date,
    )
first_user_key2 = bucket2.new_key('hello_oragin.txt')
first_user_key2.set_contents_from_string('this is the hello world that created  by first user in the second bucket but the same object name')
hello_key_first_user2 = bucket2.get_key('hello_oragin.txt')
hello_key_first_user2.set_canned_acl('public-read')
hello_url2 =hello_key_first_user2.generate_url(0,query_auth=False,force_http=True)
print "创建了新bucket1 "
print "创建了新的url"
print hello_url2
print "在新bucket2里创建与bucket1相同的object，查看两个url中是否都同时存在数据"
print "结论为：两个url中均可得到预想的数据，在两个不同bucket中可以存储相同名称的object"
print "------------------------------------------------------"
print ""

#测试逻辑3

try:
    bucket3 = conn2.create_bucket('first_user_bucket')
except Exception,e:
    print "用户2无法创建相同名称的bucket"
    print e
    print ""

#测试逻辑4


print "-------------------------------------------------------"
print ""
'''
this section is for secret url 
now we don't introduce the detials

'''
key1 = bucket.new_key('secret_plans.txt')
key1.set_contents_from_string('secret hello world!')

plans_key = bucket.get_key('secret_plans.txt')
plans_key.set_canned_acl('private')
plans_url = plans_key.generate_url(3600,query_auth=True,force_http=True)
print plans_url

key1.get_contents_to_filename('/opt/ning/secret.txt')
</code></pre>

<h2>执行上述脚本后可得出结论</h2>

<table>
<thead>
<tr>
<th> 同一用户操作        </th>
<th style="text-align:center;"> 相同objcet          </th>
<th style="text-align:right;"> 不同object  </th>
</tr>
</thead>
<tbody>
<tr>
<td> 相同bucket      </td>
<td style="text-align:center;"> object的数据会被覆盖 </td>
<td style="text-align:right;"> 正常操作 </td>
</tr>
<tr>
<td> 不同bucket      </td>
<td style="text-align:center;"> 可以存储互相不会影响，可以认证对象存储的key不是该层object的文件名称      </td>
<td style="text-align:right;">   正常操作 </td>
</tr>
<tr>
<td> 不同用户操作 </td>
<td style="text-align:center;"> 相同objcet      </td>
<td style="text-align:right;">    不同 object </td>
</tr>
<tr>
<td> 相同bucket </td>
<td style="text-align:center;"> 无法创建相同的bucket </td>
<td style="text-align:right;">无法创建相同的bucket </td>
</tr>
<tr>
<td> 不同bucket </td>
<td style="text-align:center;">可以创建相同名称的object，且互相不会影响。</td>
<td style="text-align:right;">正常操作</td>
</tr>
</tbody>
</table>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[ceph librados api python调用测试]]></title>
    <link href="http://guangningsun.github.io/blog/2016/03/25/ceph-librados-api-pythondiao-yong-ce-shi/"/>
    <updated>2016-03-25T15:17:39+08:00</updated>
    <id>http://guangningsun.github.io/blog/2016/03/25/ceph-librados-api-pythondiao-yong-ce-shi</id>
    <content type="html"><![CDATA[<h1>ceph librados api python调用测试</h1>

<h3>删除接口</h3>

<pre><code>import sys,rados


cluster = rados.Rados(conffile='/etc/ceph/ceph.conf')
print "\nlibrados version: " + str(cluster.version())
print "Will attempt to connect to: " + str(cluster.conf_get('mon initial members'))

cluster.connect()
print "\nCluster ID: " + cluster.get_fsid()

ioctx = cluster.open_ioctx('data')

print "------start delete----------------------"
try:
    print "read the content that is being deleted::::"+ioctx.read(sys.argv[1])
    print "\nRemoving object "+sys.argv[1]
    try:
        ioctx.remove_object(sys.argv[1])
    except:
        print "no such object"
except:
    print "no such object"
print "------delete end-------------------------"
</code></pre>

<hr />

<h3>读取数据接口</h3>

<pre><code>import sys,rados


cluster = rados.Rados(conffile='/etc/ceph/ceph.conf')
print "\nlibrados version: " + str(cluster.version())
print "Will attempt to connect to: " + str(cluster.conf_get('mon initial members'))

cluster.connect()
print "\nCluster ID: " + cluster.get_fsid()

ioctx = cluster.open_ioctx('data')
print "----------start read object------------ "
try:
print "\n\nContents of object "+sys.argv[1]+"\n------------------------\n"

    try :
        print ioctx.read(sys.argv[1])        
    except :
        print "no such object"
            ioctx.close()
    except:
print "no such object"
print "---------end read object-------------------"
ioctx.close()
</code></pre>

<hr />

<h3>写数据测试</h3>

<pre><code>import sys,rados


cluster = rados.Rados(conffile='/etc/ceph/ceph.conf')
print "\nlibrados version: " + str(cluster.version())
print "Will attempt to connect to: " + str(cluster.conf_get('mon initial members'))

cluster.connect()
print "\nCluster ID: " + cluster.get_fsid()

ioctx = cluster.open_ioctx('data')



try:
    objname = sys.argv[1]
    objcontent =""
    for i in range(2, len(sys.argv)):
    objcontent += sys.argv[i]
    objcontent +=" "
    print "------------start write objcet to ceph-----------"
    ioctx.write_full(objname, objcontent)
    print "\n\nContents of object "+ objname +"\n------------------------\n"
    print ioctx.read(objname)

    #print "\nRemoving object " + objname
    #ioctx.remove_object(objname)
finally:
    #file_object.close( )
    ioctx.close()
    print "------------end write object to ceph------------ "
</code></pre>

<hr />

<h3>写入图片测试</h3>

<pre><code>import rados, sys

cluster = rados.Rados(conffile='/etc/ceph/ceph.conf')
print "\nlibrados version: " + str(cluster.version())
print "Will attempt to connect to: " + str(cluster.conf_get('mon initial members'))

cluster.connect()
print "\nCluster ID: " + cluster.get_fsid()

ioctx = cluster.open_ioctx('data')



##file_object = open('/opt/ning/text.txt')

file_object = open('/opt/ning/test_photo.jpg')
try:
    all_the_text = file_object.read( )
    ioctx.write_full("hw", all_the_text)
    print "\n\nContents of object 'hw'\n------------------------\n"
    print ioctx.read("hw")

    print "\nRemoving object 'hw'"
    ioctx.remove_object("hw")
finally:
     file_object.close( )
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[ceph-deploy_自动部署ceph集群及radosgw步骤.md]]></title>
    <link href="http://guangningsun.github.io/blog/2016/03/25/ceph-deploy-zi-dong-bu-shu-cephji-qun-ji-radosgwbu-zou-dot-md/"/>
    <updated>2016-03-25T15:06:59+08:00</updated>
    <id>http://guangningsun.github.io/blog/2016/03/25/ceph-deploy-zi-dong-bu-shu-cephji-qun-ji-radosgwbu-zou-dot-md</id>
    <content type="html"><![CDATA[<h1>ceph-deploy 自动部署ceph集群步骤</h1>

<pre><code>1、新建 admin mon osd0 osd1 节点

2、admin节点执行以下语句

    sudo yum install -y 
    yum-utils &amp;&amp; sudo yum-config-manager 
    --add-repo https://dl.fedoraproject.org/pub/epel/7/x86_64/ &amp;&amp; sudo 
    yum install --nogpgcheck -y epel-release &amp;&amp; sudo rpm --import /etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7 &amp;&amp; sudo rm /etc/yum.repos.d/dl.fedoraproject.org*

3、yum install wget 下载工具

4、yum update &amp;&amp; sudo yum install ceph-deploy 更新yum 下载自动部署工具

5、yum install ntp ntpdate ntp-doc时间同步工具

6、yum install openssh-server 安装ssh

7、sudo setenforce 0 关闭方防火墙

8、yum install yum-plugin-priorities

9、ceph-deploy purgedata {ceph-node} [{ceph-node}]

10、ceph-deploy purge mon01 ceph01 ceph02

11、ceph-deploy new ceph-admin 在管理机上建立集群

12、ceph-deploy install mon01 ceph01 ceph02 在各节点上安装ceph

13、需要更改hosts配置文件并发送至各个节点 并重启节点

14、ceph-deploy mon create-initial 初始化monitor并且聚集各个节点conf文件
</code></pre>

<h4>出现错误及解决办法</h4>

<hr />

<pre><code>1、ceph-deploy执行mon初始化时 出现如下错误

[mon01][WARNIN] /etc/init.d/ceph: line 15: /lib/lsb/init-   functions: No such file or directory
[mon01][ERROR ] RuntimeError: command returned non-zero exit status: 1

解决办法:

在 mon节点手动执行yum install redhat-lsb

ceph-deploy osd prepare node2:/var/local/osd0 node3:/var/local/osd1 做安装 osd的准备

ceph-deploy osd activate node2:/var/local/osd0 node3:/var/local/osd1 激活osd

ceph-deploy激活osd时可能遇到

仍然在相应节点执行 

yum install redhat-lsb

ceph-deploy admin ceph-admin mon01 ceph01 ceph02 

将配置文件和keyring分发到指定节点 这样mon才可以有权限获取各个节点的心跳信息
</code></pre>

<hr />

<h3>ceph-deploy 安装radosgw</h3>

<pre><code>1、ceph-deploy rgw create mon01 在指定节点上创建gateway服务

2、安装后在7480 端口可访问成功数据

3、查看 pool 和object的对应关系

yum install  httpd 安装httpd apache

4、vi /etc/httpd/conf.d/rgw.conf  配置 rgw.conf

&lt;VirtualHost *:80&gt;
ServerName 192.168.28.134
DocumentRoot /var/www/html

ErrorLog /var/log/httpd/rgw_error.log
CustomLog /var/log/httpd/rgw_access.log combined

LogLevel debug

RewriteEngine On

RewriteRule .* - [E=HTTP_AUTHORIZATION:%{HTTP:Authorization},L]

SetEnv proxy-nokeepalive 1

ProxyPass / unix:///var/run/ceph/ceph.radosgw.gateway.fastcgi.sock|fcgi://192.168.28.134:9000/

&lt;/VirtualHost&gt;

5、sudo radosgw-admin user create --uid="testuser" --display-name="First User" 在gw节点上创建 用户

创建返回数据为

{
"user_id": "testuser1",
"display_name": "First1 User",
"email": "",
"suspended": 0,
 "max_buckets": 1000,
 "auid": 0,
 "subusers": [],
 "keys": [
   {
       "user": "testuser1",
       "access_key": "411L1VE75YEM3E214LO2",
       "secret_key": "dhHD5QXRmXrQuhc8rOihCl74Z4CWCCYbR9V24qKH"
   }
 ],
 "swift_keys": [],
 "caps": [],
 "op_mask": "read, write, delete",
 "default_placement": "",
 "placement_tags": [],
  "bucket_quota": {
      "enabled": false,
      "max_size_kb": -1,
      "max_objects": -1
},
"user_quota": {
    "enabled": false,
    "max_size_kb": -1,
    "max_objects": -1
},
"temp_url_keys": []
}

6、yum install python-boto 安装 boto编写调用s3插件

7、配置rgw

[global]
fsid = b2d39459-c51d-4503-b2bc-e2517aa90c86
mon_initial_members = mon01
mon_host = 192.168.28.132
auth_cluster_required = none
auth_service_required = none
auth_client_required = none
filestore_xattr_use_omap = true
osd_pool_default_size = 2
debug ms = 1
debug rgw = 20
[client.radosgw.gateway]
host = mon01
keyring = /etc/ceph/ceph.client.radosgw.keyring
rgw socket path = /var/run/ceph/ceph.radosgw.gateway.fastcgi.sock
log file = /var/log/radosgw/client.radosgw.gateway.log
rgw print continue = false

8、ceph-deploy --overwrite-conf config pull ceph-admin 拉去conf

9、ceph-deploy --overwrite-conf config push mon01 ceph01 ceph02 推送conf

10、scp /etc/ceph/ceph.client.admin.keyring  root@192.168.28.132:/etc/ceph拷贝keyring到rgw节点

11、mkdir -p /var/lib/ceph/radosgw/ceph-radosgw.gateway 在 rgw节点上创建数据目录

12、chown apache:apache /var/run/ceph 在rgw节点上更改ceph所属类别

13、chown apache:apache /var/log/radosgw/client.radosgw.gateway.log 更改rgw日志所属

14、/etc/init.d/ceph-radosgw start 启动rgw
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[无网络环境下部署ceph radosgw]]></title>
    <link href="http://guangningsun.github.io/blog/2016/03/25/wu-wang-luo-huan-jing-xia-bu-shu-ceph-radosgw/"/>
    <updated>2016-03-25T14:49:49+08:00</updated>
    <id>http://guangningsun.github.io/blog/2016/03/25/wu-wang-luo-huan-jing-xia-bu-shu-ceph-radosgw</id>
    <content type="html"><![CDATA[<h1>RGW安装</h1>

<pre><code>    RGW目前支持直接使用CivetWeb作为WebServer，实现HTTP请求的接受和回复，而不需要配置复杂的FCGI和WebServer了。
</code></pre>

<h3>创建存储池</h3>

<pre><code>1、通过ceph -s命令确认你的Ceph集群已经正常运行，并且集群状态是OK。运行以下命令创建rgw所需的存储池：

    ceph osd pool create .rgw 64 64 
    ceph osd pool create .rgw.root 64 64 
    ceph osd pool create .rgw.control 64 64 
    ceph osd pool create .rgw.gc 64 64 
    ceph osd pool create .rgw.buckets 64 64 
    ceph osd pool create .rgw.buckets.index 64 64 
    ceph osd pool create .log 64 64 
    ceph osd pool create .intent-log 64 64 
    ceph osd pool create .usage 64 64 
    ceph osd pool create .users 64 64 
    ceph osd pool create .users.email 64 64 
    ceph osd pool create .users.swift 64 64 
    ceph osd pool create .users.uid 64 64 

2、配置
使用CivetWeb作为RGW的前端非常简单，只需要在ceph.conf的末尾中加入以下配置项即可：

[client.radosgw.gateway] 
host = {your-host-name} 
log file = /var/log/radosgw/client.radosgw.log 

这三行定义了一个radosgw实例，名称就叫gateway，
运行的主机是{your-hostname}，需要将其改成实际的主机名。
“log file”配置项代表了日志路径，
需要注意的是需要保证该日志文件的父路径“ /var/log/radosgw/”必须存在，
radosgw不会自动创建，你可以使用

mkdir -p /var/log/radosgw/ 
创建该路径。

3、启动

启动命令：

radosgw -c /etc/ceph/ceph.conf -n client.radosgw.gateway 

-c参数表示使用的配置文件路径，-n表示要启动的radosgw实例名称，要与配置文件对应。

4、使用

CivetWeb启动的radosgw默认将监听7480端口。你可以直接通过访问http://your-host-ip:7480/来访问该RGW对象存储。
你可以通过radosgw-admin命令以管理员的方式访问所启动的RGW，执行例如创建用户等操作。
你也可以通过s3cmd命令行工具以用户的方式访问RGW，或者通过s3broswer图形界面访问RGW，执行上传/下载文件等操作。
</code></pre>

<h1>无网络环境手动安装步骤</h1>

<p>ceph-deploy install &ndash;rgw ceph-mon 安装rgw</p>

<p>（若无法执行可手动执行以下命令）</p>

<p>所需命令为</p>

<pre><code>yum clean all

yum -y install epel-release

yum -y install yum-priorities

rpm --import https://ceph.com/git/?p=ceph.git;a=blob_plain;f=keys/release.asc

因为没有外网环境所以可以将 release.asc文件部署在内网的httpd节点中，
后将import地址更换成release.asc 在httpd中的地址

* relsea.asc 文件在本文件对应附件中

rpm -Uvh --replacepkgs http://ceph.com/rpm-hammer/el7/noarch/ceph-   release-1-0.el7.noarch.rpm

同样relsease.noarch.rpm文件在附件中，处理办法同上

yum -y install ceph ceph-radosgw
</code></pre>

<h5>安装rgw客户端所需python 支持</h5>

<pre><code>1、下载 git clone  git://github.com/boto/boto.git

2、将下载包 导入到无网络环境的 ceph集群 rgw节点中执行下述命令

3、cd boto
pyhton setup.py

boto文件在本文的附件中可以使用
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[ceph命令解析及使用]]></title>
    <link href="http://guangningsun.github.io/blog/2016/03/10/cephming-ling-jie-xi-ji-shi-yong/"/>
    <updated>2016-03-10T15:31:16+08:00</updated>
    <id>http://guangningsun.github.io/blog/2016/03/10/cephming-ling-jie-xi-ji-shi-yong</id>
    <content type="html"><![CDATA[<h2>ceph-deploy集群部署工具</h2>

<hr />

<p>功能一般安装在ceph-admin 机器上（实验系统全部装在mon节点）</p>

<ul>
<li><p>ceph-deploy new [initial-monitor-node(s)]</p>

<pre><code>  创建新cluster 不需要自己手动编写ceph.conf自动生成keyring
</code></pre></li>
<li><p>ceph-deploy install [ceph-node] [ceph-node&hellip;]</p>

<pre><code>  在远程节点安装ceph程序
</code></pre></li>
<li><p>ceph-deploy mds create node1 node2
 ceph-deploy mds destory node1 node2</p>

<pre><code> 在远程节点 创建和删除mds服务
</code></pre></li>
<li><p>ceph-deploy mon create-initial|add</p>

<pre><code> 在远程节点 部署monitor和添加monitor节点
</code></pre></li>
<li><p>ceph-deploy gatherkeys node1 node2</p>

<pre><code> 在安装新服务 osd mon时候 需要将权限密钥 聚集到新节点指定位置（安装 radosgw admin时会用到）
</code></pre></li>
<li><p>ceph-deploy osd prepare [ceph-node]:[dir-path]</p>

<pre><code> 安装osd前准备检查目标节点状态并返回给用户
</code></pre></li>
<li><p>ceph-deploy osd create [ceph-node]:[dir-path]</p>

<pre><code>  创建 osd  dir-path可以不写（查询osd tree时候回多增加一个osd节点）
</code></pre></li>
<li><p>ceph-deploy admin [admin-node][ceph-node&hellip;]</p>

<pre><code>  把ceph.conf 和client.admin.keyring同时拷贝到目标节点
</code></pre></li>
<li><p>ceph-deploy purgedata [ceph-node][ceph-node&hellip;]</p>

<pre><code> 删除目标节点/var/lib/ceph*数据，这样重新初始化集群的时候不需要手动删除和创建文件夹
</code></pre></li>
<li><p>ceph-deploy forgetkeys</p>

<pre><code>  删除目标节点上所有密钥 
</code></pre></li>
</ul>


<p>ceph osd pool ls detail</p>

<h2>ceph 集群操作命令</h2>

<pre><code>    ceph 主要用来手工维护集群和部署集群 提供了对osd、monitor、mds、pg等 的所有功能的命令行操作 -c ceph.conf -i infile -o outfile
</code></pre>

<ul>
<li><p>ceph auth [ add | caps | del | export | get | get-key | get-or-create | get-or-create-key | import | list | print-key | print_key ] &hellip;</p></li>
<li><p>ceph config-key [ del | exists | get | list | put ] &hellip;</p></li>
<li><p>ceph df {detail}</p>

<pre><code> 查看集群存储状况
</code></pre></li>
<li><p>ceph fs [ ls | new | reset | rm ] &hellip;</p>

<pre><code>  管理 cephfs 文件系统命令
</code></pre></li>
<li><p>ceph fsid</p>

<pre><code>  查看集群的fsid或uuid
</code></pre></li>
<li><p>ceph health {detail}</p>

<pre><code>  查看集群健康状况
</code></pre></li>
<li><p>ceph heap [ dump | start_profiler | stop_profiler | release | stats ] &hellip;</p>

<pre><code>  stats 查看堆存储状况
  release 将heap所占用的RAM释放会系统

  其它的未测试
</code></pre></li>
<li><p>ceph injectargs <injectedargs> [ <injectedargs>&hellip; ]</p>

<pre><code>  将参数注入到Monitor中
</code></pre></li>
<li><p>ceph log <logtext> [ <logtext>&hellip; ]</p>

<pre><code>  将日志整合到Monitor日志中
</code></pre></li>
<li><p>ceph mds [ add_data_pool | cluster_down | cluster_up | compat | deactivate | dump | fail | getmap | newfs | remove_data_pool | rm | rmfailed | set | set_max_mds | set_state | setmap | stat | stop | tell ] &hellip;</p></li>
<li><p>ceph mon [ add | dump | getmap | remove | stat ] &hellip;</p>

<pre><code>  增删查改mon
  dump根据 map的版本号转存格式化的monmap
  getmap 获取monmap
</code></pre></li>
<li><p>ceph mon_status</p>

<pre><code>  查看mon 详细状态信息
</code></pre></li>
<li><p>ceph osd [ blacklist | blocked-by | create | deep-scrub | df | down | dump | erasure-code-profile | find | getcrushmap | getmap | getmaxosd | in | lspools | map | metadata | out | pause | perf | pg-temp | primary-affinity | primary-temp | repair | reweight | reweight-by-pg | rm | scrub | set | setcrushmap | setmaxosd  | stat | thrash | tree | unpause | unset ] &hellip;</p>

<pre><code> blacklist 黑名单列表 ls
 tree 获取osd树形结构
 create 创建osd 根据 uuid需要自己输入
 df osd数据使用列表
 stat osd状态
 rm 删除指定osd
 lspools 列举pools列表
 reweight 重新设置权重（权重值还未清楚分配原理）
 in 手动标记osd状态 
 down手动停止osd
</code></pre></li>
<li><p>ceph osd crush [ add | add-bucket | create-or-move | dump | get-tunable | link | move | remove | rename-bucket | reweight | reweight-all | reweight-subtree | rm | rule | set | set-tunable | show-tunables | tunables | unlink ] &hellip;</p>

<pre><code>  一些crush算法的操作 （还未实验）
</code></pre></li>
<li><p>ceph osd pool [ create | delete | get | get-quota | ls | mksnap | rename | rmsnap | set | set-quota | stats ] &hellip;</p>

<pre><code>  对pool的 
  create增加 
  delete删除 
  get获取信息（并不清楚作用）
  get-quota获取配额信息 
  ls列表 
  rename重命名 
  stats状态 
  set设置信息
</code></pre></li>
<li><p>ceph osd tier [ add | add-cache | cache-mode | remove | remove-overlay | set-overlay ] &hellip;</p>

<pre><code>  未实验
</code></pre></li>
<li><p>ceph pg [ debug | deep-scrub | dump | dump_json | dump_pools_json | dump_stuck | force_create_pg | getmap | ls | ls-by-osd | ls-by-pool | ls-by-primary | map | repair | scrub | send_pg_creates | set_full_ratio | set_nearfull_ratio | stat ] &hellip;</p>

<pre><code>  dump 获取集群转储信息（目前并不知道作用）
  dump_json获取 placementgroup内所有json数据 
  dump_pools_json 获取pool的json数据
</code></pre></li>
<li><p>ceph quorum [ enter | exit ]</p>

<pre><code>  未实验
</code></pre></li>
<li><p>ceph quorum_status</p>

<pre><code>  未实验
</code></pre></li>
<li><p>ceph report { <tags> [ <tags>&hellip; ] }</p>

<pre><code>  未实验
</code></pre></li>
<li><p>ceph scrub</p>

<pre><code>  未实验
</code></pre></li>
<li><p>ceph status</p>

<pre><code>  ceph 集群状态 和 ceph -s -w 一个作用
</code></pre></li>
<li><p>ceph sync force {&ndash;yes-i-really-mean-it} {&ndash;i-know-what-i-am-doing}</p>

<pre><code> 未实验
</code></pre></li>
<li><p>ceph tell <name (type.id)> <args> [<args>&hellip;]</p>

<pre><code> 未实验
</code></pre></li>
<li><p>ceph version</p>

<pre><code>  查看ceph版本
</code></pre></li>
</ul>


<h2>Rados</h2>

<p>对象存储工具</p>

<ul>
<li><p>-p pool, &ndash;pool pool</p>

<pre><code>  选择要操作的数据池
</code></pre></li>
<li><p>-s snap, &ndash;snap snap</p>

<pre><code>  从给定的poo中读取快照
</code></pre></li>
<li><p>-i infile</p></li>
<li><p>-o outfile</p></li>
<li><p>-c ceph.conf, &ndash;conf=ceph.conf</p>

<pre><code>  选择配置文件位置
</code></pre></li>
<li><p>-m monaddress[:port]</p>

<pre><code>链接指定的Monitor
</code></pre></li>
</ul>


<h5>GLOBAL COMMANDS</h5>

<ul>
<li><p>lspools</p>

<pre><code>列出所有对象池
</code></pre></li>
<li><p>df</p>

<pre><code>  磁盘使用情况
</code></pre></li>
<li><p>mkpool</p>

<pre><code>  创建对象池    
</code></pre></li>
<li><p>rmpool foo</p>

<pre><code>  删除对象池 注意格式
  rados rmpool foo foo --yes-i-really-really-mean-it foo
</code></pre></li>
</ul>


<h5>POOL SPECIFIC COMMANDS</h5>

<ul>
<li><p>get name outfile</p>

<pre><code>  通过对象名字 从集群中 将文件读取到指定目录

  用例 rados get TestAdmin /opt/ning/1.txt --pool data
</code></pre></li>
<li><p>put name infile</p>

<pre><code>  通过对象名称 将指定文件数据接入集群中

  用例 rados put testobject /opt/ning/1.txt --pool data
</code></pre></li>
<li><p>rm name</p>

<pre><code>  通过对象名称 移除对象
</code></pre></li>
</ul>


<h6>暂未实验</h6>

<hr />

<pre><code>   listwatchers name
          List the watchers of object name.

   ls outfile
          List objects in given pool and write to outfile.

   lssnap List snapshots for given pool.

   clonedata srcname dstname --object-locator key
          Clone object byte data from srcname to dstname.  Both objects must be stored with the locator key key (usually either srcname or dstname).
          Object attributes and omap keys are not copied or cloned.

   mksnap foo
          Create pool snapshot named foo.

   rmsnap foo
          Remove pool snapshot named foo.

   bench seconds mode [ -b objsize ] [ -t threads ]
          Benchmark for seconds. The mode can be write, seq, or rand. seq and rand are read benchmarks, either sequential or random. Before  running
          one of the reading benchmarks, run a write benchmark with the --no-cleanup option. The default object size is 4 MB, and the default number      of simulated threads (parallel writes) is 16.

   cleanup

   listomapkeys name
          List all the keys stored in the object map of object name.

   listomapvals name
          List all key/value pairs stored in the object map of object name.  The values are dumped in hexadecimal.

   getomapval name key
          Dump the hexadecimal value of key in the object map of object name.

   setomapval name key value
          Set the value of key in the object map of object name.

   rmomapkey name key
          Remove key from the object map of object name.

   getomapheader name
          Dump the hexadecimal value of the object map header of object name.

   setomapheader name value
          Set the value of the object map header of object name.
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[ceph多节点部署]]></title>
    <link href="http://guangningsun.github.io/blog/2016/03/09/cephduo-jie-dian-bu-shu/"/>
    <updated>2016-03-09T18:20:00+08:00</updated>
    <id>http://guangningsun.github.io/blog/2016/03/09/cephduo-jie-dian-bu-shu</id>
    <content type="html"><![CDATA[<h3>实验环境</h3>

<hr />

<ul>
<li>VMWare 虚拟4台 centos7.1 测试机</li>
<li>ceph osd不支持操作系统所在挂载盘，需要新挂载盘 并格式化xfs 并按照配置文件创建存储目录</li>
<li><p>查看 /var/lib/ceph/osd/ceph-*</p>

<p> 官方文档推荐用ceph-depoly 半自动化部署工具，详细部署过程后续补充</p>

<p> 但部署过程中出现错误较多，所以改为先手动配置</p></li>
<li><p>ceph-deploy install &ndash;rgw nodename 安装ceph gw并可更新ceph版本</p></li>
</ul>


<h3>打通各节点的ssh机器互信</h3>

<ul>
<li><p>ssh-keygen -t rsa -f .ssh/id_rsa -P &lsquo;&rsquo;</p>

<p>  此处一路回车并没有设置验证码，生产可能并不允许。</p></li>
</ul>


<h3>向被授权的主机上拷贝</h3>

<ul>
<li>ssh-copy-id -i .ssh/id_rsa.pub <a href="&#x6d;&#97;&#105;&#108;&#116;&#111;&#58;&#114;&#111;&#x6f;&#x74;&#x40;&#x31;&#57;&#50;&#46;&#x31;&#54;&#56;&#x2e;&#x32;&#x38;&#x2e;&#49;&#51;&#x30;">&#114;&#x6f;&#x6f;&#x74;&#x40;&#49;&#x39;&#x32;&#46;&#x31;&#54;&#x38;&#46;&#50;&#56;&#x2e;&#x31;&#x33;&#x30;</a></li>
</ul>


<h3>安装ntpdate</h3>

<ul>
<li>yum -y install ntpdate</li>
</ul>


<h3>开始同步时间</h3>

<ul>
<li>ntpdate time.windows.com</li>
</ul>


<h3>修改每台机器的主机名 (hostname)  ( mon,mds,osd,client 都必须设置 )</h3>

<hr />

<pre><code>    echo 192.168.28.128 ceph-mon &gt;&gt; /etc/hosts

    echo 192.168.28.129 ceph-osd0 &gt;&gt; /etc/hosts

    echo 192.168.28.130 ceph-osd1 &gt;&gt; /etc/hosts

    echo 192.168.28.131 ceph-osd2 &gt;&gt; /etc/hosts
</code></pre>

<h3>yum更新，安装相关依赖包(适用于mon,mds,osd)</h3>

<hr />

<pre><code>    rpm --import 'https://ceph.com/git/?p=ceph.git;a=blob_plain;f=keys/release.asc'
    rpm -Uvh http://mirrors.yun-idc.com/epel/7/x86_64/e/epel-release-7-5.noarch.rpm
    yum install snappy leveldb gdisk python-argparse gperftools-libs -y
    rpm -Uvh http://ceph.com/rpm-dumpling/el7/noarch/ceph-release-1-0.el7.centos.noarch.rpm
    yum install ceph-deploy -y
    yum install ceph -y

    yum install btrfs-progs           (适用于所有osd) 
</code></pre>

<h3>配置/etc/ceph/ceph.conf，  (适用于mon,mds,osd)</h3>

<hr />

<p>vi /etc/ceph/ceph.conf</p>

<hr />

<p>[global]</p>

<p>public_network = 192.168.28.1/24</p>

<p>pid_file = /var/run/ceph/$name.pid</p>

<p>auth_cluster_required = none</p>

<p>auth_service_required = none</p>

<p>auth_client_required = none</p>

<p>keyring = /etc/ceph/keyring.$name</p>

<p>osd_pool_default_size = 1</p>

<p>osd_pool_default_min_size = 1</p>

<p>osd_pool_default_crush_rule = 0</p>

<p>osd_crush_chooseleaf_type = 1</p>

<p>[mon]</p>

<p>mon_data = /var/lib/ceph/mon/$name</p>

<p>mon_clock_drift_allowed = .15</p>

<p>keyring = /etc/ceph/keyring.$name</p>

<p>[mon.0]</p>

<p>host = ceph-mon</p>

<p>mon_addr = 192.168.28.129:6789</p>

<p>[osd]</p>

<p>osd_recovery_max_active = 5</p>

<p>osd_mkfs_type = xfs</p>

<p>keyring = /etc/ceph/keyring.$name</p>

<p>[osd.0]</p>

<p>host = ceph-osd0</p>

<p>devs = /dev/sda3</p>

<p>[osd.1]</p>

<p>host = ceph-osd1</p>

<p>devs = /dev/sda3</p>

<p>[osd.2]</p>

<p>host = ceph-osd2</p>

<p>devs = /dev/sda3</p>

<p>[client.radosgw.gateway]</p>

<p>host = ceph-mon</p>

<p>log file = /var/log/radosgw/client.radosgw.ustack.log</p>

<h3>创建目录 ( osd )</h3>

<hr />

<pre><code>    mkdir -p /var/lib/ceph/osd/ceph-* 默认为该目录下存放数据，可以在[osd]中配置存放路径
</code></pre>

<h3>启动ceph(在mon上执行)</h3>

<hr />

<pre><code>    初始化：mkcephfs -a -c /etc/ceph/ceph.conf
    此时数据目录下应为空
    /etc/init.d/ceph -a start
</code></pre>

<h3>启动时遇到问题</h3>

<hr />

<pre><code>    遇到 Error ENOENT: osd.0 does not exist.  create it before updating the crush map
    执行如下代码: ceph osd create
    然后在执行   /etc/init.d/ceph -a start       既可完成（仍需清空数据目录）
</code></pre>

<h3>启动后健康检查</h3>

<hr />

<pre><code>ceph health      #也可以使用ceph -s命令查看状态  (如果返回的是HEALTH_OK，则代表成功！) 
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[ceph原理分析]]></title>
    <link href="http://guangningsun.github.io/blog/2016/03/01/ceph-analyze/"/>
    <updated>2016-03-01T15:05:00+08:00</updated>
    <id>http://guangningsun.github.io/blog/2016/03/01/ceph-analyze</id>
    <content type="html"><![CDATA[<h1>1.分析块存储、文件存储、对象存储</h1>

<ul>
<li><h3>1.1 什么是块存储</h3></li>
</ul>


<hr />

<ul>
<li><p>维基百科中定义为：块（block）数据存储，一种将电子数据存储为相同大小的方法</p></li>
<li><p>DSA和SAN是两种经典的块存储模型：</p>

<pre><code>  1） DAS（Direct Attach STorage）：是直接连接于主机服务器的一种储存方式。（主要代表设备就是个人PC电脑，硬盘直接挂载在服务器上）。
  - 每一台主机服务器有独立的储存设备，每台主机服务器的储存设备无法互通，需要跨主机存取资料时，必须经过相对复杂的设定，若主机服务器分属不同的操作系统，要存取彼此的资料，更是复杂，有些系统甚至不能存取。（windows数据优盘在mint系统就无法读取）
  - 通常用在单一网络环境下且数据交换量不大，性能要求不高的环境下，可以说是一种应用较为早的技术实现。

  2）SAN（Storage Area Network）：是一种用高速（光纤）网络联接专业主机服务器的一种储存方式，此系统会位于主机群的后端，它使用高速I/O 联结方式。
  - 例：（如 SCSI及 Fibre- Channels SAN应用在对网络速度要求高、对数据的可靠性和安全性要求高、对数据共享的性能要求高的应用环境中，特点是代价高，性能好。例如电信、银行的大数据量关键应用。）        
  * 它采用SCSI 块I/O的命令集，通过在磁盘或FC（Fiber Channel）级的数据访问提供高性能的随机I/O和数据吞吐率，它具有高带宽、低延迟的优势，在高性能计算中占有一席之地，但是由于SAN系统的价格较高，且可扩展性较差，已不能满足成千上万个CPU规模的系统。
  - 对于用户来说，SAN好比是一块大磁盘，用户可以根据需要随意将SAN格式化成想要的文件系统来使用。SAN在网络中通过iSCSI（IPSAN）协议连接，属block及存储，但可扩展性较差。
</code></pre></li>
</ul>


<p><img src="http://7xrcb8.com1.z0.glb.clouddn.com/%E5%9D%97%E5%AD%98%E5%82%A8.png" alt="storage" /></p>

<ul>
<li><h3>1.2  什么是文件存储</h3></li>
</ul>


<hr />

<ul>
<li><p>文件系统出现解决了块存储模式共享困难的弊端</p>

<pre><code>  通常，NAS产品都是文件级存储。  NAS（Network Attached Storage）：是一套网络储存设备，通常是直接连在网络上并提供资料存取服务，一套 NAS 储存设备就如同一个提供数据文件服务的系统，特点是性价比高。例如教育、政府、企业等数据存储应用。
  - 对于用户来说，NAS好比是一个共享文件夹，文件系统已经存在，用户可以直接将自己的数据存放在NAS上。NAS以文件为传输协议，开销很大，不利于在高性能集群中使用

  * 它采用NFS或CIFS命令集访问数据，以文件为传输协议（FTP  ’ （学校里有ftp服务器，下载过期的资料）FTP 是File Transfer Protocol（文件传输协议）的英文简称，而中文简称为“文传协议”。用于Internet上的控制文件的双向传输。同时，它也是一个应用程序（Application）。基于###不同的操作系统有不同的FTP应用程序####，而所有这些应用程序都遵守同一种协议以传输文件。在FTP的使用当中，用户经常遇到两个概念："下载"（Download）和"上传"（Upload）。"下载"文件就是从远程主机拷贝文件至自己的计算机上；"上传"文件就是将文件从自己的计算机中拷贝至远程主机上。用Internet语言来说，用户可通过客户机程序向（从）远程主机上传（下载）文件。‘通过TCP/IP实现网络化存储，可扩展性好、价格便宜、用户易管理，如目前在集群计算中应用较多的NFS文件系统。

  - window  和 linux 内核都有自己的ftp客户端
  - 基于不同的 操作系统可以共享文件
  - 但由于NAS的协议开销高、带宽低、延迟大，不利于在高性能集群中应用。
</code></pre></li>
<li><h3>1.3  什么是对象存储</h3></li>
</ul>


<hr />

<pre><code>    针对Linux集群对存储系统高性能和数据共享的需求，国际上希望能有效结合SAN和NAS系统的优点，支持直接访问磁盘以提高性能，通过共享的文件和元数据以简化管理，对象存储应用而生。

    - 维基百科：对象存储（也称为基于对象的存储[1]）是管理数据作为对象，而不是其他存储架构象它管理数据作为文件层次结构和块存储用于管理数据扇区内的块的文件系统的存储体系结构和跟踪。 
    - 如下图中的每个对象通常包括数据本身，元数据的可变数量，以及一个全局唯一标识符。对象存储可在多个级别，包括设备水平（对象的存储设备），系统级和接口电平来实现。在每一种情况下，对象存储旨在使未由其他存储架构寻址能力，这样可以由应用程序直接编程，可以跨越物理硬件的多个实例命名空间接口，和数据管理功能。
    - 例如数据复制和数据分配在对象级粒度。HDFS数据存储在块力度，编写程序时候要了解block数据长度，偏移量以及每个byte字节所代表含义等信息，再计算数据
</code></pre>

<p><img src="http://7xrcb8.com1.z0.glb.clouddn.com/osd%E5%AD%98%E5%82%A8.png" alt="" /></p>

<pre><code>    而所谓对象存储，就是每个数据对应着一个唯一的id，在面向对象存储中。
    - 不再有类似文件系统的目录层级结构，完全扁平化存储，即可以根据对象的id直接定位到数据的位置，这一点类似SAN。
    - 而每个数据对象即包含元数据又包括存储数据，含有文件的概念，这一点类似NAS。
    - 除此之外，用户不必关系数据对象的安全性，数据恢复，自动负载平衡等等问题，这些均由对象存储系统自身完成。
    - 而且，面向对象存储还解决了SAN面临的有限扩充和NAS传输性能开销大问题，能够实现海量数据存储。
</code></pre>

<p><img src="http://7xrcb8.com1.z0.glb.clouddn.com/%E6%99%AE%E9%80%9A%E5%AD%98%E5%82%A8%E4%BA%8E%E5%9D%97%E5%AD%98%E5%82%A8%E5%AF%B9%E6%AF%94.png" alt="osd &amp; common" /></p>

<p>总体上来讲，对象存储同兼具SAN高速直接访问磁盘特点及NAS的分布式共享特点。</p>

<ul>
<li><h3>1.4  各类存储总结</h3></li>
</ul>


<hr />

<ul>
<li><h4>1.4.1传统存储与OSD存储对比</h4>

<pre><code>  - LBA，全称为Logical Block Address，是PC数据存储装置上用来表示数据所在位置的通用机制，我们最常见到使用它的装置就是硬盘。
</code></pre>

<p><img src="http://7xrcb8.com1.z0.glb.clouddn.com/osd%E5%AD%98%E5%82%A8%E4%B8%8E%E5%9D%97%E5%AD%98%E5%82%A8.png" alt="storage end" /></p></li>
</ul>


<h1>2.经典分布式存储HDFS存储原理及特点</h1>

<h3>2.1、HDFS的主要设计理念</h3>

<hr />

<p>HDFS是Hadoop Distribute File System 的简称，也就是Hadoop的一个分布式文件系统。</p>

<pre><code>    1、存储超大文件 这里的“超大文件”是指几百MB、GB甚至TB级别的文件。
    2、最高效的访问模式是 一次写入、多次读取(流式数据访问)HDFS存储的数据集作为hadoop的分析对象。
    3、运行在普通廉价的服务器上HDFS设计理念之一就是让它能运行在普通的硬件之上，即便硬件出现故障，也可以通过容错策略来保证数据的高可用。
</code></pre>

<h3>2.2 HDFS基本概念</h3>

<hr />

<pre><code>    数据块（block）：大文件会被分割成多个block进行存储，block大小默认为64MB。每一个block会在多个datanode上存储多份副本，默认是3份。

    namenode：namenode负责管理文件目录、文件和block的对应关系以及block和datanode的对应关系。

    datanode：datanode就负责存储了，当然大部分容错机制都是在datanode上实现的。
</code></pre>

<h3>2.3 HDFS基本架构图</h3>

<hr />

<p><img src="http://7xrcb8.com1.z0.glb.clouddn.com/hdfs%E6%9E%B6%E6%9E%84%E5%9B%BE.png" alt="" /></p>

<h5>图中有几个概念需要介绍一下</h5>

<pre><code>    Rack 是指机柜的意思，一个block的三个副本通常会保存到两个或者两个以上的机柜中（当然是机柜中的服务器），这样做的目的是做防灾容错，因为发生一个机柜掉电或者一个机柜的交换机挂了的概率还是蛮高的。
</code></pre>

<h3>2.4 HDFS写文件流程</h3>

<hr />

<p><img src="http://7xrcb8.com1.z0.glb.clouddn.com/HDFS%E5%86%99%E6%96%87%E4%BB%B6.png" alt="" /></p>

<ul>
<li><p>思考：</p>

<pre><code>  1.在datanode执行create file后，namenode采用什么策略给client分配datanode？
  2.顺序写入三个datanode，写入过程中有一个datanode挂掉了，如何容错？
  3.client往datanode写入数据时挂掉了，怎么容错？
</code></pre></li>
</ul>


<h3>2.5 HDFS读文件流程</h3>

<hr />

<p><img src="http://7xrcb8.com1.z0.glb.clouddn.com/HDFS%E8%AF%BB%E6%96%87%E4%BB%B6.png" alt="" /></p>

<h3>2.6 HDFS NN 高可用容错机制</h3>

<hr />

<ul>
<li><h4>secondary NN 工作原理</h4>

<p><img src="http://7xrcb8.com1.z0.glb.clouddn.com/NN%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6.jpeg" alt="" /></p></li>
<li><h4>NN HA 高可用机制</h4>

<p><img src="http://tech.uc.cn/wp-content/uploads/2012/08/HA%E6%9E%B6%E6%9E%84%E5%9B%BE.png" alt="" /></p></li>
</ul>


<h3>2.6 HDFS的弊端</h3>

<hr />

<pre><code>    1、将HDFS用于对数据访问要求低延迟的场景,由于HDFS是为高数据吞吐量应用而设计的，必然以高延迟为代价。
    2、存储大量小文件,HDFS中元数据（文件的基本信息）存储在namenode的内存中，而namenode为单点，小文件数量大到一定程度，namenode内存就吃不消了。
    3、扔会丢失一部分数据
</code></pre>

<h1>3.CEPH的工作原理</h1>

<h2>3.1 CEPH生态环境</h2>

<hr />

<ul>
<li><p>Ceph是统一存储系统，支持三种接口。</p>

<pre><code>  Object：有原生的API，而且也兼容Swift和S3的API
  Block：支持精简配置、快照、克隆
  File：Posix接口，支持快照
</code></pre></li>
<li><p>Ceph也是分布式存储系统，它的特点是：</p>

<pre><code>  高扩展性：使用普通x86服务器，支持10~1000台服务器，支持TB到PB级的扩展。
  高可靠性：没有单点故障，多数据副本，自动管理，自动修复。
  高性能：数据分布均衡，并行化度高。对于objects storage和block storage,不需要元数据服务器。
  sage论文中说是可以无限扩展，但目前还没有证实
</code></pre>

<p><img src="http://7xrcb8.com1.z0.glb.clouddn.com/ceph%E6%9E%B6%E6%9E%84.png" alt="" /></p></li>
</ul>


<h6>一个Ceph集群由两种类型的后台进程（Daemon）组成：</h6>

<pre><code>    Ceph的底层是RADOS，它的意思是“A reliable, autonomous, distributed object storage”。 RADOS由两个组件组成：
    OSD Daemon： Object Storage Device，提供存储资源。
    Ceph Monitor：维护整个Ceph集群的全局状态。
</code></pre>

<h6>Ceph OSD Daemon</h6>

<pre><code>Object Storage Device（OSD）是Ceph集群中的重要组成部分。OSD可以存储文件或数据的内容，它使用文件系统来存储数据。OSD Daemon主要负责管理集群中的所有磁盘。OSD Daemon还负责在本地文件系统存储数据，并为不同的客户软件或存取媒介通过网络提供数据访问。而且，OSD Daemon还负责添加和删除磁盘，磁盘分区，管理OSD、低层空间管理，提供安全措施和磁盘数据的可复制性。
</code></pre>

<h6>Ceph Monitor</h6>

<pre><code>Ceph Monitor也是一种Ceph OSD Daemon，它主要负责管理全部集群。当你运行一个Ceph集群时，你就会需要Ceph Monitor每天帮你检查集群的健康情况和状态。管理一个集群需要每天做很多工作比如检测所有OSD的状态和文件系统或块数据的状态。你可以通过Ceph Monitor来管理负载均衡和数据响应的详细信息。为了更好的了解Ceph集群的工作原理，我们来看看它是如何处理三种类型数据存储的机制。
</code></pre>

<h6>Ceph Object storage</h6>

<pre><code>当向Ceph写入数据时，Ceph通过内部机制自动跨集群标记和复制数据。Ceph存储对象数据时，不仅可以通过调用Ceph内部的API来实现，还可以通过亚马逊的S3服务或AWS REST提供的API来实现。Ceph块存储机制提供了RADOS（Reliable Autonomic Distributed Object Store）服务。RADOS服务存储机制中不可或缺的；RADOS服务通过使用节点中安装的软件管理工具能够扩展千级的硬件设备（通常被应用为“Nodes“）。
</code></pre>

<h6>Ceph Block Storage</h6>

<pre><code>Ceph的块存储模式使用户可以像挂载一个小型块设备一样挂载Ceph。在块数据存储级别上，RADOS服务也保证块数据的可扩展性。Librados就是包含在这一级别上的一个python类库，你可以使用librados类库和存储服务器或节点进行通信。Librados是一个开源的应用，你可以调整和增强它。Librados通过“RADOS Block Device“即RBD与后台进行交互。RBD不仅继承了Librados的功能，还能够为集群建立快照和恢复数据。
</code></pre>

<h6>Ceph File Storage</h6>

<pre><code>CephFS 是一个为Ceph集群设计的，且遵循POSIX标准的分布式文件系统。CephFS提供把数据目录和文件映射到存储在RADOS中对象的存储的服务。通过这种方式，CephFS和RADOS可以相互协作。在这里，RADOS动态均等地把数据分布到不同的节点上。这种文件系统支持无限的数据存储和更强的数据安全性。在文件存储集群系统中，Ceph因提供容量大和高可扩展性而闻名。请注意你可以同时把Ceph与btrfs或EXT4一起使用，但Red Hat推荐使用最新Linux内核（3.14版本或者更新版本）。
</code></pre>

<h5>Object Storage、Block Storage、FileSystem。Ceph另外两个组件是：</h5>

<pre><code>MDS：用于保存CephFS的元数据。
RADOS Gateway：对外提供REST接口，兼容S3和Swift的API。
</code></pre>

<h6>结论</h6>

<pre><code>Red Hat下的Ceph文件系统拥有性价比高、操作简单、集群数据高可靠性的特点。RedHat也一直为Ceph投入了很多人力，这也确保了Bug可的跟进速度，以及新特性的引入。由于Ceph是开源的，所以你可以按照你的需求随意修改它。
</code></pre>

<p><img src="http://7xrcb8.com1.z0.glb.clouddn.com/1.png" alt="" /></p>

<h2>3.2 CEPH存储原理</h2>

<hr />

<h3>3.2.1 CEPH寻址流程</h3>

<hr />

<pre><code>    本节将对Ceph的工作原理和若干关键工作流程进行扼要介绍。如前所述，由于Ceph的功能实现本质上依托于RADOS，因而，此处的介绍事实上也是针对RADOS进行。对于上层的部分，特别是RADOS GW和RBD，由于现有的文档中（包括Sage的论文中）并未详细介绍。

    ‘不用查找算算就好’ 首先介绍RADOS中最为核心的、基于计算的对象寻址机制，然后说明对象存取的工作流程
</code></pre>

<h5>Ceph系统中的寻址流程如下图所示：</h5>

<p><img src="http://7xrcb8.com1.z0.glb.clouddn.com/ceph%E5%AF%BB%E5%9D%80%E6%B5%81%E7%A8%8B.png" alt="" /></p>

<pre><code>    1. File —— 此处的file就是用户需要存储或者访问的文件。对于一个基于Ceph开发的对象存储应用而言，这个file也就对应于应用中的“对象”，也就是用户直接操作的“对象”。

    2. Ojbect —— 此处的object是RADOS所看到的“对象”。Object与上面提到的file的区别是，object的最大size由RADOS限定（通常为2MB或4MB），以便实现底层存储的组织管理。因此，当上层应用向RADOS存入size很大的file时，需要将file切分成统一大小的一系列object（最后一个的大小可以不同）进行存储。为避免混淆，在本文中将尽量避免使用中文的“对象”这一名词，而直接使用file或object进行说明。

    3. PG（Placement Group）—— 顾名思义，PG的用途是对object的存储进行组织和位置映射。具体而言，一个PG负责组织若干个object（可以为数千个甚至更多），但一个object只能被映射到一个PG中，即，PG和object之间是“一对多”映射关系。同时，一个PG会被映射到n个OSD上，而每个OSD上都会承载大量的PG，即，PG和OSD之间是“多对多”映射关系。在实践当中，n至少为2，如果用于生产环境，则至少为3。一个OSD上的PG则可达到数百个。事实上，PG数量的设置牵扯到数据分布的均匀性问题。关于这一点，下文还将有所展开。

    4. OSD —— 即object storage device，前文已经详细介绍，此处不再展开。唯一需要说明的是，OSD的数量事实上也关系到系统的数据分布均匀性，因此其数量不应太少。在实践当中，至少也应该是数十上百个的量级才有助于Ceph系统的设计发挥其应有的优势。

    5. Failure domain —— 这个概念在论文中并没有进行定义，好在对分布式存储系统有一定概念的读者应该能够了解其大意。
</code></pre>

<h5>基于上述定义，解释寻址流程。具体而言Ceph中的寻址至少要经历以下三次映射：</h5>

<pre><code>    1. File -&gt; object映射

    这次映射的目的是，将用户要操作的file，映射为RADOS能够处理的object。其映射十分简单，本质上就是按照object的最大size对file进行切分，相当于RAID中的条带化过程。这种切分的好处有二：一是让大小不限的file变成最大size一致、可以被RADOS高效管理的object；二是让对单一file实施的串行处理变为对多个object实施的并行化处理。

    每一个切分后产生的object将获得唯一的oid，即object id。其产生方式也是线性映射，极其简单。图中，ino是待操作file的元数据，可以简单理解为该file的唯一id。ono则是由该file切分产生的某个object的序号。而oid就是将这个序号简单连缀在该file id之后得到的。举例而言，如果一个id为filename的file被切分成了三个object，则其object序号依次为0、1和2，而最终得到的oid就依次为filename0、filename1和filename2。

    这里隐含的问题是，ino的唯一性必须得到保证，否则后续映射无法正确进行。

    2. Object -&gt; PG映射

    在file被映射为一个或多个object之后，就需要将每个object独立地映射到一个PG中去。这个映射过程也很简单，如图中所示，其计算公式是：

    hash(oid) &amp; mask -&gt; pgid

    由此可见，其计算由两步组成。首先是使用Ceph系统指定的一个静态哈希函数计算oid的哈希值，将oid映射成为一个近似均匀分布的伪随机值。然后，将这个伪随机值和mask按位相与，得到最终的PG序号（pgid）。根据RADOS的设计，给定PG的总数为m（m应该为2的整数幂），则mask的值为m-1。因此，哈希值计算和按位与操作的整体结果事实上是从所有m个PG中近似均匀地随机选择一个。基于这一机制，当有大量object和大量PG时，RADOS能够保证object和PG之间的近似均匀映射。又因为object是由file切分而来，大部分object的size相同，因而，这一映射最终保证了，各个PG中存储的object的总数据量近似均匀。

    从介绍不难看出，这里反复强调了“大量”。只有当object和PG的数量较多时，这种伪随机关系的近似均匀性才能成立，Ceph的数据存储均匀性才有保证。为保证“大量”的成立，一方面，object的最大size应该被合理配置，以使得同样数量的file能够被切分成更多的object；另一方面，Ceph也推荐PG总数应该为OSD总数的数百倍，以保证有足够数量的PG可供映射。

    3. PG -&gt; OSD映射

    第三次映射就是将作为object的逻辑组织单元的PG映射到数据的实际存储单元OSD。如图所示，RADOS采用一个名为CRUSH的算法，将pgid代入其中，然后得到一组共n个OSD。这n个OSD即共同负责存储和维护一个PG中的所有object。前已述及，n的数值可以根据实际应用中对于可靠性的需求而配置，在生产环境下通常为3。具体到每个OSD，则由其上运行的OSD deamon负责执行映射到本地的object在本地文件系统中的存储、访问、元数据维护等操作。

    和“object -&gt; PG”映射中采用的哈希算法不同，这个CRUSH算法的结果不是绝对不变的，而是受到其他因素的影响。其影响因素主要有二：

    一是当前系统状态，也就是上文逻辑结构中曾经提及的cluster map。当系统中的OSD状态、数量发生变化时，cluster map可能发生变化，而这种变化将会影响到PG与OSD之间的映射。

    二是存储策略配置。这里的策略主要与安全相关。利用策略配置，系统管理员可以指定承载同一个PG的3个OSD分别位于数据中心的不同服务器乃至机架上，从而进一步改善存储的可靠性。

    因此，只有在系统状态（cluster map）和存储策略都不发生变化的时候，PG和OSD之间的映射关系才是固定不变的。在实际使用当中，策略一经配置通常不会改变。而系统状态的改变或者是由于设备损坏，或者是因为存储集群规模扩大。好在Ceph本身提供了对于这种变化的自动化支持，因而，即便PG与OSD之间的映射关系发生了变化，也并不会对应用造成困扰。事实上，Ceph正是需要有目的的利用这种动态映射关系。正是利用了CRUSH的动态特性，Ceph可以将一个PG根据需要动态迁移到不同的OSD组合上，从而自动化地实现高可靠性、数据分布re-blancing等特性。

    之所以在此次映射中使用CRUSH算法，而不是其他哈希算法，原因之一正是CRUSH具有上述可配置特性，可以根据管理员的配置参数决定OSD的物理位置映射策略；另一方面是因为CRUSH具有特殊的“稳定性”，也即，当系统中加入新的OSD，导致系统规模增大时，大部分PG与OSD之间的映射关系不会发生改变，只有少部分PG的映射关系会发生变化并引发数据迁移。这种可配置性和稳定性都不是普通哈希算法所能提供的。因此，CRUSH算法的设计也是Ceph的核心内容之一，具体介绍可以参考。
</code></pre>

<h3>3.2.2 CEPH寻址总结及问题解析</h3>

<hr />

<pre><code>    至此为止，Ceph通过三次映射，完成了从file到object、PG和OSD整个映射过程。通观整个过程，可以看到，这里没有任何的全局性查表操作需求。至于唯一的全局性数据结构cluster map，在后文中将加以介绍。可以在这里指明的是，cluster map的维护和操作都是轻量级的，不会对系统的可扩展性、性能等因素造成不良影响。
</code></pre>

<h6>一个可能出现的困惑是：为什么需要同时设计第二次和第三次映射？难道不重复么？分析如下：</h6>

<pre><code>    我们可以反过来想像一下，如果没有PG这一层映射，又会怎么样呢？在这种情况下，一定需要采用某种算法，将object直接映射到一组OSD上。如果这种算法是某种固定映射的哈希算法，则意味着一个object将被固定映射在一组OSD上，当其中一个或多个OSD损坏时，object无法被自动迁移至其他OSD上（因为映射函数不允许），当系统为了扩容新增了OSD时，object也无法被re-balance到新的OSD上（同样因为映射函数不允许）。这些限制都违背了Ceph系统高可靠性、高自动化的设计初衷。

    如果采用一个动态算法（例如仍然采用CRUSH算法）来完成这一映射，似乎是可以避免静态映射导致的问题。但是，其结果将是各个OSD所处理的本地元数据量爆增，由此带来的计算复杂度和维护工作量也是难以承受的。

    例如，在Ceph的现有机制中，一个OSD平时需要和与其共同承载同一个PG的其他OSD交换信息，以确定各自是否工作正常，是否需要进行维护操作。由于一个OSD上大约承载数百个PG，每个PG内通常有3个OSD，因此，一段时间内，一个OSD大约需要进行数百至数千次OSD信息交换。然而，如果没有PG的存在，则一个OSD需要和与其共同承载同一个object的其他OSD交换信息。由于每个OSD上承载的object很可能高达数百万个，因此，同样长度的一段时间内，一个OSD大约需要进行的OSD间信息交换将暴涨至数百万乃至数千万次。而这种状态维护成本显然过高。

    综上引入PG的好处至少有二：一方面实现了object和OSD之间的动态映射，从而为Ceph的可靠性、自动化等特性的实现留下了空间；另一方面也有效简化了数据的存储组织，大大降低了系统的维护管理开销。理解这一点，对于彻底理解Ceph的对象寻址机制，是十分重要的。
</code></pre>

<h3>3.3.3 数据操作流程</h3>

<hr />

<pre><code>此处将首先以file写入过程为例，对数据操作流程进行说明。为简化说明，便于理解，此处进行若干假定。首先，假定待写入的file较小，无需切分，仅被映射为一个object。其次，假定系统中一个PG被映射到3个OSD上。
</code></pre>

<h6>file写入流程如下图表示：</h6>

<p><img src="http://7xrcb8.com1.z0.glb.clouddn.com/Ceph%E5%86%99%E5%85%A5%E6%B5%81%E7%A8%8B.png" alt="" /></p>

<pre><code>如图所示，当某个client需要向Ceph集群写入一个file时，首先需要在本地完成5.1节中所叙述的寻址流程，将file变为一个object，然后找出存储该object的一组三个OSD。这三个OSD具有各自不同的序号，序号最靠前的那个OSD就是这一组中的Primary OSD，而后两个则依次是Secondary OSD和Tertiary OSD。

找出三个OSD后，client将直接和Primary OSD通信，发起写入操作（步骤1）。Primary OSD收到请求后，分别向Secondary OSD和Tertiary OSD发起写入操作（步骤2、3）。当Secondary OSD和Tertiary OSD各自完成写入操作后，将分别向Primary OSD发送确认信息（步骤4、5）。当Primary OSD确信其他两个OSD的写入完成后，则自己也完成数据写入，并向client确认object写入操作完成（步骤6）。

之所以采用这样的写入流程，本质上是为了保证写入过程中的可靠性，尽可能避免造成数据丢失。同时，由于client只需要向Primary OSD发送数据，因此，在Internet使用场景下的外网带宽和整体访问延迟又得到了一定程度的优化。

当然，这种可靠性机制必然导致较长的延迟，特别是，如果等到所有的OSD都将数据写入磁盘后再向client发送确认信号，则整体延迟可能难以忍受。因此，Ceph可以分两次向client进行确认。当各个OSD都将数据写入内存缓冲区后，就先向client发送一次确认，此时client即可以向下执行。待各个OSD都将数据写入磁盘后，会向client发送一个最终确认信号，此时client可以根据需要删除本地数据。

分析上述流程可以看出，在正常情况下，client可以独立完成OSD寻址操作，而不必依赖于其他系统模块。因此，大量的client可以同时和大量的OSD进行并行操作。同时，如果一个file被切分成多个object，这多个object也可被并行发送至多个OSD。

从OSD的角度来看，由于同一个OSD在不同的PG中的角色不同，因此，其工作压力也可以被尽可能均匀地分担，从而避免单个OSD变成性能瓶颈。

如果需要读取数据，client只需完成同样的寻址过程，并直接和Primary OSD联系。目前的Ceph设计中，被读取的数据仅由Primary OSD提供。但目前也有分散读取压力以提高性能的讨论。
</code></pre>

<h2>3.3 普通Hash算法及一致性Hash算法</h2>

<hr />

<h3>3.3.1 普通Hash算法</h3>

<hr />

<pre><code>    哈希表就是一种以 键-值(key-indexed) 存储数据的结构，我们只要输入待查找的值即key，即可查找到其对应的值。

    哈希的思路很简单，如果所有的键都是整数，那么就可以使用一个简单的无序数组来实现：将键作为索引，值即为其对应的值，这样就可以快速访问任意键的值。这是对于简单的键的情况，我们将其扩展到可以处理更加复杂的类型的键。
</code></pre>

<ul>
<li><p>使用哈希查找有两个步骤:</p>

<pre><code>  使用哈希函数将被查找的键转换为数组的索引。在理想的情况下，不同的键会被转换为不同的索引值，但是在有些情况下我们需要处理多个键被哈希到同一个索引值的情况。所以哈希查找的第二个步骤就是处理冲突
</code></pre></li>
<li><p>处理哈希碰撞冲突。有很多处理哈希碰撞冲突的方法，本文后面会介绍拉链法和线性探测法。</p>

<pre><code>  哈希表是一个在时间和空间上做出权衡的经典例子。如果没有内存限制，那么可以直接将键作为数组的索引。那么所有的查找时间复杂度为O(1)；如果没有时间限制，那么我们可以使用无序数组并进行顺序查找，这样只需要很少的内存。哈希表使用了适度的时间和空间来在这两个极端之间找到了平衡。只需要调整哈希函数算法即可在时间和空间上做出取舍。
</code></pre></li>
<li><p>数据分布是分布式存储系统的一个重要部分，数据分布算法至少要考虑以下三个因素：</p>

<pre><code>  1) 故障域隔离。同份数据的不同副本分布在不同的故障域，降低数据损坏的风险；

  2) 负载均衡。数据能够均匀地分布在磁盘容量不等的存储节点，避免部分节点空闲部分节点超载，从而影响系统性能；

  3) 控制节点加入离开时引起的数据迁移量。当节点离开时，最优的数据迁移是只有离线节点上的数据被迁移到其它节点，而正常工作的节点的数据不会发生迁移。

  对象存储中一致性Hash和Ceph的CRUSH算法是使用地比较多的数据分布算法。在Aamzon的Dyanmo键值存储系统中采用一致性Hash算法，并且对它做了很多优化。OpenStack的Swift对象存储系统也使用了一致性Hash算法。
</code></pre></li>
</ul>


<h3>3.3.2 一致性Hash算法</h3>

<hr />

<pre><code>    假设数据为 x ，存储节点数目为 N 。将数据分布到存储节点的最直接做法是，计算数据 x 的Hash值，并将结果同节点数目 N 取余数，余数就是数据x的目的存储节点。即目的存储节点为 Hash(x) % N 。对数据计算Hash值的目的为了可以让数据均匀分布在N个节点中。这种做法的一个严重问题是，当加入新节点或则节点离开时，几乎所有数据都会受到影响，需要重新分布。因此，数据迁移量非常大。
</code></pre>

<p><img src="http://7xrcb8.com1.z0.glb.clouddn.com/%E6%99%AE%E9%80%9AHash.png" alt="" /></p>

<pre><code>    一致性Hash算法将数据和存储节点映射到同个Hash空间，如上图所示。Hash环中的3存储节点把Hash空间划分成3个分区，每个存储节点负责一个分区上的数据。例如，落在分区[N2,N0]上的数据存储在节点N0。

    一致性Hash算法能够很好地控制节点加入离开导致的迁移数据的数量。如图(b)所示，当节点N0离开时，原来由它负责的[N2, N0]分区将同[N0, N1]分区合并成[N2, N1]分区，并且都由节点N1负责。也就是说，本来存储在节点N0上的数据都迁移到节点N1，而原来存储在N1和N2节点的数据不受影响。图(c)给出了当节点N3加入时，原来[N2, N0]分区分裂成[N3, N0]和[N2, N3]两个分区，其中[N3, N0]分区上是数据迁移到新加入的N3节点。
</code></pre>

<h4>虚拟节点</h4>

<pre><code>一致性Hash的一个问题是，存储节点不能将Hash空间划分地足够均匀。如上图(a)所示，分区[N2, N0]的大小几乎是其它两个分区大小之和。这容易让负责该分区的节点N0负载过重。假设3个节点的磁盘容量相等，那么当节点N0的磁盘已经写满数据时其它两个节点上的磁盘还有很大的空闲空间，但此时系统已经无法继续向分区[N2, N0]写入数据，从而造成资源浪费。
</code></pre>

<p><img src="http://7xrcb8.com1.z0.glb.clouddn.com/%E8%99%9A%E6%8B%9F%E8%8A%82%E7%82%B9HASH.png" alt="" /></p>

<pre><code>虚拟节点是相对于物理存储节点而言的，虚拟节点负责的分区上的数据最终存储到其对应的物理节点。在一致性Hash中引入虚拟节点可以把Hash空间划分成更多的分区，从而让数据在存储节点上的分布更加均匀。如上图(b)所示，黄颜色的节点代表虚拟节点，Ni_0代表该虚拟节点对应于物理节点i的第0个虚拟节点。增加虚拟节点后，物理节点N0负责[N1_0, N0]和[N0, N0_0]两个分区，物理节点N1负责[N0_0, N1]和[N2_0, N1_0]两个分区，物理节点N2负责[N2, N1]和[N2_0, N2]两个分区，三个物理节点负责的总的数据量趋于平衡。

实际应用中，可以根据物理节点的磁盘容量的大小来确定其对应的虚拟节点数目。虚拟节点数目越多，节点负责的数据区间也越大。
</code></pre>

<h4>分区与分区位置</h4>

<pre><code>前文提到，当节点加入或者离开时，分区会相应地进行分裂或合并。这不对新写入的数据构成影响，但对已经写入到磁盘的数据需要重新计算Hash值以确定它是否需要迁移到其它节点。因为需要遍历磁盘中的所有数据，这个计算过程非常耗时。如下图(a)所示，分区是由落在Hash环上的虚拟节点 Ti 来划分的，并且分区位置(存储分区数据的节点)也同虚拟节点相关，即存储到其顺时针方向的第1个虚拟节点。
</code></pre>

<p><img src="http://7xrcb8.com1.z0.glb.clouddn.com/%E5%88%86%E5%8C%BAHASH.png" alt="" /></p>

<pre><code>在Dynamo的论文中提出了分离分区和分区位置的方法来解决这个问题。该方法将Hash空间划分成固定的若干个分区，虚拟节点不再用于划分分区而用来确定分区的存储位置。如上图(b)所示，将Hash空间划分成[A,B]，[B,C], [C,D]和[D,A]四个固定的分区。虚拟节点用于确定分区位置，例如T1负责分区[B,C]，T2负责分区[C,D]，T0负责[D,A]和[A,B]两个分区。由于分区固定，因此迁移数据时可以很容易知道哪些数据需要迁移哪些数据不需要迁移。

上图(b)中虚拟节点T0负责了[D,A]和[A,B]两个分区的数据，这是由分区数目和虚拟节点数目不相同导致的。为让分区分布地更加均匀，Dyanmo提出了维持分区数目和虚拟节点数目相等的方法。这样每个虚拟节点负责一个分区，在物理节点的磁盘容量都相同并且虚拟节点数目都相同的情况下，每个物理节点负责的分区大小是完全相同的，从而可以达到最佳的数据分布。
</code></pre>

<h2>3.4 CEPH核心算法CRUSH原理</h2>

<pre><code>Ceph分布数据的过程：首先计算数据 x 的Hash值并将结果和PG数目取余，以得到数据 x 对应的 PG 编号。然后，通过CRUSH算法将PG映射到一组OSD中。最后把数据 x 存放到PG对应的OSD中。这个过程中包含了两次映射，第一次是数据 x 到PG的映射。如果把PG当作存储节点，那么这和文章开头提到的普通Hash算法一样。不同的是，PG是抽象的存储节点，它不会随着物理节点的加入或则离开而增加或减少，因此数据到PG的映射是稳定的。
</code></pre>

<p><img src="http://7xrcb8.com1.z0.glb.clouddn.com/CRUSH%E7%AE%97%E6%B3%95.png" alt="" /></p>

<pre><code>在这个过程中，PG起到了两个作用：第一个作用是划分数据分区。每个PG管理的数据区间相同，因而数据能够均匀地分布到PG上；第二个作用是充当Dyanmo中Token的角色，即决定分区位置。实际上，这和Dynamo中固定分区数目，以及维持分区数目和虚拟节点数目相等的原则是同一回事。

在没有多副本的情况下，Dynamo中分区的数据直接存储到Token，而每个Token对应唯一的一个物理存储节点。在多副本(假设副本数目为 N )的情况下，分区的数据会存储到连续的 N 个Token中。但这会引入一个新问题：因为副本必须保持在不同的物理节点，但是如果这组Token中存在两个或多个Token对应到同个物理存储节点，那么就必须要跳过这样的节点。Dynamo采用Preference列表来记录每个分区对应的物理节点。然而，Dynmao论文中没有详述分区的Preference列表如何选取物理节点，以及选取物理节点时该如何隔离故障域等问题。

(osd0, osd1, osd2 … osdn) = CRUSH(x)

Ceph的PG担当起Dynamo中Token、固定分区以及Preference列表的角色，解决的是同样的问题。PG的Acting集合对应于Dynamo的Preference列表。CRUSH算法解决了Dynamo论文中未提及的问题。
</code></pre>

<h3>OSD层级结构和权重大小</h3>

<ul>
<li><p>CRUSH算法的目的是，为给定的PG(即分区)分配一组存储数据的OSD节点。选择OSD节点的过程，要考虑以下几个因素：</p>

<pre><code>  1) PG在OSD间均匀分布。假设每个OSD的磁盘容量都相同，那么我们希望PG在每个OSD节点上是均匀分布的，也就是说每个OSD节点包含相同数目的PG。假如节点的磁盘容量不等，那么容量大的磁盘的节点能够处理更多数量的PG。 
  2) PG的OSD分布在不同的故障域。因为PG的OSD列表用于保存数据的不同副本，副本分布在不同的OSD中可以降低数据损坏的风险。
</code></pre></li>
</ul>


<p><img src="http://7xrcb8.com1.z0.glb.clouddn.com/osd%E6%9D%83%E9%87%8D.png" alt="" /></p>

<pre><code>    Ceph使用树型层级结构描述OSD的空间位置以及权重(同磁盘容量相关)大小。如上图所示，层级结构描述了OSD所在主机、主机所在机架以及机架所在机房等空间位置。这些空间位置隐含了故障区域，例如使用不同电源的不同的机架属于不同的故障域。CRUSH能够依据一定的规则将副本放置在不同的故障域。

    OSD节点在层级结构中也被称为Device，它位于层级结构的叶子节点，所有非叶子节点称为Bucket。Bucket拥有不同的类型，如上图所示，所有机架的类型为Rack，所有主机的类型为Host。使用者还可以自己定义Bucket的类型。Device节点的权重代表存储节点的性能，磁盘容量是影响权重大小的重要参数。Bucket节点的权重是其子节点的权重之和。

    CRUSH通过重复执行Take(bucketID)和Select(n, bucketType)两个操作选取副本位置。      
    Take(bucketID)指定从给定的bucketID中选取副本位置，例如可以指定从某台机架上选取副本位置，以实现将不同的副本隔离在不同的故障域; Select(n, bucketType)则在给定的Bucket下选取 n 个类型为bucketType的Bucket，它选取Bucket主要考虑层级结构中节点的容量，以及当节点离线或者加入时的数据迁移量。
</code></pre>

<h1>4.浅析CEPH所能提供的服务</h1>

<h1>5.CEPH API Documents</h1>

<h6>LIBRADOS (PYTHON)</h6>

<p>The rados module is a thin Python wrapper for librados.</p>

<h6>INSTALLATION</h6>

<p>To install Python libraries for Ceph, see Getting librados for Python.</p>

<h6>GETTING STARTED</h6>

<p>You can create your own Ceph client using Python. The following tutorial will show you how to import the Ceph Python module, connect to a Ceph cluster, and perform object operations as a client.admin user.</p>

<p>Note To use the Ceph Python bindings, you must have access to a running Ceph cluster. To set one up quickly, see Getting Started.
First, create a Python source file for your Ceph client. ::
linenos:    sudo vim client.py
IMPORT THE MODULE</p>

<p>To use the rados module, import it into your source file.</p>

<pre><code> import rados
</code></pre>

<h6>CONFIGURE A CLUSTER HANDLE</h6>

<p>Before connecting to the Ceph Storage Cluster, create a cluster handle. By default, the cluster handle assumes a cluster named ceph (i.e., the default for deployment tools, and our Getting Started guides too), and a client.admin user name. You may change these defaults to suit your needs.</p>

<p>To connect to the Ceph Storage Cluster, your application needs to know where to find the Ceph Monitor. Provide this information to your application by specifying the path to your Ceph configuration file, which contains the location of the initial Ceph monitors.</p>

<pre><code> import rados, sys

 #Create Handle Examples.
 cluster = rados.Rados(conffile='ceph.conf')
 cluster = rados.Rados(conffile=sys.argv[1])
 cluster = rados.Rados(conffile = 'ceph.conf', conf = dict (keyring = '/path/to/keyring'))
</code></pre>

<p>Ensure that the conffile argument provides the path and file name of your Ceph configuration file. You may use the sys module to avoid hard-coding the Ceph configuration path and file name.</p>

<p>Your Python client also requires a client keyring. For this example, we use the client.admin key by default. If you would like to specify the keyring when creating the cluster handle, you may use the conf argument. Alternatively, you may specify the keyring path in your Ceph configuration file. For example, you may add something like the following line to you Ceph configuration file:</p>

<p>keyring = /path/to/ceph.client.admin.keyring
For additional details on modifying your configuration via Python, see Configuration.</p>

<h6>CONNECT TO THE CLUSTER</h6>

<p>Once you have a cluster handle configured, you may connect to the cluster. With a connection to the cluster, you may execute methods that return information about the cluster.</p>

<pre><code> import rados, sys

 cluster = rados.Rados(conffile='ceph.conf')
 print "\nlibrados version: " + str(cluster.version())
 print "Will attempt to connect to: " + str(cluster.conf_get('mon initial members'))

 cluster.connect()
 print "\nCluster ID: " + cluster.get_fsid()

 print "\n\nCluster Statistics"
 print "=================="
 cluster_stats = cluster.get_cluster_stats()

 for key, value in cluster_stats.iteritems():
         print key, value
</code></pre>

<p>By default, Ceph authentication is on. Your application will need to know the location of the keyring. The python-ceph module doesn’t have the default location, so you need to specify the keyring path. The easiest way to specify the keyring is to add it to the Ceph configuration file. The following Ceph configuration file example uses the client.admin keyring you generated with ceph-deploy.</p>

<pre><code> [global]
 ...
 keyring=/path/to/keyring/ceph.client.admin.keyring
</code></pre>

<h6>MANAGE POOLS</h6>

<p>When connected to the cluster, the Rados API allows you to manage pools. You can list pools, check for the existence of a pool, create a pool and delete a pool.</p>

<pre><code> print "\n\nPool Operations"
 print "==============="

 print "\nAvailable Pools"
 print "----------------"
 pools = cluster.list_pools()

 for pool in pools:
         print pool

 print "\nCreate 'test' Pool"
 print "------------------"
 cluster.create_pool('test')

 print "\nPool named 'test' exists: " + str(cluster.pool_exists('test'))
 print "\nVerify 'test' Pool Exists"
 print "-------------------------"
 pools = cluster.list_pools()

 for pool in pools:
         print pool

 print "\nDelete 'test' Pool"
 print "------------------"
 cluster.delete_pool('test')
 print "\nPool named 'test' exists: " + str(cluster.pool_exists('test'))
</code></pre>

<h6>INPUT/OUTPUT CONTEXT</h6>

<p>Reading from and writing to the Ceph Storage Cluster requires an input/output context (ioctx). You can create an ioctx with the open_ioctx() method of the Rados class. The ioctx_name parameter is the name of the pool you wish to use.</p>

<pre><code> ioctx = cluster.open_ioctx('data')
</code></pre>

<p>Once you have an I/O context, you can read/write objects, extended attributes, and perform a number of other operations. After you complete operations, ensure that you close the connection. For example:</p>

<pre><code> print "\nClosing the connection."
 ioctx.close()
</code></pre>

<h6>WRITING, READING AND REMOVING OBJECTS</h6>

<p>Once you create an I/O context, you can write objects to the cluster. If you write to an object that doesn’t exist, Ceph creates it. If you write to an object that exists, Ceph overwrites it (except when you specify a range, and then it only overwrites the range). You may read objects (and object ranges) from the cluster. You may also remove objects from the cluster. For example:</p>

<pre><code>print "\nWriting object 'hw' with contents 'Hello World!' to pool 'data'."

ioctx.write_full("hw", "Hello World!")

print "\n\nContents of object 'hw'\n------------------------\n"
print ioctx.read("hw")

print "\nRemoving object 'hw'"
ioctx.remove_object("hw")
</code></pre>

<h6>WRITING AND READING XATTRS</h6>

<p>Once you create an object, you can write extended attributes (XATTRs) to the object and read XATTRs from the object. For example:</p>

<pre><code>print "\n\nWriting XATTR 'lang' with value 'en_US' to object 'hw'"
ioctx.set_xattr("hw", "lang", "en_US")

print "\n\nGetting XATTR 'lang' from object 'hw'\n"
print ioctx.get_xattr("hw", "lang")
</code></pre>

<h6>LISTING OBJECTS</h6>

<p>If you want to examine the list of objects in a pool, you may retrieve the list of objects and iterate over them with the object iterator. For example:</p>

<pre><code>object_iterator = ioctx.list_objects()

while True :

    try :
            rados_object = object_iterator.next()
            print "Object contents = " + rados_object.read()

    except StopIteration :
            break
</code></pre>

<p>The Object class provides a file-like interface to an object, allowing you to read and write content and extended attributes. Object operations using the I/O context provide additional functionality and asynchronous capabilities.</p>

<h1>6.基于CEPH的网盘设计与实现</h1>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[abstract_ceph]]></title>
    <link href="http://guangningsun.github.io/blog/2016/01/22/abstract-ceph/"/>
    <updated>2016-01-22T16:22:21+08:00</updated>
    <id>http://guangningsun.github.io/blog/2016/01/22/abstract-ceph</id>
    <content type="html"><![CDATA[<h1>Ceph简介及研究计划书</h1>

<h2>简介</h2>

<h3>系统架构</h3>

<p>Ceph 生态系统架构可以划分为四部分：</p>

<ul>
<li>client：客户端（数据用户）</li>
<li>mds：Metadata server cluster，元数据服务器（缓存和同步分布式元数据）</li>
<li>osd：Object storage cluster，对象存储集群（将数据和元数据作为对象存储，执行其他关键职能）</li>
<li>mon：Cluster monitors，集群监视器（执行监视功能）</li>
</ul>


<p><img src="http://b.hiphotos.baidu.com/baike/c0%3Dbaike80%2C5%2C5%2C80%2C26/sign=cfb8f11a0a7b020818c437b303b099b6/91ef76c6a7efce1b3ca81175af51f3deb58f658a.jpg" alt="Alt text" /></p>

<pre><code>                  图1 ceph生态架构图
</code></pre>

<h1>各组件浅析</h1>

<ul>
<li>各组件作用可参考
<a href="http://blog.csdn.net/jackjones_008/article/details/43303667">http://blog.csdn.net/jackjones_008/article/details/43303667</a></li>
<li>系列讲解和官方文档详解
<a href="http://docs.ceph.com/docs/master/start/intro/">http://docs.ceph.com/docs/master/start/intro/</a></li>
</ul>


<h3>Ceph客户端 client</h3>

<pre><code>   因为 Linux 显示文件系统的一个公共界面（通过虚拟文件系统交换机 [VFS]），Ceph 的用户透视图就是透明的。管理员的透视图肯定是不同的，考虑到很多服务器会包含存储系统这一潜在因素。从用户的角度看，他们访问大容量的存储系统，却不知道下面聚合成一个大容量的存储池的元数据服务器，监视器，还有独立的对象存储设备。用户只是简单地看到一个安装点，在这点上可以执行标准文件 I/O。Ceph 文件系统 — 或者至少是客户端接口 — 在 Linux 内核中实现。值得注意的是，在大多数文件系统中，所有的控制和智能在内核的文件系统源本身中执行。但是，在 Ceph 中，文件系统的智能分布在节点上，这简化了客户端接口，并为 Ceph 提供了大规模（甚至动态）扩展能力。Ceph 使用一个有趣的备选，而不是依赖分配列表（将磁盘上的块映射到指定文件的元数据）。Linux 透视图中的一个文件会分配到一个来自元数据服务器的 inode number（INO），对于文件这是一个唯一的标识符。然后文件被推入一些对象中（根据文件的大小）。使用 INO 和 object number（ONO），每个对象都分配到一个对象 ID（OID）。在 OID 上使用一个简单的哈希，每个对象都被分配到一个放置组。放置组（标识为 PGID）是一个对象的概念容器。最后，放置组到对象存储设备的映射是一个伪随机映射，使用一个叫做 Controlled Replication Under Scalable Hashing（CRUSH）的算法。这样一来，放置组（以及副本）到存储设备的映射就不用依赖任何元数据，而是依赖一个伪随机的映射函数。这种操作是理想的，因为它把存储的开销最小化，简化了分配和数据查询。分配的最后组件是集群映射。集群映射 是设备的有效表示，显示了存储集群。有了 PGID 和集群映射，您就可以定位任何对象。
</code></pre>

<h3>Ceph元数据服务器 mds</h3>

<pre><code>    元数据服务器（cmds）的工作就是管理文件系统的名称空间。虽然元数据和数据两者都存储在对象存储集群，但两者分别管理，支持可扩展性。事实上，元数据在一个元数据服务器集群上被进一步拆分，元数据服务器能够自适应地复制和分配名称空间，避免出现热点。如图 4 所示，元数据服务器管理名称空间部分，可以（为冗余和性能）进行重叠。元数据服务器到名称空间的映射在 Ceph 中使用动态子树逻辑分区执行，它允许 Ceph 对变化的工作负载进行调整（在元数据服务器之间迁移名称空间）同时保留性能的位置。


图 4. 元数据服务器的 Ceph 名称空间的分区
首页：元数据服务器的 Ceph 名称空间的分区
但是因为每个元数据服务器只是简单地管理客户端人口的名称空间，它的主要应用就是一个智能元数据缓存（因为实际的元数据最终存储在对象存储集群中）。进行写操作的元数据被缓存在一个短期的日志中，它最终还是被推入物理存储器中。这个动作允许元数据服务器将最近的元数据回馈给客户（这在元数据操作中很常见）。这个日志对故障恢复也很有用：如果元数据服务器发生故障，它的日志就会被重放，保证元数据安全存储在磁盘上。元数据服务器管理 inode 空间，将文件名转变为元数据。元数据服务器将文件名转变为索引节点，文件大小，和 Ceph 客户端用于文件 I/O 的分段数据（布局）。
</code></pre>

<h3>Ceph 监视器 mon</h3>

<pre><code>   Ceph 包含实施集群映射管理的监视器，但是故障管理的一些要素是在对象存储本身中执行的。当对象存储设备发生故障或者新设备添加时，监视器就检测和维护一个有效的集群映射。这个功能按一种分布的方式执行，这种方式中映射升级可以和当前的流量通信。Ceph 使用 Paxos，它是一系列分布式共识算法。
</code></pre>

<h3>Ceph对象存储 osd</h3>

<pre><code>    和传统的对象存储类似，Ceph 存储节点不仅包括存储，还包括智能。传统的驱动是只响应来自启动者的命令的简单目标。但是对象存储设备是智能设备，它能作为目标和启动者，支持与其他对象存储设备的通信和合作。从存储角度来看，Ceph 对象存储设备执行从对象到块的映射（在客户端的文件系统层中常常执行的任务）。这个动作允许本地实体以最佳方式决定怎样存储一个对象。Ceph 的早期版本在一个名为 EBOFS 的本地存储器上实现一个自定义低级文件系统。这个系统实现一个到底层存储的非标准接口，这个底层存储已针对对象语义和其他特性（例如对磁盘提交的异步通知）调优。今天，B-tree 文件系统（BTRFS）可以被用于存储节点，它已经实现了部分必要功能（例如嵌入式完整性）。因为 Ceph 客户实现 CRUSH，而且对磁盘上的文件映射块一无所知，下面的存储设备就能安全地管理对象到块的映射。这允许存储节点复制数据（当发现一个设备出现故障时）。分配故障恢复也允许存储系统扩展，因为故障检测和恢复跨生态系统分配。Ceph 称其为 RADOS
</code></pre>

<hr />

<h3>ceph工作原理浅析</h3>

<p>有待后续补充</p>

<hr />

<h3>ceph与HDFS对比浅析</h3>

<pre><code>   Ceph对比HDFS优势在于易扩展，无单点。HDFS是专门为Hadoop这样的云计算而生，在离线批量处理大数据上有先天的优势，而Ceph是一个通用的实时存储系统。虽然Hadoop可以利用Ceph作为存储后端，但整合过程不及HDFS轻便，且执行计算任务上性能还是略逊于HDFS
</code></pre>

<p><code>有待后续补充</code></p>

<h3>ceph安装部署</h3>

<pre><code>    部署可分为单节点部署和多节点部署，软件版本可从测试环境和生产环境来区分，具体部署过程可参考官方文档和一些博客，部署中遇到问题再详细分析。
</code></pre>

<ul>
<li>官方安装文档 <a href="http://docs.ceph.com/docs/master/start/">http://docs.ceph.com/docs/master/start/</a></li>
<li>手动安装文档  <a href="http://docs.ceph.com/docs/master/install/">http://docs.ceph.com/docs/master/install/</a></li>
<li>可参考博客<br/>
<a href="http://blog.csdn.net/i_chips/article/details/19985795">http://blog.csdn.net/i_chips/article/details/19985795</a>
<a href="http://blog.csdn.net/pc620/article/details/9002045">http://blog.csdn.net/pc620/article/details/9002045</a></li>
</ul>


<p><code>详细部署步骤可参考《ceph部署实验过程》文档</code></p>

<h3>ceph整体架构分析工作原理分析</h3>

<p><code>有待后续补充</code></p>

<h3>ceph API接口分析</h3>

<p><code>有待后续补充</code></p>

<h3>ceph 开发解析</h3>

<p><code>有待后续补充</code></p>

<h3>硬件配置推荐</h3>

<ul>
<li><p>小型测试集群最低配置推荐</p>

<p> Ceph可以运行在廉价的普通硬件上，小型生产集群和开发集群可以在一般的硬件上。</p></li>
<li><table>
<thead>
<tr>
<th>组件 </th>
<th> 标准 </th>
<th> 最低配置推荐</th>
</tr>
</thead>
<tbody>
<tr>
<td>ceph-osd     </td>
<td> 处理器          </td>
<td> 1X 64位AMD-64/i386的双核</td>
</tr>
<tr>
<td></td>
<td> RAM           </td>
<td> 每个守护进程需要500 MB</td>
</tr>
<tr>
<td></td>
<td>磁盘空间</td>
<td>1x storage drive per daemon</td>
</tr>
<tr>
<td></td>
<td>网络</td>
<td>2个1GB以太网网卡</td>
</tr>
<tr>
<td>ceph-mds         </td>
<td> 处理器          </td>
<td> 1X 64位AMD-64/i386的双核</td>
</tr>
<tr>
<td></td>
<td> RAM           </td>
<td> 每个守护进程最低1GB</td>
</tr>
<tr>
<td></td>
<td>磁盘空间</td>
<td>每个守护进程1MB</td>
</tr>
<tr>
<td></td>
<td>网络</td>
<td>2个1GB以太网网卡</td>
</tr>
<tr>
<td>ceph-mon     </td>
<td> 处理器          </td>
<td> 1X 64位AMD-64/i386的双核</td>
</tr>
<tr>
<td></td>
<td> RAM           </td>
<td> 每个守护进程最低1GB</td>
</tr>
<tr>
<td></td>
<td>磁盘空间</td>
<td>每个守护进程10GB</td>
</tr>
<tr>
<td></td>
<td>网络</td>
<td>2个1GB以太网网卡</td>
</tr>
</tbody>
</table>
</li>
<li><p>生产集群最低标准配置推荐</p>

<p>PB级生产集群也可以使用普通硬件，但应该配备更多内存、CPU和数据存储空间来解决流量压力。一个2012年的Ceph集群项目使用了两个相当强悍的OSD硬件配置，和稍逊的监视器配置。</p></li>
<li><table>
<thead>
<tr>
<th>组件 </th>
<th> 标准 </th>
<th> 最低配置推荐</th>
</tr>
</thead>
<tbody>
<tr>
<td>戴尔PE R510   </td>
<td>处理器      </td>
<td>2个64位四核Xeon处理器</td>
</tr>
<tr>
<td></td>
<td>RAM</td>
<td>   16 GB</td>
</tr>
<tr>
<td></td>
<td>磁盘空间   </td>
<td>8个2TB驱动器。1个操作系统，7个存储</td>
</tr>
<tr>
<td></td>
<td>客户端网络</td>
<td>       2个1GB以太网网卡</td>
</tr>
<tr>
<td></td>
<td>OSD网络</td>
<td> 2个1GB以太网网卡</td>
</tr>
<tr>
<td></td>
<td>管理网络   </td>
<td>   2个1GB以太网网卡</td>
</tr>
<tr>
<td>戴尔PE R515</td>
<td>      处理器   </td>
<td>   1X 16核 Opteron CPU</td>
</tr>
<tr>
<td></td>
<td>RAM    </td>
<td>16GB</td>
</tr>
<tr>
<td></td>
<td>os存储   </td>
<td>1X 500GB硬盘的操作系统。</td>
</tr>
<tr>
<td></td>
<td>磁盘空间   </td>
<td>12X 3TB硬盘的存储</td>
</tr>
<tr>
<td></td>
<td>客户端网络</td>
<td>       2个1GB以太网网卡</td>
</tr>
<tr>
<td></td>
<td>OSD网络  </td>
<td>2个1GB以太网网卡</td>
</tr>
<tr>
<td></td>
<td>管理网络   </td>
<td>   2个1GB以太网网卡</td>
</tr>
</tbody>
</table>
</li>
</ul>


<h3>操作系统推荐</h3>

<p>Linux内核</p>

<ul>
<li>Ceph的核心客户端目前建议：

<ol>
<li>v4.1.4.或更高版本</li>
<li>v3.16.3或更高版本</li>
</ol>
</li>
<li>btrfs文件系统： B-tree File System (Btrfs)如果想在btrfs上运行Ceph，我们推荐使用一个最新的Linux内核（V3.14或更高版本）</li>
</ul>


<p>系统平台
       下面的表格展示了Ceph需求和各种Linux发行版的对应关系。一般来说，Ceph对内核和系统初始化阶段的依赖很少（如sysvinit，upstart, systemd）。</p>

<ul>
<li><table>
<thead>
<tr>
<th>Distro    </th>
<th>Release    </th>
<th>Code Name  </th>
<th>Kernel </th>
<th>Notes  </th>
<th>Testing</th>
</tr>
</thead>
<tbody>
<tr>
<td>CentOS      </td>
<td>7  </td>
<td>N/A</td>
<td>   linux-3.10.0    </td>
<td>   </td>
<td>B, I, C</td>
</tr>
<tr>
<td>Debian  </td>
<td>8.0    </td>
<td>Jessie</td>
<td>    linux-3.16.0</td>
<td>   </td>
<td>   B, I</td>
</tr>
<tr>
<td>Fedora</td>
<td> 22  </td>
<td>N/A    </td>
<td>linux-3.14.0   </td>
<td></td>
<td>  B, I</td>
</tr>
<tr>
<td>RHEL    </td>
<td>7  </td>
<td>Maipo  </td>
<td>linux-3.10.0   </td>
<td></td>
<td>  B, I</td>
</tr>
<tr>
<td>Ubuntu  </td>
<td>14.04  </td>
<td>Trusty Tahr    </td>
<td>linux-3.13.0</td>
<td></td>
<td> B, I, C</td>
</tr>
</tbody>
</table>
</li>
<li><p>详细操作系统推荐可查询</p></li>
</ul>


<p> <a href="http://docs.ceph.com/docs/master/start/os-recommendations/">http://docs.ceph.com/docs/master/start/os-recommendations/</a></p>

<h2>- 附注</h2>

<ol>
<li>默认内核btrfs版本较老，不推荐用于Ceph-osd存储节点；要升级到推荐的内核，或者改用xfs、ext4。</li>
<li>默认内核带的Ceph客户端较老，不推荐做内核空间客户端（内核RBD或Ceph文件系统），请升级到推荐内核。</li>
<li>已安装的glibc版本不支持syncfs（2）系统调用，同一台机器上使用xfs或ext4的Ceph-osd守护进程性能一般，它可以更好。</li>
</ol>


<h2>- 测试</h2>

<ol>
<li>B：我们持续地在这个平台上编译所有分支、做基本单元测试；也为这个平台构建可发布软件包。</li>
<li>I： 我们在这个平台上做基本的安装和功能测试。</li>
<li>C：我们在这个平台上持续地做全面的功能、退化、压力测试，包括开发分支、预发布版本、正式发布版本。</li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[ning]]></title>
    <link href="http://guangningsun.github.io/blog/2016/01/22/ning/"/>
    <updated>2016-01-22T10:39:50+08:00</updated>
    <id>http://guangningsun.github.io/blog/2016/01/22/ning</id>
    <content type="html"><![CDATA[<p><meta http-equiv="Content-Type" content="text/html; charset=utf-8"></p>

<h2>单点安装部署ceph实验过程</h2>

<p><strong>1.官方推荐Ubuntu12.0.4服务器版本操作系统。</strong></p>

<blockquote><p>下载地址
<a href="http://releases.ubuntu.com/12.04/ubuntu-12.04.5-alternate-amd64.iso.torrent">http://releases.ubuntu.com/12.04/ubuntu-12.04.5-alternate-amd64.iso.torrent</a></p></blockquote>

<p><strong>2.更新系统source</strong></p>

<blockquote><ul>
<li>命令：sudo apt-get update 若网速不好 ，最好更换镜像 方法1.直接下载镜像的sources.list
方法2.手动修改sources.list<br/>
针对方法一 以163镜像为例，Ubuntu12.04 cd /etc/apt/
备份原有list</li>
<li>命令：mv sources.list sources.list.bak
curl -0 <a href="http://mirrors.163.com/.help/sources.list.precise">http://mirrors.163.com/.help/sources.list.precise</a>
下载新的source.list</li>
<li>命令：wget <a href="http://mirrors.163.com/.help/sources.list.precise">http://mirrors.163.com/.help/sources.list.precise</a>
应用source.list.precise</li>
<li>命令：mv source.list.precise source/list</li>
</ul>
</blockquote>

<p><strong>3.查看防火墙状态</strong></p>

<blockquote><ul>
<li>命令 ：sudo ufw status 若是开启状态，需关闭防火墙</li>
<li>命令：sudo ufw disable</li>
</ul>
</blockquote>

<p><strong>4.查看系统时间是否正确</strong></p>

<blockquote><ul>
<li>命令：sudo date -s “2016-01-20 11:38:50”</li>
<li>命令：sudo hwclock -w 写入硬件时间</li>
</ul>
</blockquote>

<p><strong>5.安装ceph</strong></p>

<blockquote><p>ubuntu12.0.4默认ceph为0.4版本，但当并没有发现安装痕迹</p>

<ul>
<li>命令：sudo apt-get install ceph ceph-common ceph-mds</li>
<li>命令：ceph  -v 查看ceph版本信息</li>
</ul>
</blockquote>

<p><strong>6.创建 ceph配置文件</strong></p>

<blockquote><ul>
<li>命令：vi /etc/ceph/ceph.conf</li>
</ul>
</blockquote>

<p>配置文件内容</p>

<blockquote><p>[global]
    max open files = 131072
    auth cluster required = none
    auth client required = none</p>

<p> [osd] <br/>
      osd journal size = 1000  <br/>
       filestore xattruse omap = true<br/>
       osd mkfs options xfs = -f      osd mount
options xfs = rw,noatime</p>

<p>[mon.a]   <br/>
host = ceph1
mon addr = 192.168.73.129:6789</p>

<p>[osd.0]   <br/>
     host = ceph1   <br/>
     devs = /dev/sdb1</p>

<p>[osd.1]
    host= ceph1
    devs= /dev/sdb2</p>

<p>  [mds.a]
    host= ceph1</p></blockquote>

<p>注意:对于较低的Ceph版本（例如0.42），需要在[mon]项下添加一行内容:mondata=/data/name,以及在[osd]项下添加一行内容：osd data = /data/$name，需要新建/data目录以作为后续的数据目录；相应的，后续针对数据目录的步骤也需要调整。</p>

<p><strong>7.创建数据目录</strong></p>

<blockquote><p>安装分区工具 命令: apt-get install xfsprogs 创建数据目录命令：
mkdir -p /var/lib/ceph/osd/ceph-0
mkdir -p /var/lib/ceph/osd/ceph-1
mkdir -p /var/lib/ceph/mon/ceph-a
mkdir -p /var/lib/ceph/mds/ceph-a</p></blockquote>

<p><strong>8.为osd创建分区和进行挂载</strong></p>

<blockquote><p>对新分区进行xfs或者btrfs的格式化名称根据实际情况来命名，实验机为sda3和sda4
mkfs.xfs -f /dev/sdb1
mkfs.xfs -f /dev/sdb2<br/>
自动挂载分区到指定路径 第一次必须先挂载分区写入初始化数据：
mount /dev/sdb1 /var/lib/ceph/osd/ceph-0
mount /dev/sdb2 /var/lib/ceph/osd/ceph-1</p></blockquote>

<p><strong>9.执行初始化</strong></p>

<blockquote><p>执行初始化前，都先停止Ceph服务，并清空原有数据目录：</p>

<ul>
<li>/etc/init.d/ceph stop</li>
<li>rm -rf /var/lib/ceph/<em>/ceph-</em>/*<br/>
然后，就在mon所在的节点上执行初始化了：</li>
<li><p> sudo mkcephfs -a -c /etc/ceph/ceph.conf -k /etc/ceph/ceph1.keyring</p></li>
<li><p> 注意，一旦配置文件ceph.conf发生改变，初始化最好重新执行一遍。</p></li>
</ul>
</blockquote>

<p><strong>10.启动ceph</strong></p>

<blockquote><p>在mon所在的节点上执行：</p>

<ul>
<li>sudo service ceph -a start<br/>
注意，执行上面这步时，可能会遇到如下提示：
=== osd.0 ===  Mounting xfs onceph4:/var/lib/ceph/osd/ceph-0  Error ENOENT: osd.0 does not exist.  create it before updating the crush map
执行如下命令后，再重复执行上面那条启动服务的命令，就可以解决：</li>
<li>ceph osd create</li>
</ul>
</blockquote>

<p><strong>11.执行健康检查</strong></p>

<blockquote><ul>
<li>命令：sudo ceph health
也可以使用ceph -s命令查看状态 <br/>
如果返回的是HEALTH_OK，则代表成功！ <br/>
如果遇到如下提示： <br/>
HEALTH_WARN 384 pgs degraded; 384 pgs stuck unclean;
recovery 21/42degraded (50.000%) <br/>
则说明osd数量不够，Ceph默认至少需要提供两个osd。</li>
</ul>
</blockquote>

<hr />
]]></content>
  </entry>
  
</feed>
