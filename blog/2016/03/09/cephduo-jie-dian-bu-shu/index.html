<hr />

<p>layout: post
title: &ldquo;ceph多节点部署&rdquo;
date: 2016-03-09 18:20:28 +0800
comments: true</p>

<h2>categories:</h2>

<h3>实验环境</h3>

<hr />

<ul>
<li>VMWare 虚拟4台 centos7.1 测试机</li>
<li>ceph osd不支持操作系统所在挂载盘，需要新挂载盘 并格式化xfs 并按照配置文件创建存储目录</li>
<li><p>查看 /var/lib/ceph/osd/ceph-*</p>

<p> 官方文档推荐用ceph-depoly 半自动化部署工具，详细部署过程后续补充</p>

<p> 但部署过程中出现错误较多，所以改为先手动配置</p></li>
<li><p>ceph-deploy install &ndash;rgw nodename 安装ceph gw并可更新ceph版本</p></li>
</ul>


<h3>打通各节点的ssh机器互信</h3>

<ul>
<li><p>ssh-keygen -t rsa -f .ssh/id_rsa -P &lsquo;&rsquo;</p>

<p>  此处一路回车并没有设置验证码，生产可能并不允许。</p></li>
</ul>


<h3>向被授权的主机上拷贝</h3>

<ul>
<li>ssh-copy-id -i .ssh/id_rsa.pub <a href="&#x6d;&#x61;&#x69;&#108;&#116;&#x6f;&#58;&#114;&#111;&#111;&#x74;&#x40;&#x31;&#57;&#x32;&#x2e;&#x31;&#x36;&#56;&#46;&#x32;&#x38;&#x2e;&#49;&#51;&#48;">&#114;&#x6f;&#x6f;&#116;&#64;&#x31;&#x39;&#x32;&#x2e;&#49;&#54;&#56;&#46;&#50;&#56;&#x2e;&#x31;&#x33;&#x30;</a></li>
</ul>


<h3>安装ntpdate</h3>

<ul>
<li>yum -y install ntpdate</li>
</ul>


<h3>开始同步时间</h3>

<ul>
<li>ntpdate time.windows.com</li>
</ul>


<h3>修改每台机器的主机名 (hostname)  ( mon,mds,osd,client 都必须设置 )</h3>

<hr />

<pre><code>    echo 192.168.28.128 ceph-mon &gt;&gt; /etc/hosts

    echo 192.168.28.129 ceph-osd0 &gt;&gt; /etc/hosts

    echo 192.168.28.130 ceph-osd1 &gt;&gt; /etc/hosts

    echo 192.168.28.131 ceph-osd2 &gt;&gt; /etc/hosts
</code></pre>

<h3>yum更新，安装相关依赖包(适用于mon,mds,osd)</h3>

<hr />

<pre><code>    rpm --import 'https://ceph.com/git/?p=ceph.git;a=blob_plain;f=keys/release.asc'
    rpm -Uvh http://mirrors.yun-idc.com/epel/7/x86_64/e/epel-release-7-5.noarch.rpm
    yum install snappy leveldb gdisk python-argparse gperftools-libs -y
    rpm -Uvh http://ceph.com/rpm-dumpling/el7/noarch/ceph-release-1-0.el7.centos.noarch.rpm
    yum install ceph-deploy -y
    yum install ceph -y

    yum install btrfs-progs           (适用于所有osd) 
</code></pre>

<h3>配置/etc/ceph/ceph.conf，  (适用于mon,mds,osd)</h3>

<hr />

<p>vi /etc/ceph/ceph.conf</p>

<hr />

<p>[global]</p>

<p>public_network = 192.168.28.1/24</p>

<p>pid_file = /var/run/ceph/$name.pid</p>

<p>auth_cluster_required = none</p>

<p>auth_service_required = none</p>

<p>auth_client_required = none</p>

<p>keyring = /etc/ceph/keyring.$name</p>

<p>osd_pool_default_size = 1</p>

<p>osd_pool_default_min_size = 1</p>

<p>osd_pool_default_crush_rule = 0</p>

<p>osd_crush_chooseleaf_type = 1</p>

<p>[mon]</p>

<p>mon_data = /var/lib/ceph/mon/$name</p>

<p>mon_clock_drift_allowed = .15</p>

<p>keyring = /etc/ceph/keyring.$name</p>

<p>[mon.0]</p>

<p>host = ceph-mon</p>

<p>mon_addr = 192.168.28.129:6789</p>

<p>[osd]</p>

<p>osd_recovery_max_active = 5</p>

<p>osd_mkfs_type = xfs</p>

<p>keyring = /etc/ceph/keyring.$name</p>

<p>[osd.0]</p>

<p>host = ceph-osd0</p>

<p>devs = /dev/sda3</p>

<p>[osd.1]</p>

<p>host = ceph-osd1</p>

<p>devs = /dev/sda3</p>

<p>[osd.2]</p>

<p>host = ceph-osd2</p>

<p>devs = /dev/sda3</p>

<p>[client.radosgw.gateway]</p>

<p>host = ceph-mon</p>

<p>log file = /var/log/radosgw/client.radosgw.ustack.log</p>

<h3>创建目录 ( osd )</h3>

<hr />

<pre><code>    mkdir -p /var/lib/ceph/osd/ceph-* 默认为该目录下存放数据，可以在[osd]中配置存放路径
</code></pre>

<h3>启动ceph(在mon上执行)</h3>

<hr />

<pre><code>    初始化：mkcephfs -a -c /etc/ceph/ceph.conf
    此时数据目录下应为空
    /etc/init.d/ceph -a start
</code></pre>

<h3>启动时遇到问题</h3>

<hr />

<pre><code>    遇到 Error ENOENT: osd.0 does not exist.  create it before updating the crush map
    执行如下代码: ceph osd create
    然后在执行   /etc/init.d/ceph -a start       既可完成（仍需清空数据目录）
</code></pre>

<h3>启动后健康检查</h3>

<hr />

<pre><code>ceph health      #也可以使用ceph -s命令查看状态  (如果返回的是HEALTH_OK，则代表成功！)  
</code></pre>
